{% extends '../layout.html' %} {% load static %} {% block description %}
<meta name="description" content="Matrices or grids in C++" />
{% endblock %} {% block title %}
<title>Matrices</title>
{% endblock %} {% block content %}
<h1>Matrices & Tensors</h1>
<section id="intro">
	<p>
		<span class="drop">F</span>ollowing lists, the next simplest data structure
		we explore is the <span class="term">matrix</span> or
		<span class="term">grid</span>. In the simplest case, the matrix is a data
		structure that consists of <span class="italicsText">rows</span> and
		<span class="italicsText">columns</span>. Each row in the matrix is a list,
		and each column is itself a list. This corresponds to the fact the simplest
		matrix is a 2-dimensional array. Like lists, there are both
		<span class="term">static matrices</span> and
		<span class="term">dynamic matrices</span>.
	</p>
</section>

<section id="static_matrices">
	<h2>Static Matrices</h2>
	<p>
		The <span class="italicsText">static matrix</span> is a matrix of fixed
		size. We can implement the static matrix with a 2-dimensional array:
	</p>
	<pre class="language-cpp"><code>
		int main() {
			int M[3][4];

			return 0;
		}
	</code></pre>
	<p>
		The implementation above represents a matrix of 3 rows and 4 columns. In
		other words, a ${3 \times 4}$ matrix. Visually:
	</p>
	<div id="matrix1"></div>
	<p>
		Note, however, that in memory, this is really an array where each element
		contains an array. However, because of indices, we can treat the square
		bracket indexing as ${M[r][c],}$ where ${M}$ is an array, ${r}$ is a row
		number (an integer and the index of an element in the array), and ${c}$ is a
		column number (an integer and the index of the element inside an element in
		the array).
	</p>
	<p>Suppose we initialized the array:</p>
	<pre class="language-cpp"><code>
		int main() {
			int M[3][4] = {{1, 2, 3}, {4, 5, 6&rcub;, {7, 8, 9&rcub;&rcub;;

			return 0;
		}
	</code></pre>
	<p>This generates the following matrix in mathematical notation:</p>
	<figure>
		<div>
			$$ M = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix}
			$$
		</div>
	</figure>
</section>

<section id="dynamic_matrices">
	<h2>Dynamic Matrices</h2>
	<p>
		A <span class="term">dynamic matrix</span> is a matrix whose size can be
		changed. We can implement the dynamic matrix with 2-dimensional
		<span class="italicsText">dynamic arrays</span>. This is done by first
		creating an array of pointers, then having each pointer element point to an
		array created in the heap. For example, we can visualize a dynamic ${3
		\times 3}$ matrix as such:
	</p>
	<figure>
		<img
			src="{% static 'images/heapMatrix.svg' %}"
			alt="A matrix in the heap"
			loading="lazy"
			width="250px"
			height="250px"
		/>
	</figure>
	<p>The implementation is straightforward:</p>
	<pre class="language-cpp"><code>
		int main() {
			int *A[3];
			A[0] = new int[3];
			A[1] = new int[3];
			A[2] = new int[3];

			return 0;
		}
	</code></pre>
	<p>
		With the implementation above, however, only the array's columns are
		dynamic. We can, however, make the entire matrix dynamic (i.e., both the
		rows and columns are dynamic), by having both the rows and the columns in
		the heap. This is done with
		<span class="italicsText">double pointers</span>.
	</p>
	<pre class="language-cpp"><code>
		int main() {
			int **M = new int*[3];
			M[0] = new int[3];
			M[1] = new int[3];
			M[2] = new int[3];

			return 0;
		}
	</code></pre>
	<p>
		As we can likely tell, accessing each of the elements in a matrix can be
		done through nested loops. The outter loop iterates through the rows, whilst
		the inner loop iterates through the columns.
	</p>
</section>

<section id="row_major_formula">
	<h2>Addressing Matrices</h2>
	<p>
		As with any array, an initialized multidimensional array must be stored
		somehow in memory. This in turn requires a method for storage and
		addressing. On modern computers, there are two storage methods:
		<span class="term">row-major ordering</span> and
		<span class="term">column-major ordering</span>. We analyze both approaches
		in turn.
	</p>
	<section id="row_major_order">
		<h3>Row-major Ordering</h3>
		<p>
			First, let's consider row-major ordering (also called
			<span class="italicsText">row-major mapping</span>). This storage method
			is used by C, C++, Objective-C, Pascal, and many other high-level
			languages. Under row-major ordering, arrays are stored row by row. This
			idea is best clarified by example. Suppose we initialized the following
			matrix:
		</p>
		<pre class="language-cpp"><code>
			int main() {
				int A[2][3] = {{3, 6, 9}, {12, 15, 18 &rcub;};
			}
		</code></pre>
		<p>With row-major mapping, the matrix is represented as such:</p>
		<figure>
			<img
				src="{% static 'images/matrix2.svg' %}"
				alt="2 by 3 matrix"
				loading="lazy"
				width="150px"
				height="150px"
			/>
		</figure>
		<p>
			Notice that the column number represents the index of each element in the
			array, while the row number represents the index of each element within an
			element in the array. Notice further the transparent red line displaying
			the actual sequence. This is where the name &#8220;row-major
			ordering&#8221; comes from. Array elements are assigned to successive
			memory locations by moving across the rows first and then down the
			columns:
		</p>
		<figure>
			<img
				src="{% static 'images/matrix2Memory.svg' %}"
				alt="2 by 3 matrix"
				loading="lazy"
				width="150px"
				height="150px"
			/>
		</figure>
		<p>
			We can interpret the matrix declaration as a declaration of the following:
		</p>
		<figure>
			<div>
				<p>${A[\textit{number of rows}][\textit{number of columns}]}$</p>
			</div>
		</figure>
		<p>Accordingly, when we write:</p>
		<figure>
			<div>
				<p>${A[i][j]}$</p>
			</div>
		</figure>
		<p>${i}$ is the row index, and ${j}$ is the column index.</p>
		<p>
			Say the matrix's index starts at 100. How does the compiler transform the
			expression <span class="monoText">A[1][2]</span> to an address? The
			compiler achieves this transformation by rewriting the expression as a
			formula computing the address. The question then, is, what is this
			formula? From the above diagram, the first element's address is 100. Thus,
			the address of ${\alpha(\texttt{A[0][0]})}$ is 100, where ${\alpha(n)}$ is
			some function returning the address of the element ${n.}$ The base address
			serves as the first term in the formula. Thus:
		</p>
		<figure>
			<div>
				<p><span class="monoText">${\alpha}$(A[1][2]) = ${100}$</span></p>
			</div>
		</figure>
		<p>
			Next, given the value <span class="monoText">3</span> in the array's
			declaration <span class="monoText">A[2][3]</span>, the compiler knows the
			array <span class="monoText">A</span> contains 3 columns. This is called
			the <span class="term">row size</span> &mdash; the number of elements in
			each row. Accordingly, when it encounters the expression
			<span class="monoText">A[1][2]</span>, it sees the value
			<span class="monoText">1</span> (the row index), so it multiplies this
			number by the row size, 3. This yields the number of elements it must move
			<span class="italicsText">from</span> the base address: ${1 \times 3 =
			3.}$
		</p>
		<figure>
			<div>
				<p>
					<span class="monoText"
						>${\alpha}$(A[1][2]) = ${100 + (1 \cdot 3)}$</span
					>
				</p>
			</div>
		</figure>
		<p>
			Next, it sees in <span class="monoText">A[1][2]</span> the value
			<span class="monoText">2</span> (the column index), so it includes the
			operation <span class="monoText">+ 2</span> (i.e., &#8220;move an
			additional 2 elements&#8221;):
		</p>
		<figure>
			<div>
				<p>
					<span class="monoText"
						>${\alpha}$(A[1][2]) = ${100 + ((1 \cdot 3) + 2)}$</span
					>
				</p>
			</div>
		</figure>
		<p>
			It then multiples the sum by 4 bytes, the size of an
			<span class="monoText">int</span>:
		</p>
		<figure>
			<div>
				<p>
					<span class="monoText"
						>${\alpha}$(A[1][2]) = ${100 + 4((1 \cdot 3) + 2)}$</span
					>
				</p>
			</div>
		</figure>
		<p>
			Simplifying the right-hand side, we have ${100 + 20 = 120,}$ the address
			of <span class="monoText">A[1][2]</span>. Generalizing this analysis, the
			row-major order approach employs the following formula for communicating
			addresses:
		</p>

		<p>
			<span class="topic"
				>Formula: Row-major Ordering (${0}$-based Indexing).</span
			>
			Given a matrix of order ${n \times m,}$ where ${n}$ is the number of rows
			and ${m}$ is the number of columns (i.e., ${A[n][m]}$),
		</p>
		<figure>
			<div>
				<p>${\alpha(A[i][j]) = \alpha(A[0][0]) + \omega((i \cdot m) + j)}$</p>
			</div>
		</figure>
		<p>
			Where ${\alpha(A[i][j])}$ is the address of the element ${A[i][j],}$
			${\alpha(A[0][0])}$ is the base address, ${\omega}$ is the size of the
			array's type, ${i}$ is the row index, ${m}$ is the row size, and ${j}$ is
			the column index.
		</p>

		<p>
			Note that this formula changes when the language uses 1-based indexing:
		</p>

		<p>
			<span class="topic"
				>Formula: Row-major Ordering (${1}$-based Indexing).</span
			>
			Given a matrix of order ${n \times m,}$ where ${n}$ is the number of rows
			and ${m}$ is the number of columns (i.e., ${A[n][m]}$),
		</p>
		<figure>
			<div>
				<p>
					${\alpha(A[i][j]) = \alpha(A[0][0]) + \omega(((i - 1) \cdot m) + (j -
					1))}$
				</p>
			</div>
		</figure>
		<p>
			Where ${\alpha(A[i][j])}$ is the address of the element ${A[i][j],}$
			${\alpha(A[0][0])}$ is the base address, ${\omega}$ is the size of the
			array's type, ${i}$ is the row index, ${m}$ is the row size, and ${j}$ is
			the column index.
		</p>

		<p>
			The difference is that we must subtract 1 from the row index ${i,}$ and 1
			from the column index ${j.}$ This is because if we start from 1, we're no
			longer offsetting from 0, but rather offsetting from 1. In other words, if
			we used the previous formula, we would be off by 1, since the index starts
			at 1. We can see that with 1-based indexing, there are 2 additional
			decrementing operations to be performed, totalling 6 distinct operations.
			This is in contrast to 0-based indexing, where there are only 4 distinct
			operations.
		</p>
	</section>

	<section id="column_major_ordering">
		<h3>Column-major Ordering</h3>
		<p>
			The alternative to row-major ordering is
			<span class="italicsText">column-major ordering</span> (also called
			<span class="italicsText">column-major mapping</span>). In column-major
			ordering, array elements are stored column by column. This method is used
			by languages like Fortran, MATLAB, Octave, S-Plus, R, Julia, Scilab, and a
			few others. Suppose we initialized the following matrix:
		</p>
		<pre class="language-cpp"><code>
			int main() {
				int A[3][4] = {{2, 4, 6, 8}, {10, 12, 14, 16}, {18, 20, 22, 24&rcub;};
			}
		</code></pre>
		<p>
			In column-major ordering, this matrix is visually interpreted as such:
		</p>
		<figure>
			<img
				src="{% static 'images/column_major.svg' %}"
				alt="A column major matrix"
				loading="lazy"
				width="150px"
				height="150px"
			/>
		</figure>
		<p>And in memory, the elements are stored column by column:</p>
		<figure>
			<img
				src="{% static 'images/column_major_memory.svg' %}"
				alt="Column major in memory"
				loading="lazy"
				width="150px"
				height="150px"
			/>
		</figure>
		<p>
			Suppose we wrote the expression <span class="monoText">A[1][2]</span>. To
			return the address for this element, we use the same form ${A[r][s],}$
			where ${r}$ is the row index and ${s}$ is the column index. Again, the
			compiler uses a formula, the first term being the base address:
		</p>
		<figure>
			<div>
				<p><span class="monoText">${\alpha}$(A[1][2]) = ${100}$</span></p>
			</div>
		</figure>
		<p>
			Next, the compiler sees the value <span class="monoText">2</span> in the
			expression <span class="monoText">A[1][2]</span> (the number of columns),
			and multiplies this by the number of elements in each column (the number
			of rows): ${2 \cdot 3 = 6.}$ This yields the number of elements offset
			from the first element. Thus:
		</p>
		<figure>
			<div>
				<p>
					<span class="monoText"
						>${\alpha}$(A[1][2]) = ${100 + (2 \cdot 3)}$</span
					>
				</p>
			</div>
		</figure>
		<p>
			Next, the compiler sees the value <span class="monoText">1</span> in the
			expression <span class="monoText">A[1][2]</span>, so it includes the
			operation <span class="monoText">+ 1</span> (move 1 more element):
		</p>
		<figure>
			<div>
				<p>
					<span class="monoText"
						>${\alpha}$(A[1][2]) = ${100 + ((2 \cdot 3) + 1)}$</span
					>
				</p>
			</div>
		</figure>
		<p>Finally, it multiplies the size of the array's type:</p>
		<figure>
			<div>
				<p>
					<span class="monoText"
						>${\alpha}$(A[1][2]) = ${100 + 4((2 \cdot 3) + 1)}$</span
					>
				</p>
			</div>
		</figure>
		<p>
			Simplifying the right-hand expression, we have ${100 + 28 = 128,}$ which
			is in fact the address of <span class="monoText">A[1][2]</span>.
			Generalizing this formula:
		</p>
		<p>
			<span class="topic"
				>Formula: Column-major Ordering (${0}$-based Indexing).</span
			>
			Given a matrix of order ${n \times m,}$ where ${n}$ is the number of rows
			and ${m}$ is the number of columns (i.e., ${A[n][m]}$),
		</p>
		<figure>
			<div>
				<p>${\alpha(A[i][j]) = \alpha(A[0][0]) + \omega((j \cdot n) + i)}$</p>
			</div>
		</figure>
		<p>
			Where ${\alpha(A[i][j])}$ is the address of the element ${A[i][j],}$
			${\alpha(A[0][0])}$ is the base address, ${\omega}$ is the size of the
			array's type, ${i}$ is the row index, ${m}$ is the row size, and ${j}$ is
			the column index.
		</p>

		<p>
			Like row-major ordering with 1-based indexing, we must decrement when
			considering languages that use 1-based indexing (e.g., Fortran):
		</p>

		<p>
			<span class="topic"
				>Formula: Column-major Ordering (${1}$-based Indexing).</span
			>
			Given a matrix of order ${n \times m,}$ where ${n}$ is the number of rows
			and ${m}$ is the number of columns (i.e., ${A[n][m]}$),
		</p>
		<figure>
			<div>
				<p>
					${\alpha(A[i][j]) = \alpha(A[0][0]) + \omega(((j - 1) \cdot n) + (i -
					1))}$
				</p>
			</div>
		</figure>
		<p>
			Where ${\alpha(A[i][j])}$ is the address of the element ${A[i][j],}$
			${\alpha(A[0][0])}$ is the base address, ${\omega}$ is the size of the
			array's type, ${i}$ is the row index, ${m}$ is the row size, and ${j}$ is
			the column index.
		</p>

		<p>
			Note that the only difference between column-major ordering and
			row-majoring order is that the ${i}$ and the ${j}$ are swapped in the
			formula. In essence, this means that with row-major ordering, we go from
			left to right, but with column-major ordering, we go from right to left.
			Both formulas have the same number of operations, 4, so in terms of time
			complexity, both formulas are equally efficient.
		</p>
		<p>
			Why are there two different approaches? Largely because of what earlier
			languages were designed for. And given that earlier languages play a
			significant role in inspiring future languages, these earlier approaches
			tend to stick. Fortran, for example, was designed primarily to solve
			scientific and engineering problems. In these fields, it is much more
			natural to think of matrices from a column-major perspective, since the
			general linear algebra convention is to treat matrices as concatenations
			of column-vectors. This makes it much easier to think about computations
			like matrix-vector multiplication. We can see some evidence of this by the
			fact that most languages employing column-major ordering are geared at
			scientific or statistical computing: MATLAB, Octave, R, Julia, etc.
		</p>
		<p>
			In contrast, in later languages like COBOL (which grew out of the finance
			world), it made much more sense to treat matrices as row-records.
			Accordingly, row-major ordering feels more natural. The notion of records,
			in general, made its way to languages like C (and later to C++ and
			Python).
		</p>
	</section>
</section>

<section id="tensors">
	<h2>Tensors</h2>
	<p>
		In the previous example, we saw instances of a 2-dimensional matrix. We can,
		of course, insert another dimension, leading to some data structure with the
		dimensions ${a \times b \times c.}$ This data structure represents the
		abstract data type of a <span class="term">tensor</span>. In the case of ${a
		\times b \times c,}$ the data structure represents a
		<span class="italicsText">rank-3 tensor</span>.
	</p>
	<section id="multidimensional_arrays">
		<p>
			To implement tensors, we use
			<span class="term">multidimensional arrays</span>. Let's consider a rank-3
			tensor, a tensor with the dimensions ${a \times b \times c.}$ We can
			initialize this data structure as such:
		</p>
		<pre class="language-cpp"><code>
			int main() {
				int T[3][3][3];
	
				return 0;
			}
		</code></pre>
		<p>Visually, this data structure resembles a cube:</p>
		<figure>
			<img
				src="{% static 'images/tensor_rank3.svg' %}"
				alt="Tensor of rank-3"
				loading="lazy"
				width="100px"
				height="100px"
			/>
		</figure>
		<p>
			A rank-4 tensor with the order ${3 \times 3 \times 3 \times 3,}$ is
			implemented as such:
		</p>
		<pre class="language-cpp"><code>
			int main() {
				int T[3][3][3][3];
	
				return 0;
			}
		</code></pre>
		<p>Visually, this looks like a sequence of cubes:</p>
		<figure>
			<img
				src="{% static 'images/tensor_rank4.svg' %}"
				alt="Tensor of rank-4"
				loading="lazy"
				width="100px"
				height="100px"
			/>
		</figure>
		<p>
			A rank-5 tensor with the order ${3 \times 3 \times 3 \times 3 \times 3}$
			would be implemented as such:
		</p>
		<pre class="language-cpp"><code>
			int main() {
				int T[3][3][3][3][3];
	
				return 0;
			}
		</code></pre>
		<p>
			Once we add the additional dimension, we have what looks like a matrix of
			cubes:
		</p>
		<figure>
			<img
				src="{% static 'images/tensor_rank5.svg' %}"
				alt="Tensor of rank 5"
				loading="lazy"
				width="150px"
				height="150px"
			/>
		</figure>
	</section>
	<section id="tensor_addressing">
		<h3>Tensor Addressing</h3>
		<p>
			As we saw with sequences and matrices, it's helpful to ask how the
			compiler translates an expressing indexing into a tensor. As is the case
			with matrices, there are two approaches: row-major ordering and
			column-major ordering. Let's first start by considering a sufficiently
			complex tensor, the rank-4 tensor.
		</p>
		<p>
			Suppose we wrote the expression
			<span class="monoText">T[0][2][3][0]</span>. Interpreting this from the
			array perspective, we're asking for: The first element in the fourth
			element of the third element in the first element of the array
			<span class="monoText">A</span>. Abstracting this expression, we're asking
			for the element
		</p>
		<figure>
			<div>
				<p>${T[i][j][k][l]}$</p>
			</div>
		</figure>
		<p>To keep things clean, we denote this in tensor notation:</p>
		<figure>
			<div>
				<p>${T[i][j][k][l] \equiv T_{ijkl}}$</p>
			</div>
		</figure>
		<p>
			<span class="topic">Row-major Ordering.</span>
			In row-major, we go from left to right in offsetting, starting from the
			base address. Suppose ${L_0}$ is the base address, and the array is
			initialized as ${A[d_1][d_2][d_3][d_4].}$ With row-major ordering, we
			start with the first index, ${i:}$
		</p>
		<figure>
			<div>
				<p>${\alpha(T_{ijkl}) = L_0 + (i d_1 d_2 d_3 d_4)}$</p>
			</div>
		</figure>
		<p>Next, we include the offsets from the second index, ${j:}$</p>
		<figure>
			<div>
				<p>${\alpha(T_{ijkl}) = L_0 + (i d_2 d_3 d_4 + j d_3 d_4)}$</p>
			</div>
		</figure>
		<p>Then include the offsets from the third index, ${k:}$</p>
		<figure>
			<div>
				<p>${\alpha(T_{ijkl}) = L_0 + (i d_2 d_3 d_4 + j d_3 d_4 + k d_4)}$</p>
			</div>
		</figure>
		<p>And then include the offsets from the fourth index:</p>
		<figure>
			<div>
				<p>
					${\alpha(T_{ijkl}) = L_0 + (i d_2 d_3 d_4 + j d_3 d_4 + k d_4 + l)}$
				</p>
			</div>
		</figure>
		<p>
			Finally, we multiply the resulting sum by the tensor's type, ${\omega:}$
		</p>
		<figure>
			<div>
				<p>
					${\alpha(T_{ijkl}) = L_0 + \omega(i d_2 d_3 d_4 + j d_3 d_4 + k d_4 +
					l)}$
				</p>
			</div>
		</figure>
		<p>
			<span class="topic">Column-major Ordering.</span> With column-major
			ordering, we go from right to left in offsetting. Once more, suppose
			${L_0}$ is the base address, and the array is initialized as
			${A[d_1][d_2][d_3][d_4],}$ and we are asking for the element ${A_{ijkl}.}$
		</p>
		<p>We start with the base address:</p>
		<figure>
			<div>
				<p>${A_{ijkl} = L_0}$</p>
			</div>
		</figure>
		<p>Then we include the offsets from the last index, ${l:}$</p>
		<figure>
			<div>
				<p>${A_{ijkl} = L_0 + (l d_1 d_2 d_3)}$</p>
			</div>
		</figure>
		<p>Next we include the offsets from the second-to-last index, ${k:}$</p>
		<figure>
			<div>
				<p>${A_{ijkl} = L_0 + (l d_1 d_2 d_3 + k d_1 d_2)}$</p>
			</div>
		</figure>
		<p>Then we include the offsets from the third-to-last index, ${j:}$</p>
		<figure>
			<div>
				<p>${A_{ijkl} = L_0 + (l d_1 d_2 d_3 + k d_1 d_2 + j d_1)}$</p>
			</div>
		</figure>
		<p>And then include the offsets from the last index, ${i:}$</p>
		<figure>
			<div>
				<p>${A_{ijkl} = L_0 + (l d_1 d_2 d_3 + k d_1 d_2 + j d_1 + i)}$</p>
			</div>
		</figure>
		<p>Finally, multiply the tensor's type, ${\omega:}$</p>
		<figure>
			<div>
				<p>
					${A_{ijkl} = L_0 + \omega(l d_1 d_2 d_3 + k d_1 d_2 + j d_1 + i)}$
				</p>
			</div>
		</figure>
	</section>

	<section id="general_formula">
		<h3>General Formula for Addressing</h3>
		<p>
			After the above analysis, we've likely observed a pattern from addressing.
			Indeed, this pattern evidences the fact that all numbers are tensors (a
			tensor of rank-0), all sequences are tensors (a tensor of rank-1), and all
			matrices are tensors (a tensor of rank-2). Accordingly, we can construct a
			general formula for determining the address of a tensor. First, let's look
			back at the 4-rank tensor's formula:
		</p>
		<figure>
			<div>
				<p>
					${\alpha(T_{ijkl}) = L_0 + \omega(i d_2 d_3 d_4 + j d_3 d_4 + k d_4 +
					l)}$
				</p>
			</div>
		</figure>
		<p>We will rewrite this formula as such:</p>
		<figure>
			<div>
				<p>
					${\alpha(T[i_1][i_2][i_3][i_4]) = L_0 + \omega(i_1 d_2 d_3 d_4 + i_2
					d_3 d_4 + i_3 d_4 + i_4)}$
				</p>
			</div>
		</figure>
		<p>
			There's no magic going on here. We're just changing the variable names and
			reverting back to the array notation we're familiar with in programming.
			Writing the formula in this way, we can more readily see that a summation
			and a product lurks beneath the surface. Applying the distributive
			property, we have:
		</p>
		<figure>
			<div>
				<p>
					${\alpha(T[i_1][i_2][i_3][i_4]) = L_0 + \omega(i_1(d_2 d_3 d_4) + i_2
					(d_3 d_4) + i_3(d_4) + i_4(1))}$
				</p>
			</div>
		</figure>
		<p>
			Grouping the terms in this manner, we see that we can express the
			computation more generally with summation and product notation:
		</p>
		<figure>
			<div>
				<p>
					${\alpha(T[i_1][i_2][i_3][i_4]) = L_0 + \omega \sum\limits_{j = 1}^{n}
					\left( i_j \cdot \prod_{k = n + 1}^{n} d_k\right)}$
				</p>
			</div>
		</figure>
		<p>
			With this general formula, we can quickly determine the row-major ordering
			formula for rank-3 tensor (a 3-dimensional array):
		</p>
		<figure>
			<div>
				<p>
					Given a tensor ${A[l][m][n],}$ the address of element ${T_{ijk}}$ is
					given by:
				</p>
				<figure>
					<div>
						<p>${\alpha(T_{ijk}) = L_0 + \omega(i m n + j n + k)}$</p>
					</div>
				</figure>
			</div>
		</figure>
		<p>And for column-major ordering:</p>
		<figure>
			<div>
				<p>
					Given a tensor ${A[l][m][n],}$ the address of element ${T_{ijk}}$ is
					given by:
				</p>
				<figure>
					<div>
						<p>${T_{ijk} = L_0 + \omega(k l m + j l + i)}$</p>
					</div>
				</figure>
			</div>
		</figure>
	</section>
</section>

<section id="time_complexity">
	<h2>Time Complexity for Translation</h2>
	<p>
		Once we move beyond the sequence data type into matrices and tensors, we
		begin seeing the costs of accessing. Revisiting the 4-rank tensor, let's
		look at how many operations there (we'll ignore the addition operations):
	</p>
	<figure>
		<div>
			<p>
				${\alpha(T[i_1][i_2][i_3][i_4]) = L_0 + \omega(i_1(d_2 d_3 d_4) + i_2
				(d_3 d_4) + i_3(d_4) + i_4(1))}$
			</p>
		</div>
	</figure>
	<p>
		We see that there 3 multiplication operations for the first offset, 2
		multiplication operations for the second offset, and 1 multiplication
		operation for the third offset. Summing these:
	</p>
	<figure>
		<div>
			<p>${3 + 2 + 1 = 6}$</p>
		</div>
	</figure>
	<p>For a 5-rank tensor:</p>
	<figure>
		<div>
			<p>${4 + 3 + 2 + 1}$</p>
		</div>
	</figure>
	<p>For a 6-rank tensor:</p>
	<figure>
		<div>
			<p>${5 + 4 + 3 + 2 + 1}$</p>
		</div>
	</figure>
	<p>And so on and so forth. This expresses the sequence:</p>
	<figure>
		<div>
			<p>${\lang n-1, n-2, n-3, n-4, \ldots 3, 2, 1 \rang}$</p>
		</div>
	</figure>
	<p>
		Examining this sequence, this is just the arithmetic sequence of the
		positive integers. And as we know, the sum of this sequence is given by the
		equation:
	</p>
	<figure>
		<div>
			<p>
				${\dfrac{n(n - 1)}{2} = \dfrac{n^2}{2} - \dfrac{1}{2} = \dfrac{1}{2} n^2
				- \dfrac{1}{2}}$
			</p>
		</div>
	</figure>
	<p>
		Applying complexity analysis, we drop the constants, ${\dfrac{1}{2},}$ and
		focus only on the leading term &mdash; ${n^2.}$ Hence, accessing the
		${n}$-rank tensor has a time complexity of ${O(n^2).}$ This is bad &mdash;
		we're working in quadratic time. However, it turns out that we can rewrite
		our expression above to return a different time complexity.
	</p>
	<section id="horners_method">
		<h3>Horner's Method</h3>
		<p>
			The trick is to use an algorithm over a century old, called
			<span class="term">Horner's method</span> (also called
			<span class="italicsText">Horner's scheme</span>). Named after the British
			mathematician William George Horner, the algorithm allows us to quickly
			compute polynomials. The method is clearest through some simple algebraic
			manipulation. We focus on the just the offsetting terms for the row-major
			formula:
		</p>
		<figure>
			<div>
				<p>${i_1 d_2 d_3 d_4 + i_2 d_3 d_4 + i_3 d_4 + i_4}$</p>
			</div>
		</figure>
		<p>Applying the commutative property:</p>
		<figure>
			<div>
				<p>${i_4 + i_3 d_4 + i_2 d_3 d_4 + i_1 d_2 d_3 d_4}$</p>
			</div>
		</figure>
		<p>
			If we look closely, we see that there's a common term, ${d_4,}$ for some
			of the terms. Applying the distributive property:
		</p>
		<figure>
			<div>
				<p>${i_4 + d_4 (i_3 + i_2 d_3 + i_1 d_2 d_3 )}$</p>
			</div>
		</figure>
		<p>Then, we see that ${d_3}$ is common:</p>
		<figure>
			<div>
				<p>${i_4 + d_4 (i_3 + d_3 (i_2 + i_1 d_2))}$</p>
			</div>
		</figure>
		<p>
			It is this pattern that underlies
			<span class="italicsText">Horner's method</span>. Using this approach, we
			see that for a rank-4 tensor, there are 3 multiplication operations. For a
			rank-5 tensor, there are 4, for rank-6 there are 5, and so on. Hence, more
			generally, given a rank-n tensor, there are are ${n - 1}$ operations.
			Applying complexity analysis, we have a time complexity of order ${O(n).}$
		</p>
	</section>
</section>

<section id="special_matrices">
	<h2>Special Matrices</h2>

	<p>
		Before we examine algorithms on matrices, we consider certain kinds of
		matrices. Each of these matrices have unique properties that separate them
		from the typical ${n \times n}$ matrix. Because of these unique properties,
		some matrices are better-suited for a particular problem over others.
	</p>

	<section id="diagonal_matrix">
		<h3>Diagonal Matrix</h3>
		<p>The diagonal matrix looks like the following:</p>
		<figure>
			$$ \begin{bmatrix} 3 & 0 & 0 & 0 & 0 \\ 0 & 7 & 0 & 0 & 0 \\ 0 & 0 & 4 & 0
			& 0 \\ 0 & 0 & 0 & 9 & 0 \\ 0 & 0 & 0 & 0 & 6 \\ \end{bmatrix} $$
			<figcaption>A diagonal ${5 \times 5}$ matrix.</figcaption>
		</figure>
		<p>
			Notice the numbers cutting diagonally: 3, 7, 4, 9, and 6. More generally,
			we observe that all spaces in the matrix are occupied by a zero, except:
			those spaces that run diagonally. The diagonal line could run from the
			top-left to the bottom-right, or from top-right to bottom-left:
		</p>
		<figure>
			$$ \begin{bmatrix} 0 & 0 & 0 & 0 & 3 \\ 0 & 0 & 0 & 7 & 0 \\ 0 & 0 & 4 & 0
			& 0 \\ 0 & 9 & 0 & 0 & 0 \\ 6 & 0 & 0 & 0 & 0 \\ \end{bmatrix} $$
		</figure>
		<p>
			Let's focus on the former, where the diagonal runs from top-left to
			bottom-right. Suppose the matrix is called ${M,}$ ${i}$ is the index for
			the row elements, and ${j}$ is the index for the column elements.
			Visualizing the matrix in memory:
		</p>
		<table class="matrix">
			<thead>
				<th></th>
				<th>0</th>
				<th>1</th>
				<th>2</th>
				<th>3</th>
				<th>4</th>
				<th>${\rarr i}$</th>
			</thead>
			<tbody>
				<tr>
					<td>0</td>
					<td class="beige">3</td>
					<td>0</td>
					<td>0</td>
					<td>0</td>
					<td>0</td>
				</tr>
				<tr>
					<td>1</td>
					<td>0</td>
					<td class="beige">7</td>
					<td>0</td>
					<td>0</td>
					<td>0</td>
				</tr>
				<tr>
					<td>2</td>
					<td>0</td>
					<td>0</td>
					<td class="beige">4</td>
					<td>0</td>
					<td>0</td>
				</tr>
				<tr>
					<td>3</td>
					<td>0</td>
					<td>0</td>
					<td>0</td>
					<td class="beige">9</td>
					<td>0</td>
				</tr>
				<tr>
					<td>4</td>
					<td>0</td>
					<td>0</td>
					<td>0</td>
					<td>0</td>
					<td class="beige">6</td>
				</tr>
				<tr>
					<td>${\darr j}$</td>
				</tr>
			</tbody>
		</table>
		<p>
			Notice that when ${i = j,}$ the position ${M[i][j]}$ is occupied by an
			element other than zero. And when ${i \neq j,}$ the position ${M[i][j]}$
			is occupied by a zero. This yields the following proposition:
		</p>
		<figure>
			<div class="rule">
				<p>
					<span class="topic">Diagonal Matrix Positions.</span> In a diagonal
					matrix ${M,}$ the element ${M[i][j],}$ where ${i}$ is a row index and
					${j}$ is a column index, the following propositions hold:
				</p>
				<ul>
					<li>If ${i = j,}$ then the element ${M[i][j] \neq 0.}$</li>
					<li>If ${i \neq j,}$ then the element ${M[i][j] = 0.}$</li>
				</ul>
			</div>
		</figure>
		<p>
			Examining the diagonal matrix, we see that most of the matrix's elements
			are zero. Suppose ${b_t}$ is the number of bytes some type ${t}$ takes.
			Given a diagonal matrix ${D}$ of size ${n \times n,}$ the amount of memory
			the matrix ${D}$ takes is given by the function ${m(D)}$:
		</p>
		<figure>
			<div>
				<p>${m(D) = b_t \cdot n^2}$</p>
			</div>
		</figure>
		<p>
			For example, in a C compiler where an <var>int</var> takes ${4~\text{B}}$
			of memory, a matrix of size ${5 \times 5}$ would consume:
		</p>
		<figure>
			<div>
				<p>${m(D) = b_t \cdot n^2 = 4(5)^2 = 100}$</p>
			</div>
		</figure>
		<p>
			bytes of memory. This can be a significant performance issue. For many
			problems involving diagonal matrices, the only relevant, or useful, data
			are the elements occupying the diagonal. Adding or multiplying two
			diagonal matrices, the zeroes are left unchanged. The only relevant pieces
			of information are the non-zero elements. Because each row contains only
			one useful element, we can quantify the amount of wasted space:
		</p>
		<figure>
			<div>
				<p>${w(D) = m(D) - b_t(n)}$</p>
			</div>
		</figure>
		<p>
			Where ${w(D)}$ is a function returning the amount of wasted memory,
			${m(D)}$ is a function returning the total amount of memory consumed,
			${b_t}$ is the number of bytes consumed by a type ${t,}$ and ${n}$ is the
			number of rows.
		</p>
		<p>
			Thus, of the ${100}$ bytes used to store the matrix ${M,}$ only ${4 \cdot
			5 = 20}$ bytes are actually needed. The remaining ${80}$ bytes are wasted.
			We can avoid this problem by converting the diagonal of matrix ${M}$ into
			an array of size ${n.}$ In other words, the matrix:
		</p>
		<div id="matrix2"></div>
		<p>becomes the array:</p>
		<div class="compare">
			<div id="matrix2Array"></div>
		</div>
		<p>
			How do we store the non-zero elements of the matrix ${M}$ in the array
			${A?}$ The first step, on our part, is to determine the mapping pattern.
			Examining the array and the matrix together, we see the following mapping:
		</p>
		<div class="func">
			<ul>
				<li>${M[i][j]}$</li>
				<li>${A[k]}$</li>
			</ul>
			<ul>
				<li>${M[0][0] = 3}$</li>
				<li>${A[0] = 3}$</li>
			</ul>
			<ul>
				<li>${M[1][1] = 7}$</li>
				<li>${A[1] = 7}$</li>
			</ul>
			<ul>
				<li>${M[2][2] = 4}$</li>
				<li>${A[2] = 4}$</li>
			</ul>
			<ul>
				<li>${M[3][3] = 9}$</li>
				<li>${A[3] = 9}$</li>
			</ul>
			<ul>
				<li>${M[4][4] = 6}$</li>
				<li>${A[4] = 6}$</li>
			</ul>
		</div>
		<p>Based on this mapping, we have the following implementation:</p>
		<ol class="alg">
			<li>${f}$ diagonalMatrixToArray(matrix M[]):</li>
			<ol>
				<li>let n = M[0].size;</li>
				<li>let A = new array A[n];</li>
				<li>int i, j, k;</li>
				<li>for (k = 0, i = 0; i < n; i++, k++):</li>
				<ol>
					<li>for (j = 0; j < n; j++):</li>
					<ol>
						<li>if (i = j):</li>
						<ol>
							<li>A[k] = M[i][j];</li>
						</ol>
						<li>else: continue</li>
					</ol>
				</ol>
				<li>return A[];</li>
			</ol>
		</ol>
		<p></p>
		<div class="demo">
			<div class="options">
				<button>Java</button>
				<button>Python</button>
			</div>
			<div class="implementation">
				<p>Here's an implementation in Java.</p>
				<pre class="language-java"><code>
					import java.util.Arrays;

					class matrix {
						public static int[] flatten(int[][] m) {
							int n = m[0].length;
							int arr[] = new int[n];
							for (int i = 0; i < n; i++) {
								for (int j = 0; j < n; j++) {
									if (i == j) {arr[i] = m[i][j];}
									else {continue;}
								}
							}
							return arr;
						}
					}
					public class diagonalMatrix {
						public static void main(String[] args) {
							int[][] M = &lcub;{1, 0, 0}, {0, 2, 0}, {0, 0, 3}};
							int[] A = matrix.flatten(M);
							System.out.println(Arrays.toString(A));
						}
					}
				</code></pre>
			</div>
			<div class="implementation">
				<p>Here's an implementation in Python.</p>
				<pre class="language-python"><code>
					def flattenDiagonally(M):
						arr = []
						for i in range(len(M)):
							for j in range(len(M[i])):
								if (i == j): arr.append(M[i][j])
						return arr
	
					matrix = [[1,0,0],[0,2,0],[0,0,3]]
					matrixArr = flattenDiagonally(matrix)
	
					for i in matrixArr:
						print(i)
				</code></pre>
				<pre class="language-bash"><code>
					1
					2
					3
				</code></pre>
			</div>
		</div>
	</section>

	<section id="lower_triangular_matrix">
		<h3>Lower Triangular Matrix</h3>
		<p>A <b>lower triangular matrix</b> looks like the following:</p>
		<figure>
			$$ \begin{bmatrix} a_{0_0} & 0 & 0 & 0 & 0 \\ a_{1_0} & a_{1_1} & 0 & 0 &
			0 \\ a_{2_0} & a_{2_1} & a_{2_2} & 0 & 0 \\ a_{3_0} & a_{3_1} & a_{3_2} &
			a_{3_3} & 0 \\ a_{4_0} & a_{4_1} & a_{4_2} & a_{4_3} & a_{4_4} \\
			\end{bmatrix} $$
		</figure>
		<p>
			Needless to say, this special matrix's name is self-descriptive. The
			matrix is essentially cut diagonally, forming two triangles. The lower
			triangle is occupied by non-zero values (represented above with the
			variable ${a}$), and the upper triangle is occupied by zeroes. Examining
			the non-zero elements:
		</p>
		<div class="func">
			<ul>
				<li>Element</li>
				<li>${i}$</li>
				<li>${j}$</li>
				<li>Relation</li>
			</ul>
			<ul>
				<li>${a_{0_0}}$</li>
				<li>0</li>
				<li>0</li>
				<li>${i = j}$</li>
			</ul>
			<ul>
				<li>${a_{1_0}}$</li>
				<li>1</li>
				<li>0</li>
				<li>${i > j}$</li>
			</ul>
			<ul>
				<li>${a_{1_1}}$</li>
				<li>1</li>
				<li>1</li>
				<li>${i = j}$</li>
			</ul>
			<ul>
				<li>${a_{2_0}}$</li>
				<li>2</li>
				<li>0</li>
				<li>${i > j}$</li>
			</ul>
			<ul>
				<li>${a_{2_1}}$</li>
				<li>2</li>
				<li>1</li>
				<li>${i > j}$</li>
			</ul>
			<ul>
				<li>${a_{2_2}}$</li>
				<li>2</li>
				<li>2</li>
				<li>${i = j}$</li>
			</ul>
			<ul>
				<li>${a_{3_0}}$</li>
				<li>3</li>
				<li>0</li>
				<li>${i > j}$</li>
			</ul>
			<ul>
				<li>${a_{3_1}}$</li>
				<li>3</li>
				<li>1</li>
				<li>${i > j}$</li>
			</ul>
			<ul>
				<li>${a_{3_2}}$</li>
				<li>3</li>
				<li>2</li>
				<li>${i > j}$</li>
			</ul>
			<ul>
				<li>${a_{3_3}}$</li>
				<li>3</li>
				<li>3</li>
				<li>${i = j}$</li>
			</ul>
			<ul>
				<li>&vellip;</li>
				<li>&vellip;</li>
				<li>&vellip;</li>
				<li>&vellip;</li>
			</ul>
		</div>
		<p>
			If we look at the relations between ${i}$ and ${j,}$ we see that for the
			non-zero elements, ${i}$ is <em>always</em> greater than or equal to
			${j.}$ Everything else is zero. Stated another way: If ${i < j,}$ then the
			element ${a_{i_j} = 0.}$ Hence, we have the following conditions:
		</p>
		<figure>
			<div>
				<ol class="numd">
					<li>${(i < j) \implies a_{i_j} = 0}$</li>
					<li>${(i \geq j) \implies a_{i_j} \neq 0}$</li>
				</ol>
			</div>
		</figure>
		<p>
			As we saw with the diagonal matrix, depending on what we're doing with the
			lower triangular matrix, there's potentially wasted memory. Here, we see a
			pattern to the zero elements. In the first row there are four zeroes, in
			the second there are three, in the third there are two, and in the fourth
			there is one. This is an arithmetic sequence:
		</p>
		<figure>
			<div>
				<p>${\lang 4, 3, 2, 1 \rang}$</p>
			</div>
		</figure>
		<p>The arithmetic series:</p>
		<figure>
			<div>
				<p>${1 + 2 + 3 + 4 = 10}$</p>
			</div>
		</figure>
		<p>We can generalize this formula:</p>
		<figure>
			<div>
				<p>${\sum\limits_{k = 0}^{n-1} k = \dfrac{n(n-1)}{2}}$</p>
			</div>
		</figure>
		<p>
			Thus, given an ${n \times n}$ matrix, the amount of memory used by the
			zero elements is:
		</p>
		<figure>
			<div>
				<p>${m_w(T) = b_t \left(\dfrac{n(n-1)}{2}\right)}$</p>
			</div>
			<figcaption>
				Where ${m_w(T)}$ is a function of the wasted memory, ${T}$ is some lower
				triangular matrix of size ${n \times n,}$ and ${b_t}$ is the amount of
				bytes consumed by the matrix type ${t.}$
			</figcaption>
		</figure>
		<p>
			Because there are ${n^2}$ elements, the amount of memory needed for the
			non-zero elements is:
		</p>
		<figure>
			$$ \begin{aligned} M(T) &= n^2 - \dfrac{n(n-1)}{2} \\[1em] &= \dfrac{2n^2
			- n^2 + n}{2} \\[1em] &= \dfrac{n^2 + n}{2} \end{aligned} $$
		</figure>
		<p>
			We can confirm that this is correct by observing that the sum of all the
			non-zero elements is the arithmetic series of the positive integers:
		</p>
		<figure>
			$$ \begin{aligned} \sum\limits_{k = 0}^{n} k &= 1 + 2 + 3 + \ldots + n
			\\[1em] &= \dfrac{n(n + 1)}{2} \end{aligned} $$
		</figure>
	</section>
</section>
<script src="../../../static/numerc/array.js"></script>
{% endblock %}

