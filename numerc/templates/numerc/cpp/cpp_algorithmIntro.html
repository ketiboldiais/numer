{% extends '../layout.html' %} {% load static %} {% block description %}
<meta
	name="description"
	content="Introduction to C++ algorithms, data structures versus abstract types"
/>
{% endblock %} {% block title %}
<title>C++ Introduction to Algorithms</title>
{% endblock %} {% block content %}
<h1>Algorithms: Prelude</h1>
<section id="alg_intro">
	<p>
		An <span class="term">algorithm</span> is a solution to a
		<span class="term">computational problem</span>. As a solution, it is a
		sequence of computational steps that consumes a set of
		<span class="term">inputs</span> and produces a set of
		<span class="term">outputs</span>.
	</p>

	<p>
		Because of this definition, the way a problem is stated determines the
		relationship between inputs and outputs. We will use certain notations to
		state problems. For example, if the problem asks, "Sort the numbers into
		non-decreasing order," we would write:
	</p>

	<figure class="math-display">
		<div class="rule">
			<p>${s \coloneqq}$ ${\lang a_1, a_2, \ldots, a_n \rang}$</p>
			<p>
				${f(s) \coloneqq}$ ${\lang a_1', a_2', \ldots, a_n' \rang}$ : ${a_1'
				\leq a_2' \leq \ldots \leq a_n'}$
			</p>
		</div>
	</figure>

	<p>
		In the notation above, we use ${s}$ to denote the input. Of course, we are
		free to use whatever variable we would like. We use ${f(s)}$ to denote the
		output, where ${f}$ is the actual algorithm itself. We can use any other
		variable. For example, ${r}$ and ${t(r),}$ ${g}$ and ${h(g).}$ The symbol
		${\coloneqq}$ reads "defined as," and the colon is read "such that."
		Finally, the brackets ${\lang \ldots \rang}$ indicate a sequence.
	</p>

	<p>
		The problem above is an example of a
		<span class="term">sorting problem</span>. This is a particularly important
		problem in computer science, because numerous algorithms rely on its
		solution (for example, many
		<span class="italicsText">search algorithms</span> assume values are
		sorted).
	</p>

	<p>
		When we write algorithms, we want our algorithms to be
		<span class="italicsText">correct</span>. An algorithm is correct if, and
		only if, it halts at the correct output (in other words, the algorithm stops
		the moment it finds a correct output). An incorrect algorithm is one that
		halts at the incorrect output, or one that never halts.
	</p>

	<p>
		<span class="topic">Performance is currency.</span> At the end of the day,
		we study algorithms because of performance. With efficient and correct
		algorithms, we can obtain other things &mdash; security and features. Some
		programmers prefer Python over C, even if C is so much faster. Why? For a
		variety of reasons &mdash; Python is more readable, more user-friendly, more
		widely used, more abstracted, and so on. The cost, however, is performance.
		Python's memory management algorithms may not be the best approach for our
		problem. Python's garbage collection algorithms may be not how we would like
		our data handled. However, we may be willing to take on those costs because
		we've written well-performing algorithms.
	</p>
</section>

<section id="efficiency">
	<h2>Efficiency</h2>
	<p>
		One of the core concerns for algorithms is
		<span class="term">efficiency</span>. For now, let's consider efficiency in
		terms of time. Accordingly, the term efficiency refers to how long it takes
		an algorithm to solve a given problem. How do we measure this length of
		time?
	</p>
	<p>
		The most obvious way is to write some sort of stop watch program, where the
		timer begins once the algorithm commences, and stops when the algorithm
		finishes. We then compare that time to other algorithm times.
	</p>
	<p>
		But what's the problem with this approach? The metrics would be entirely
		machine dependent! The time it takes to perform algorithm ${x}$ on a Macbook
		Pro would be different from the time it takes to perform ${x}$ on a
		Raspberry Pi, even if we're measuring the same algorithm and using the same
		data set. Because of this limitation, we must measure &#8220;speed&#8221;
		with a different metric.
	</p>
	<p>
		Instead of measuring speed with time, we assume that every step a computer
		takes consumes a constant amount of time, then we count how many operations,
		or steps, the algorithm takes. In doing so, an algorithm's efficiency is not
		so much a measure of <span class="italicsText">how long</span> the algorithm
		takes to solve a problem, but
		<span class="italicsText">how many steps</span> the algorithm takes.
	</p>
	<p>
		To measure the number of steps an algorithm takes to solve a given problem,
		we perform <span class="term">asymptotic analysis</span>. The subsequent
		comments provide a big-picture view of asymptotic analysis.
	</p>

	<section id="asymptotic_complexity">
		<h3>Asymptotic Complexity</h3>
		<p>
			Asymptotic complexity is the tool used by computer scientists and
			mathematicians for measuring algorithmic efficiency. The formal
			definition:
		</p>
		<figure class="math-display">
			<div class="rule">
				<p>
					<span class="topic">Asymptotic Complexity.</span> A function ${f(x)}$
					runs on the order of ${g(x)}$ if there exists some ${x}$ value ${x_0}$
					and some constant ${C}$ for which ${f(x) \leq C \cdot g(x)}$ for all
					${x > x_0.}$
				</p>
			</div>
		</figure>
		<p>
			At its core, asymptotic complexity is a measure of how fast a program's
			runtime grows asymptotically. This is equivalent to asking: As the size of
			${f(x)}$'s domain (its inputs) tends towards infinity, how does the
			runtime of the program grow? Consider the example of counting the number
			of characters in this string:
		</p>
		<figure class="math-display">
			<div>
				<p><span class="monoText">"hegel"</span></p>
			</div>
		</figure>
		<p>
			If we used the algorithm of counting each character one at a time, we
			would count ${(h, 1),}$ ${(e, 2),}$ ${(g, 3),}$ ${(e, 4),}$ and ${(l,
			5).}$ We say that this algorithm runs on
			<span class="italicsText">linear time</span> with respect to the number of
			characters. Why? Because it takes a constant amount of time to count one
			character, and as such, the number of characters is directly proportional
			to the amount of time it takes to count the number of characters in the
			string. As the number of characters increases, the runtime increases
			linearly with the number of characters (the input). Denoting the number of
			characters with the variable ${n,}$ we say that the character-counting
			algorithm has a complexity of order ${O(n).}$
		</p>
	</section>

	<section id="classes_or_runtime">
		<h3>Functions of Runtime</h3>
		<p>
			There are several classes of runtime that we're most concerned with the
			analysis of algorithms. We cover each of them below.
		</p>

		<section id="constant_time">
			<p>
				<span class="topic">Constant Time: ${O(1).}$</span> If an algorithm
				always takes a certain amount of time regardless of the size of its
				input, we say that the algorithm's running time is
				<span class="term">constant</span>.
			</p>
		</section>

		<section id="logarithmic_time">
			<p>
				<span class="topic">Logarithmic Time: ${O(\log n).}$</span> If an
				algorithm's running time grows logarithmically with increasing inputs,
				we say that the algorithm's running time is
				<span class="term">logarithmic</span>. Logarithmic time is most often
				found with algorithms that solve a large problem by dividing it into a
				series of smaller and smaller problems.
			</p>
		</section>

		<section id="linear_time">
			<p>
				<span class="topic">Linear Time: ${O(1).}$</span> If an algorithm's
				running time increases in direct proportion to its inputs, we say that
				the algorithm's running time is <span class="term">linear</span>. Linear
				time is most often found with algorithms that must perform some
				processing on every input one by one. For example, if the number of
				inputs is ${n = 1000,}$ then the algorithm takes ${1000}$ operations to
				complete. Such an algorithm runs on linear time.
			</p>
		</section>

		<section id="linearithmic_time">
			<p>
				<span class="topic">Linearithmic Time: ${O(n \log n).}$</span> Some
				algorithms break problems into smaller problems, solve them
				individually, then combine the solutions. These algorithms' running time
				is <span class="term">linearithmic</span>.
			</p>
		</section>

		<section id="quadratic_time">
			<p>
				<span class="topic">Quadratic Time: ${O(n^2).}$</span> Algorithms that
				require a nested loop will almost always have
				<span class="term">quadratic</span> running times. Needless to say,
				quadratic time is not the most efficient, and it is generally restricted
				to small problems.
			</p>
		</section>

		<section id="cubic_time">
			<p>
				<span class="topic">Cubic Time: ${O(n^3).}$</span> Like quadratic time,
				algorithms requiring loops nested within nested loops almost assuredly
				have <span class="term">cubic</span> running times. Like algorithms of
				order ${O(n^2),}$ algorithms of order ${O(n^3)}$ are generally relegated
				to small problems.
			</p>
		</section>

		<section id="exponential_time">
			<p>
				<span class="topic">Exponential Time: ${O(2^n).}$</span> Algorithms with
				<span class="term">exponential</span> running times are those where
				every input results in an exponential increase in running time. For
				example, where ${n = 2,}$ the running time is ${4,}$ and where ${n =
				20,}$ the running time is ${1,000,000.}$ Exponential running times are
				most often found with brute-force algorithms.
			</p>
		</section>
	</section>
</section>

<section id="data_structures_v_abstract_data_types">
	<p>
		<span class="topic">Data Structures v. Abstract Data Types.</span> After
		programming for some time, we might ask, what is the difference between a
		data structure and an abstract data type?
	</p>
	<p>
		A <span class="italicsText">data structure</span> is a collection. It
		contains data, relationships between data, and operations to be applied to
		the data. Data structures are very explicity &mdash; they state exactly what
		and how tasks are performed.
	</p>
	<p>
		In contrast, an <span class="italicsText">abstract data type</span> is
		something that defines a particular datum's behavior from the perspective of
		its user. In creating an abstract data type, we only describe what task must
		be done. We do not explicitly state how that that task should be done.
	</p>
</section>
{% endblock %}
