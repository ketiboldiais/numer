{% extends '../layout.html' %} {% load static %} 

{% block description %}
<meta
	name="description"
	content="An overview of hashing: hashes, hash tables, hash functions, key spaces, collisions, and probing."
/>
{% endblock %} {% block title %}
<title>Hashing</title>
{% endblock %} 


{% block content %}
<h1>Hashing</h1>
<section id="hashing">
	<p>
		Hashing is the process of turning a
		value of a particular type &mdash; whether it's a
		<span class="monoText">String</span>, <span class="monoText">Int</span>,
		etc. &mdash; into a numeric value. The resulting numeric value is called a
		<span class="term">hash value</span>. These hash values are then used to
		lookup values in a <span class="term">hash table</span>. To better
		understand how this works, we briefly consider some searching algorithms.
	</p>

	<p>
		Say we had some array ${A}$ containing ${n}$ elements, and we want to check
		if the array contains some value ${v}$ (let's say it does). To do so, we
		need a searching algorithm. If we use linear search, then we have a
		complexity of ${O(n).}$ This is because Swift has to look at each element,
		one by one from left to right, checking if the given element is equal to
		${v.}$
	</p>

	<p>
		Alternatively, we could use binary search. With binary search, however, the
		array's elements must be first be sorted from least to greatest. Assuming
		the elements are sorted correctly, then we have a complexity of ${O(\text{lg
		} n).}$ Swift just has to split the array in half, check which head of each
		half is closer to ${v,}$ toss the half further, and continue searching in
		the other half. Dividing over and over again, Swift eventually finds ${v.}$
		Mathematically, this takes ${\text{lg } n}$ steps.
		<span class="marginnote"
			>For those unfamiliar, the symbol ${\text{lg}}$ is equivalent to
			${\log_{2}.}$ We use ${\text{lg}}$ rather than ${\log}$ because ${\log}$
			is usually reserved for ${\log_{10},}$ the common logarithm. ${\text{lg}}$
			is the binary logarithm.</span
		>
	</p>

	<p>
		Comparing the two, binary search is much faster &mdash; linear time is
		generally slower than logarithmic time. The catch, of course, is that the
		elements in the array must first be sorted. Now, even though logarithmic
		time is pretty good, constant time, ${O(1),}$ is even better. It is this
		goal, achieving ${O(1),}$ that sparks the need for hashing.
	</p>

	<p>
		With hashing, we want to transform values into some number corresponding to
		an index in the array. For example, if we had the elements 8, 4, 9, 11, and
		3. We call these elements <span class="term">keys</span>. The first idea:
		store each key at a particular index, rather than just blindly storing them
		as presented. 8 goes to index 8, 4 goes to index 4, 9 goes to index 9, 11 to
		11, 3 to 3, and so on. If the value we're looking for is 8, we tell Swift to
		look at index 8. If it's 4, look at 4. Notice what this leads to in terms of
		complexity. Swift just needs to take one step, like accessing an element in
		the array. This is a complexity of order ${O(1)}$ &mdash; constant time.
	</p>

	<p>
		The problem, however, is when we have a key like 8435. This is very far down
		the array. Worse, there are a lot of spaces wasted in the array. With our
		original keys, 11 was the greatest, so there are 8422 positions left empty
		(or more accurately, 8429, since there are 7 empty positions before 11).
		This is a lot of wasted space. It seems that by achieving ${O(1),}$ we did
		so at a gigantic cost in memory. So now we have another goal &mdash;
		reducing memory costs while maintaing ${O(1).}$
	</p>

	<p>
		The solution is to use a mathematical model. To do so, we need a function,
		more specifically called a <span class="term">hash function</span>. Before
		we consider the hash function, we should lay out the problem thus far, and
		use more specific terminology. First, the keys, all together, form a
		<span class="term">key space</span>. Second, we'll refer to the array we
		mentioned previously as a <span class="term">hash table</span>. Third, there
		is some hash function ${h(x) = x}$ that maps elements of the key space to
		elements of the hash table. E.g., ${h(8) = 8,}$ ${h(11) = 11,}$ and so on.
	</p>

	<figure>
		<img
			src="{% static 'images/keyspace.svg' %}"
			alt="keyspace and hashtable"
			loading="lazy"
			class="forty-p"
		/>
	</figure>

	<p>
		Applying the hash function to all the elements in the key space, the final
		result looks like:
	</p>

	<figure>
		<img
			src="{% static 'images/keyspace2.svg' %}"
			alt="keyspace"
			loading="lazy"
			class="forty-p"
		/>
	</figure>

	<p>
		Again, notice the empty spaces in the hash table. Let's think more about
		that hash function. Mathematically, functions consist of two types: (1)
		<span class="italicsText">one-to-one functions</span>, and (2)
		<span class="italicsText">many-to-one functions</span>. Our function above,
		${h(x) = x,}$ is a one-to-one type function. Keeping this detail in mind, we
		want to rewrite our hash function such that we use the empty positions in
		the hash table.
	</p>

	<p>
		How might we do so? Well, looking at the hash table above, it has a size of
		12. One possible definition for ${h(x):}$
	</p>

	<figure class="math-display">
		<div>
			<p>${h(x) = x \bmod s}$</p>
			<ul class="def">
				<li class="where">${s}$ is the size of the hash table.</li>
				<li></li>
			</ul>
		</div>
	</figure>

	<p>With this definition, let's say our key space now includes 13:</p>

	<figure>
		<img
			src="{% static 'images/keyspace3.svg' %}"
			alt="keyspace"
			loading="lazy"
			class="sixty-p"
		/>
	</figure>

	<p>
		Great, it works. We are now actually using the empty spaces. But there's a
		problem lingering &mdash; what happens when we the keyspace includes 1?
		${h(1) = 1 \bmod 12 = 1.}$ But 1 is already occupied by 13. We can have 1
		map to 1, but that would result in a <span class="term">collision</span>. We
		do not want collisions. So now we have another problem to solve: How do we
		avoid collisions?
	</p>

	<p>
		It turns out there are a lot of different methods to for avoiding or
		removing collisions. Generally, they fall into two categories: (1)
		<span class="term">open hashing</span>, and (2)
		<span class="term">closed hashing</span>. Within open hashing, we have the
		method of <span class="term">chaining</span>. Within closed hashing, we have
		a category of methods called <span class="term">open addressing</span>,
		which itself consists of <span class="term">linear probing</span> and
		<span class="term">quadratic probing</span>. These are just a few methods.
	</p>

	<p>
		Let's first consider chaining. With chaining, we use our usual hash
		function. But instead of simply inserting the key into an empty position in
		the hash table, we insert the key
		<span class="italicsText">and</span> include a pointer. In other words, we
		start a chain. If we encounter a key that maps to an occupied position in
		the hash table, the key is chained to the occupying key by having the
		occupying key's pointer point to the subsequent key. For example, let's say
		our key space now includes 1:
	</p>

	<figure>
		<img
			src="{% static 'images/keyspace4.svg' %}"
			alt="keyspace"
			loading="lazy"
			class="seventy-p"
		/>
	</figure>

	<p>
		The chain above is really just a
		<span class="italicsText">linked list</span>. With linked lists, we solve
		the collision proble. But, in doing so, we've increased complexity. On
		average, this is still faster than ${O(\text{lg } n),}$ but if we have to
		search for 1, we'll have to search through the linked list as well. If the
		chain grows too long, then we potentially hit ${O(n).}$ Moreover, there are
		open positions in the hash table, but instead of using them, we're adding to
		chains.
	</p>

	<p>
		Let's consider another method, <span class="term">linear probing</span>.
	</p>
</section>
{% endblock %}
