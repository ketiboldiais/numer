{% extends '../layout.html' %} {% load static %} {% block description %}
<meta
	name="description"
	content="C++ data structures: Dynamic vs. static memory, heap, stack, overflows, arrays, linked-lists."
/>
{% endblock %} {% block title %}
<title>C++ Data Structures</title>
{% endblock %} {% block content %}

<h1>Data Structures & Algorithms: An Introduction</h1>
<section id="introduction">
	<p>
		In earlier sections, we defined a program as an ordered sequence of
		instructions for solving a problem. We now further specify that definition:
		A program is a set of instructions for performing computations with a given
		collection of data. Using this definition, we can now see how closely linked
		algorithms and data structures are. Without data, we cannot write programs,
		and without programs, we cannot meaningfully use data.
	</p>
	<p>
		<span class="term">Data</span>, broadly speaking, are representations of
		real world information. Specific to computer science, they are
		representations of information that computer can understand. For example, a
		computer cannot determine whether two people are &#8220;attracted&#8221; to
		one another, unless we feed the computer data: age difference, shared
		properties like mutual interests, friends, and alma matter, favorite songs,
		as well preferences. A <span class="term">data structure</span> is an
		arrangement of all that collected data.
	</p>
	<p>
		Whenever we program, we are actually creating two files: A
		<span class="term">program file</span>, and a
		<span class="term">data file</span>. To understand the roles for these two
		files, let's consider what happens when a user opens a Word Document. On
		double clicking the Word document's icon, the Word program (the program
		file) is loaded into main memory. Once loaded into the main memory, the CPU
		begins executing the Word program's source code. This is the point where we
		start seeing things on the screen; if the program is coded well, it should
		be almost instantaneous.<sup></sup>
		Now, if we clicked on a file icon (some file with the extension
		<span class="monoText">.docx</span>), we would see all the that file's
		contents as well as presets. This <span class="monoText">.docx</span> file
		is a data file, and it too is loaded into main memory. All the text,
		formatting, and presets we see in the Word file is the result of the Word
		program operating on the
		<span class="italicsText">data structure</span> contained in the data file.
	</p>
	<div class="note">
		<p>
			For poorly code programs, we might notice the twirling beach ball on a
			Mac, slow loading times, or the worst of all, a crash.
		</p>
	</div>
	<p>
		What does this example tell us? It tells us that every program handles data
		in some form or another. Even the simplest programs handle data &mdash; the
		variables initialized, the functions called, the arrays declared, it's all
		data.
	</p>
	<p>
		How we arrange the data (i.e., our choice of data structure) dramatically
		impacts our program, whether that's in terms of efficiency, security,
		user-friendliness, or maintainability. Data structures impact how well a
		computer handles the program, since data is loaded into main memory at
		runtime. If the data structure is too memory intensive, then our program's
		user base is necessarily limited. Apple's XCode IDE, the Unreal game engine,
		Blender, and high definition games like GTA V or Fallout 4 are all heavy,
		resource intensive, but relatively efficient, programs. They all work well
		on computers with large amounts of RAM (i.e., at least 8GB), but anything
		lower, and the programs start getting into trouble. In contrast, programs
		like Sublime (with default settings) or TextEdit consume very little
		resources, but come with less features.
	</p>
	<p>
		The decisions above are fine, but only if they are intentional. Maybe the
		programmers decided the features were more important, so the user has to
		decide if they really want them. Or maybe the programmer decided efficiency
		was more important, so it's up to the user to modify and tweak the program
		with extensions. Nevertheless, some decisions are worse than others. No one
		wants a program that offers a useless feature and hogs large amounts of
		memory. Similarly, a user would be fairly reluctant to use an extremely
		efficient program if the program has enormous security risks.<sup></sup>
		Along the margins, if two programs offer the same features, the user is
		usually going to decide based on some other criteria &mdash; efficiency,
		reliability, user-friendliness, cheapness, etc. &mdash; all tradeoffs
		determined, at the lowest level, by data structure choices.
	</p>
	<div class="note">
		<p>
			Given how valuable of a commodity security has become in the modern era,
			the operating system would likely prevent even the most efficient and
			feature-filled programs from running.
		</p>
	</div>
	<p>
		A data structure should not be confused with a
		<span class="term">database</span>. A database is a collection of related
		data stored in some form of permanent storage (perhaps an HDD, an SD, a USB,
		floppy disk) for ease of access by a program. Most databases are organized
		as tables. The database, however, is really just a crude way of storing
		data. We can think of it like a giant container filled (but organized) with
		numerous, different pieces of wood, metal, nails, screws, glass, etc. To
		build anything useful, however, we take the pieces we need, and arrange them
		in our own way. The glass is separated, the wood is stacked, the nails in
		one pouch, the screws in another. The same goes for data structures. The
		program must retrieve the data from somewhere (i.e., a database), but the
		data must still be arranged into a data structure. The database is really
		just there for the program to easily retrieve data for structuring.
	</p>
	<div class="note">
		<p>
			The distinguishing trait between a data structure and a database: A data
			structure answers the question, How do I efficiently and correctly store
			this data in main memory (i.e., RAM), so that a program can use it easily?
			A database answers the question, How do I efficiently and correctly store
			this data in permanent memory (i.e., an HDD or SD), so that a program can
			retrieve it easily?
		</p>
	</div>
	<p>
		Following this distinction between database and data structure, there is a
		further distinction within databases. Data stored in databases generally
		fall into two categories &mdash;
		<span class="term">operational data</span> and
		<span class="term">legacy data</span>. Generally, operational data is new
		and frequently-used data. For example, a new user registering for a site, or
		a user frequently using the site. Both users' data constitutes operational
		data. In contrast, legacy data is data that is old, or not frequently used.
		For example, that Gmail account we forgot the password to fifteen years ago.
		It'd probably be fairly difficult to reset the password for the account
		because the data has been stored in a
		<span class="term">data warehouse</span> &mdash; a database specifically
		made for legacy data.<sup></sup> As one might suspect, databases and data
		warehouses are more commonly used by large, commercial programs, where there
		are enough users to justify the costs of storage and maintenance.<sup></sup>
	</p>
	<div class="note">
		<p>
			Developers tend not to want users touching or requesting legacy data. Most
			data warehouses are simply arrays of disks, making retrieval costly.
			Moreover, data warehouses are generally intended for the developer. They
			are helpful in identifying factors like user behavior and trends; facts
			crucial for making business decisions. The algorithms used for analyzing
			such legacy data are called
			<span class="term">data mining algorithms</span>.
		</p>
		<p>
			The term <span class="term">big data</span> refers to the vast amount of
			data now available via the internet. Analyzing that data, with clever data
			structures and algorithms, can lead to breakthrough insights, some of
			which spawn new programs, companies, laws, and even entire markets.
		</p>
	</div>
	<p>
		In sum and crudely: A data structure is an arrangement of data in main
		memory. A database is an arrangmenet of data in permanent memory. A data
		warehouse is an arrangement of data in a collection of permanent memory.
	</p>
</section>

<section id="memory_allocation">
	<h2>Static v. Dynamic Memory Allocation</h2>
	<p>
		As we saw in the previous section, data structures concern the arrangement
		of data in main memory. We now want to delve deeper into how the data is
		arranged. To do so, we need a clearer perspective on main memory, and the
		way memory is allocated.
	</p>
	<p>
		Conceptually, we can think of main memory as a large grid of small squares.
	</p>
	<figure>
		<img
			src="{% static 'images/cpp_mainMemory.svg' %}"
			alt="a grid of memory"
			loading="lazy"
			class="twenty-p"
		/>
	</figure>
	<p>
		Each of the small squares are units with addresses. We call the unit a
		<span class="term">byte</span>, and the address a
		<span class="term">memory address</span>. The memory addresses are linear,
		and arranged in order:
	</p>
	<figure>
		<img
			src="{% static 'images/cpp_mainMemory1.svg' %}"
			alt="a grid of memory"
			loading="lazy"
			class="seventy-p"
		/>
	</figure>
	<p>
		The sum of all the individual units, or bytes, is the total amount of memory
		we have. For example, if a program consumes 64 kilobytes of memory at
		runtime, then it takes up 64,000 bytes (64,000 of the grid's squares) in
		memory. If the main memory is large (e.g., one of 8GB), then the bytes are
		sectioned into <span class="term">segments</span>. Usually, the size of a
		segment is 64KB.
	</p>
	<p>
		At the highest level, there are four large segments, sometimes called
		<span class="term">regions</span>, of memory: (1) the
		<span class="term">code segment</span>; (2) the
		<span class="term">data segment</span>; (3) the
		<span class="term">stack segment</span>; and (4) the
		<span class="term">heap segment</span>:
	</p>
	<figure>
		<img
			src="{% static 'images/cpp_memorySegments.svg' %}"
			alt="memory segments"
			loading="lazy"
		/>
	</figure>
	<p>
		There's a lot going on in the diagram above, but for now, are just focusing
		on the stack, heap, and code regions. Suppose we wrote the following code:
	</p>
	<pre class="language-cpp"><code>
		void func1(int i) {
			int a;
		}
		void func2() {
			int x;
			func1(x);
			// more code
		}
		void main() {
			int a;
			float b;
			func2();
			// more code
		}
	</code></pre>
	<p>
		Suppose further that according to the compiler, an
		<span class="monoText">int</span> takes 2 bytes, and a
		<span class="monoText">float</span> takes 4 bytes. We compile the code, and
		get back an executable. When we run the executable, the machine code is
		stored in the code region. Once there, the CPU executes, or loads, the
		machine code &mdash; the program begins using the stack and the heap. The
		variables <span class="monoText">a</span> and
		<span class="monoText">b</span> are allocated inside the stack, 6 bytes
		total. How does the CPU know to allocate 6 bytes total? With the compiler.
		The compiler determines 6 bytes are needed, and that amount is fixed.
		Because the amount of memory needed is fixed, we call this
		<span class="term">static memory allocation</span>.
	</p>
	<p>
		What about those functions? Again, the machine code is stored in the code
		region &mdash; <span class="monoText">func1()</span>'s machine code;
		<span class="monoText">func2()</span>'s machine code; and
		<span class="monoText">main()</span>'s machine code. Now, to start, the
		<span class="monoText">main()</span> function's machine code is executed
		first. As stated earlier, the variables <span class="monoText">a</span> and
		<span class="monoText">b</span> are allocated in the stack. While executing
		<span class="monoText">main()</span>,
		<span class="monoText">func2()</span> is encountered. The CPU begins
		executing <span class="monoText">func2()</span>'s machine code. In doing so,
		it encounters another variable, <span class="monoText">x</span>. Again, this
		is stored in the stack, but this time in a different
		<span class="term">stack frame</span> &mdash; a sort of layer in the stack
		on top of the layer where <span class="monoText">a</span> and
		<span class="monoText">b</span> are stored. As the CPU continues executing
		<span class="monoText">func2()</span>'s machine code, it encounters
		<span class="monoText">func1(x)</span>. The CPU begins executing
		<span class="monoText">func1()</span>'s machine code. The CPU allocates
		<span class="monoText">i</span> and <span class="monoText">a</span> to a
		memory location in a different stack.
	</p>
	<p>
		Once <span class="monoText">func1()</span> is done executing, the CPU
		<span class="italicsText">pops off</span> its stack (i.e.,
		&#8220;deletes&#8221; the stack), and continues in
		<span class="monoText">func2()</span>. Once
		<span class="monoText">func2()</span> is done executing, the CPU again pops
		off its stack, and continues in <span class="monoText">main()</span>.
		Finally, once the CPU finishes executing
		<span class="monoText">main()</span>, it pops off its stack &mdash; the
		program has finished.
	</p>
	<p>
		Notice what this discussion entails. How much memory a function takes up
		depends on its variables. And how much memory variables take up depend on
		their sizes, which are in turn determined by the compiler.<sup></sup> This
		means that we, the programmers, have little to no control over &mdash; (a)
		the automatic destruction of stacks and (b) how much memory is allocated for
		an initialized variable. The rules are different, however, for variables
		that end up in the <span class="term">heap</span>.
	</p>
	<div class="note">
		<p>This demonstrates why optimization is critical in compiler design.</p>
	</div>
	<p>
		The Oxford Dictionary defines a &#8220;heap&#8221; as a disorderly
		collection of objects haphazardly on top of each other. This captures the
		essence of the heap segment in memory. The heap is an open field of memory,
		where values can be stored. The difference: everything allocated into the
		heap is free to move around. We can think of the heap as this chaotic, open
		field of memory &mdash; everything in it is crawling, walking, and running
		in every direction. This is in contrast to the stack, where values are nice
		and organized into small neighborhoods &mdash; stacks. As an open field, the
		heap is a resource. We let values hang out in the heap momentarily, and once
		the value is done being used, we remove the value from the heap. All values
		allocated to the heap must be removed eventually. We can't have them hanging
		out in there forever; we will see why shortly. This discussion entails that
		memory in the heap is <span class="term">dynamic</span>.
	</p>
	<p>
		Because heap memory is <span class="term">dynamic</span> the program cannot
		<span class="italicsText">directly</span> access the values in the heap.
		This is because of the point we made earlier &mdash; values in the heap are
		all over the place; they don't just stay in one place. So how does the
		program access values in the heap? With <span class="term">pointers</span>.
		Pointers are how we take memory from the heap:
	</p>
	<pre class="language-cpp"><code>
		main() {
			int *p;
		}
	</code></pre>
	<p>
		In C and C++, we indicate a pointer with the asterisk symbol. For the sake
		of discussion, we will say that the pointer
		<span class="monoText">*p</span> above takes up 2 bytes of memory. Now,
		<span class="monoText">*p</span> is a variable inside the function
		<span class="monoText">main()</span>. Accordingly,
		<span class="monoText">*p</span> is allocated into the stack, taking up 2
		bytes.<sup></sup> To take up memory in the heap:
	</p>
	<div class="note">
		<p>
			The amount of memory a pointer takes varies. Usually, it depends on the
			size of the pointer's type. For example, if an
			<span class="monoText">int</span> type takes 2 bytes, an
			<span class="monoText">int</span> pointer takes up 2 bytes.
		</p>
	</div>
	<pre class="language-cpp"><code>
		main() {
			int *p;
			p = new int[5];  
		}
	</code></pre>
	<p>
		In writing <span class="monoText">p = new int[5]</span>, we do two things:
		(1) create an <span class="monoText">int</span> array of size
		<span class="monoText">5</span> in the
		<span class="italicsText">heap</span>; and (2) made the pointer
		<span class="monoText">*p</span> point to that array. This is a crucial
		point to understand. The <span class="monoText">*p</span> variable resides
		in the stack. The <span class="monoText">int[5]</span> value resides in the
		heap. To access <span class="monoText">int[5]</span>, the program must use
		the variable <span class="monoText">p</span>. It cannot directly access
		things inside the heap.
	</p>
	<p>
		As we said earlier, <span class="monoText">int[5]</span> must be
		de-allocated from the heap. This is because there is no automatic
		destruction for things we place in the heap.
	</p>
	<pre class="language-cpp"><code>
		main() {
			int *p;
			p = new int[5];  
			delete []p;
			p = null;
		}
	</code></pre>
	<p>
		The statement <span class="monoText">delete []p</span> is the way we
		de-allocate heap memory. In this case, we include
		<span class="monoText">[]</span> because
		<span class="monoText">*p</span> points to an array. We then must write
		<span class="monoText">p = null;</span> to ensure the pointer no longer
		points anywhere.
	</p>

	<h3>Overflows</h3>
	<p>
		Why must we de-allocate heap memory? Because of
		<span class="term">overflows</span>. Remember, memory is
		<span class="underlineText">finite</span>, and memory segments are discrete,
		strict boundaries.
	</p>
	<p>
		Going back to the diagram above, there are two arrows stemming from the heap
		and the stack. These arrows symbolize how the heap and the stack are free to
		grow in size. With the stack, we at least have automatic stack destruction,
		which keeps its size in check to some extent. We say &#8220;to some
		extent&#8221; because there are situations where automatic stack destruction
		doesn't work. Automatic destruction only kicks in if the CPU finishes
		executing the function's machine code. There are many situations where a CPU
		cannot finish executing a given function's machine code. Some examples:
	</p>
	<figure class="math-display">
		<ul>
			<li>
				The function calls itself without any way to terminate (i.e., a
				recursive function without a base case or with a bad base case). This
				causes the function to call itself over and over again, allocating more
				and more memory in the stack.
			</li>
			<li>
				The function takes up too much memory in the stack. For example, a
				function with an infinite loop, or a function performing too complex and
				large or a computation.
			</li>
		</ul>
	</figure>
	<p>
		In the situations above, the function continues taking up memory in the
		stack, to the point where the stack segment collides with the heap segment.
		This causes a <span class="term">stack overflow</span>.
	</p>
	<p>
		On the other end, we have the heap, which is arguably even more insidious
		than the stack. With the heap, we do not have any automatic destruction.
		This means that when we create pointers without manually de-allocating, the
		heap keeps growing. With enough pointers, the heap can grow so large that it
		collides with the stack segment. What makes the heap so dangerous is that
		heap overflows are much harder to detect. They almost always tend to be
		extremely tiny and small amounts of overflow, but that's all it takes to
		cause a collision. That collision results in a
		<span class="term">heap overflow</span>.
	</p>
</section>

<section id="physical_v_logical_dataStructures">
	<h2>Data Structures: Physical v. Logical Interpretations</h2>
	<p>
		To better understand data structures, we will momentarily create a
		distinction between <span class="term">physical data structures</span> and
		<span class="term">logical data structures</span>. With this momentary
		distinction, we analyze the differences. Let's first compare an array and a
		linked list.
	</p>
	<figure>
		<img
			src="{% static 'images/array_v_linkedList.svg' %}"
			alt="what's the difference betwen an array and a linked list?"
			loading="lazy"
			class="seventy-p"
		/>
	</figure>
	<p>
		An <span class="term">array</span> is the simplest data structure.
		Physically (i.e., inside the computer) it is a contiguous collection of
		memory. It has a fixed size, and it can reside either in the memory or the
		heap (e.g., a pointer in the stack pointing to the array in the heap). The
		array is what we normally use when (a) we have data that should be ordered,
		and (b) we know for certain what the size of the array should be. We say
		that the array is a <span class="term">static data structure</span> because
		it is of fixed size.
	</p>
	<p>
		A <span class="term">linked list</span> is like an array, but instead of a
		contiguous collection of memory, it consists of
		<span class="term">nodes</span> composed of two parts: (1) the value the
		node stores, and (2) a pointer pointing to the next node. Structurally, the
		linked list consists of a <span class="term">head</span> &mdash; the first
		node in the list &mdash; and a <span class="term">tail</span> &mdash; the
		last node in the list. The tail always points to
		<span class="monoText">null</span>. Like the array, the linked list allows
		us to order data. However, unlike the array, the pointer is a
		<span class="term">dynamic data structure</span>. We can make it bigger or
		smaller by having the tail of the node point to a new node rather than
		<span class="monoText">null</span>. Furthermore, in contrast to the array, a
		linked list is always created in the heap. The head may exist in the stack,
		but the rest of the node always exists in the heap.
	</p>
	<p>
		Both linked lists and arrays are examples of
		<span class="term">physical data structures</span>. They directly state how
		data is actually stored in memory. Physical data structures are different
		from <span class="term">logical data structures</span>. Most of the data
		structures we're interested in are logical data structures: stacks, queues,
		trees, graphs, hash tables, and many more. These are all examples of logical
		data structures. All logical data structures are implemented using some
		combination of physical data structures, whether they're linked linked lists
		or arrays.
	</p>
	<p>
		Having said this, we now distill this artificial distinction. The difference
		between physical data structures and logical data structures boils down to a
		distinction between a data structure's underlying logic and its
		implementation. The computer does not know how to organize data into a tree.
		We have to actually give it instructions as to how a tree is structured. And
		to do so, we must state those instructions in ways the computer understands
		&mdash; with arrays or linked lists. Accordingly, every data structure has
		two interpretations: (1) its
		<span class="term">physical interpretation</span>, and (2) its
		<span class="term">logical interpretation</span>. The physical
		interpretation, what we momentarily referred to as a physical data
		structure, refers to the way we
		<span class="italicsText">implement</span> the data structure. The logical
		interpretation, what we momentarily referred to as a logical data structure,
		refers to the way we <span class="italicsText">design</span> the data
		structure.
	</p>
	<p>
		Both the logical interpretation and the physical interpretation are
		necessary to constructing data structures, but call for different skill
		sets. Implementing data structures requires knowledge and familiarity with
		the language we are using and programming principles &mdash; i.e.,
		<span class="italicsText">coding</span>. Designing data structures, however,
		requires knowledge of <span class="italicsText">discrete mathematics</span>.
		The materials that follow employ both skill sets, but err more so on the
		side of design.
	</p>
</section>

<section id="abstract_data_types">
	<h2>Abstract Data Types</h2>
	<p>
		There is a distinction between an
		<span class="italicsText">abstract data type</span> and a
		<span class="italicsText">data structure</span>.<sup></sup> In the broadest
		terms, an abstract data type represents &#8220;what we want to do&#8221;
		while the data structure represents &#8220;how we do it.&#8221; Thus, an
		abstract data type is a specification; it tells us what data we can store.
		The data structure, in contrast, is the implementation; examining it tells
		us <span class="italicsText">how</span> the data is stored.
	</p>
	<div class="note">
		<p>
			Abstract data types are also called
			<span class="italicsText">interfaces</span> or
			<span class="italicsText">Application Programming Interface (API)</span>.
		</p>
	</div>
	<p>
		In sum, the <span class="term">abstract data type</span> is a rule, or
		instruction, that communicates to the computer how a particular piece of
		data should be interpreted. More generally, a data type encapsulates, or
		contains, two kinds of information: (1) what data can be
		<span class="italicsText">stored</span>; and (2) what operations can be
		performed with that data.
	</p>
	<p>
		The data structure is what implements the two pieces of information above.
		It dictates how the data is stored, and how the operations are performed
		(via algorithms).
	</p>
	<p>
		So, for example, the type <span class="monoText">int</span> is a data type.
		The type <span class="monoText">int</span> contains the two kinds of
		information for abstract data types. What data can be stored with
		<span class="monoText">int</span>? A datum of type
		<span class="monoText">int</span> stores integers. What operations can be
		performed on a datum of type <span class="monoText">int</span>? Addition,
		subtraction, multiplication, division, modulus, and a plethora of other
		arithmetic operations.
	</p>
	<p>
		In most languages, the data type <span class="monoText">int</span> is what
		we would call a <span class="term">base type</span> or a
		<span class="term">primitive type</span>. Base types are data types provided
		natively by the language. In other words, the language's implementation
		provides the data type by default. An
		<span class="term">abstract data type</span>, however, is most often a data
		type that we, the programmers, create. More broadly, it's a data type that
		hides away the implementations. In many languages, particuarly
		object-oriented languages, we can create our own data types with special
		constructs. For example, in Java and C++, we can create our own data types
		with classes.
	</p>
	<p>For example, suppose we have the following data:</p>
	<figure class="math-display">
		<div>
			<p>${John, Luke, Michael, Kento, Idris}$</p>
		</div>
	</figure>
	<p>
		We want the order to be kept as is. Based on this constraint, we can think
		of an abstract data type &mdash; a <span class="term">list</span>. That idea
		&mdash; the abstraction <span class="italicsText">list</span> &mdash; is
		distinct from its implementation. How do we implement a list? We have two
		options: An array, or a linked list.
	</p>
	<p>
		The abstract type list contains the two kinds of information we discussed
		earlier. (1) How is the data represented? The list type takes up space for
		storing the element; it has a capacity (i.e., how big can list be); and it
		has a size (how big the list actually is). (2) What operations can be
		performed on data of type list? We can
		<span class="italicsText">append</span> a new element;
		<span class="italicsText">remove</span> an element;
		<span class="italicsText">sort</span> the elements;
		<span class="italicsText">search</span> the elements; and so on and so
		forth.
	</p>
	<p>
		The two kinds of information in an abstract data type correspond to the two
		things we focus on in the materials. The question of how data is represented
		is answered by a <span class="term">data structure</span>. The question of
		what operations can be performed on the data is answered by
		<span class="term">algorithms</span>. It should not be apparent why data
		structures and algorithms are closely linked. The abstract data type is the
		product of gathering a data structure and its operations (created via
		algorithms) into a single box (e.g., a <span class="term">class</span> or a
		<span class="term">struct</span>) so as to hide away all the
		implementations.
	</p>
	<section id="data_structures_v_abstract_data_types">
		<p>
			<span class="topic">Data Structures v. Abstract Data Types</span>
			After programming for some time, we might ask, what is the difference
			between a data structure and an abstract data type?
		</p>
		<p>
			A <span class="italicsText">data structure</span> is a collection. It
			contains data, relationships between data, and operations to be applied to
			the data. Data structures are very explicity &mdash; they state exactly
			what and how tasks are performed.
		</p>
		<p>
			In contrast, an <span class="italicsText">abstract data type</span> is
			something that defines a particular datum's behavior from the perspective
			of its user. In creating an abstract data type, we only describe what task
			must be done. We do not explicitly state how that that task should be
			done.
		</p>
		<p>
			It's critical that we keep this difference in mind as we explore
			algorithms. For example, with the array, is a data structure. However,
			when we confront particular problems, we might want to think of the array
			as the implementation of certain abstractions: a
			<span class="italicsText">sequence</span>, a
			<span class="italicsText">tuple</span>, a
			<span class="italicsText">list</span>, or a
			<span class="italicsText">vector</span>. Similarly, the multidimensional
			array is a data structure, but depending on the problem, we might want to
			think of it as a <span class="italicsText">matrix</span>, a
			<span class="italicsText">grid</span>, or a
			<span class="italicsText">tensor</span>. Why might we think in terms of
			these abstractions? Because doing so allows us to model the problem in
			terms of established facts in other areas. Whether that's logic,
			mathematics, physics, biology, economics, linguistics, etc. The more facts
			we can rely on, the more likely we are to come up with a solution. Perhaps
			even an <span class="italicsText">elegant</span> one.
		</p>
	</section>
</section>

<section id="complexity">
	<h2>Complexity; Big-O Notation</h2>
	<p>
		Having seen the close connection between data structures and algorithms, we
		are now ready to address the guiding concern of data structures and
		algorithms &mdash; <span class="term">complexity</span>. We can think of
		complexity as a quantification of how &#8220;costly&#8221; a given data
		structure or algorithm is. Usually, that cost is measured in the context of
		space and time. This results in two forms of complexity, or two kinds of
		costs &mdash; (1) <span class="italicsText">time complexity</span> and
		<span class="italicsText">space compexity</span>. We address the former
		first and the later second.
	</p>
	<p>
		A short story: A factory recently hired Luke, a 20 year-old male in good
		shape. Part of Luke's job is transferring barrels from one rack to another
		rack. As strong as Luke is, the barrels are large and weigh at least 100
		lbs. The first few days, he carries them, one by one. After a while,
		however, Luke notices how unwieldy this is. It takes far too long, and his
		mornings are now marked by lingering pain in his lower back. Luke begins
		asking, how do I completely move these barrels in a shorter amount of time?
		Aha! Why not just roll them? Luke takes this route, and it works almost
		miraculously. Suddenly, what took him at least an hour is suddenly cut to 20
		minutes. He's delighted, and begins rolling two, three, four barrels at a
		time. Luke continues his job, happily moving barrels, until management finds
		an even faster solution &mdash; a giant robotic system that simply swaps the
		racks.
	</p>
	<p>
		The question of &#8220;How long does it take to do ${x}$&#8221; is what
		<span class="term">time complexity</span> answers, where ${x}$ is some task.
		In programming, every task reduces to computations on data. Accordingly, the
		time complexity of a given task is determined by the how those computations
		are performed &mdash; the algorithm.
	</p>
	<p>
		Let's take an example. Suppose we want to summate all the integers from
		${0}$ some integer ${n.}$ One way to do this would be to write:
	</p>
	<pre class="language-cpp"><code>
		#include &lt;iostream&gt;
		using namespace std;
		
		int addUpTo(int n) {
			int x = 0;
			for (int i = 1; i <= n; i++) {
				x += i;
			}
			return x;
		}
		
		int main() {
			int sum3 = addUpTo(5);
			cout << sum3 << endl;
			return 0;
		}
	</code></pre>
	<pre class="language-bash"><code>
		15
	</code></pre>
	<p>
		With the code above, we use a loop to compute the sum. Here is another
		possible approach, using Gauss's formula:
	</p>
	<pre class="language-cpp"><code>
		#include &lt;iostream&gt;
		using namespace std;
		
		int addUpTo(int n) {
			return n * (n + 1) / 2;
		}
		
		int main() {
			int sum3 = addUpTo(5);
			cout << sum3 << endl;
			return 0;
		}
	</code></pre>
	<pre class="language-bash"><code>
		15
	</code></pre>
	<p>
		Complexity analysis is what allows us to compare these two approaches.
		Consider the two implementations of
		<span class="monoText">addUpTo()</span> side by side:
	</p>
	<pre class="language-cpp"><code>
		int addUpToLoop(int n) {
			int x = 0;
			for (int i = 1; i <= n; i++) {
				x += i;
			}
			return x;
		}
	</code></pre>
	<pre class="language-cpp"><code>
		int addUpToFormula(int n) {
			return n * (n + 1) / 2;
		}
	</code></pre>
	<p>
		Clearly, <span class="monoText">addUpToFormula()</span> is much shorter than
		<span class="monoText">addUpToLoop()</span>. But brevity does not imply
		better, at least not always. Instead, we count the number of operations each
		implementation requires. That count provides one premise for an argument of
		the form &#8220;This algorithm is better than this one.&#8221;
	</p>
	<p>
		Let's consider <span class="monoText">addUpToLoop()</span>. How many
		operations occur here? We can start by counting all of the individual steps
		the code performs.
	</p>
	<pre class="language-cpp"><code>
		int addUpToLoop(int n) {
			int x = 0;
			for (int i = 1; i <= n; i++) {
				x += i;
			}
			return x;
		}

		/* 
		Operations:
			int x = 0 happens once. (1).
			int i = 1 happense once. (1).
			int i <= n happens n times. (n).
			int i++ happens n times. (n).
			int x + i happens n times. (n).
			int x = i happens n times. (n).
			return x happens once. (1).
		*/
	</code></pre>
	<p>
		Putting the commented analysis above together, we have ${4n + 3}$ steps
		(another count might yield more steps, but we'll see that this doesn't
		really matter). Accordingly, we have ${O(4n + 3)}$ complexity for this
		approach. However, with complexity analysis, we're really just concerned
		with the leading term. In this case, the leading term is ${n.}$ Thus, the
		<span class="monoText">addUpToLoop()</span> function executes in
		<span class="italicsText">linear time</span>.
	</p>
	<p>
		Now compare that with <span class="monoText">addUpToFormula()</span>. Again,
		we count the operations:
	</p>
	<pre class="language-cpp"><code>
		int addUpToFormula(int n) {
			return n * (n + 1) / 2;
		}

		/*
		Operations:
			n + 1 happens once. (1).
			n * (n + 1) happens once. (1).
			n * (n + 1) / 2 happens once. (1).
		*/
	</code></pre>
	<p>
		Here, we have 3 steps. Thus, the
		<span class="monoText">addUpToFormula()</span> has a complexity of ${O(3).}$
		More generally, it has a complexity of ${O(1);}$
		<span class="italicsText">constant time</span>. Comparing the two: ${O(1),}$
		constant time, is much faster than ${O(n),}$ linear time. We now have a
		premise for arguing that <span class="monoText">addUpToFormula()</span> is
		better than <span class="monoText">addUpToLoop()</span> in terms of time
		complexity. When handling big-O expressions, there are several helpful rules
		to keep in mind.
	</p>
	<figure class="math-display">
		<div class="rule">
			<p>
				<span class="topic">Rule 1.</span>
				Constants are dropped. For example, ${O(2n)}$ is simply ${O(n).}$
				${O(1,000),}$ i.e., an algorithm that takes ${1,000}$ operations every
				time, is simply ${O(1).}$ Similarly, ${O(17n^2)}$ is simply ${O(n^2).}$
			</p>
			<p>
				<span class="topic">Rule 2.</span>
				Smaller terms are dropped. ${O(n + 2)}$ is reduced to ${O(n).}$ ${O(38n
				+ 7)}$ is ${O(n).}$ ${O(n^2 + 2n - 1)}$ is reduced to ${O(n^2).}$
			</p>
			<p>
				<span class="topic">Rule 3.</span>
				Arithmetic operations are in constant time. Thus, the operations
				<span class="monoText">+</span>, <span class="monoText">-</span>,
				<span class="monoText">*</span>, <span class="monoText">/</span> and
				<span class="monoText">%</span> are all operations that take constant
				time.
			</p>
			<p>
				<span class="topic">Rule 4.</span>
				Variable declarations and assignments are in constant time. Thus,
				writing <span class="monoText">int n</span> is in constant time, and
				<span class="monoText">int n = 2</span> is in constant time.
			</p>
			<p>
				<span class="topic">Rule 5.</span>
				Accessing elements in an array by index is in constant time. E.g., given
				the array <span class="monoText">arr[] {1, 2, 3}</span>, writing
				<span class="monoText">arr[0]</span> to access
				<span class="monoText">1</span> takes constant time.
			</p>
			<p>
				<span class="topic">Rule 6.</span>
				In a loop, the complexity is the length of the loop, multiplied by the
				the complexity of the loop's body. For example, a loop that runs from
				${0}$ to ${n}$ has a complexity of ${O(n).}$ If the loop's body (the
				code to be executed at each iteration) has a complexity of ${O(4),}$
				then we have ${O(4n),}$ or simply ${O(n).}$ If the loop contains a loop
				in its body, then the outer loop has a complexity of ${O(n \cdot n) =
				O(n^2).}$
			</p>
		</div>
	</figure>
	<p>
		Another example: let's say we have a <span class="monoText">list</span> type
		datum:
	</p>
	<figure class="math-display">
		<div>
			<p>
				<span class="monoText">["Sam", "Julie", "Eric", "Dan", "Helen"]</span>
			</p>
		</div>
	</figure>
	<p>
		Again, there are two says to implement this abstract type: an array or a
		linked list. Say we use an array. Now suppose we want to check if
		<span class="monoText">"Mike"</span> is in the list. Checking if
		<span class="monoText">"Mike"</span> is in the list is a
		<span class="italicsText">searching problem</span>. We have to search for a
		string in the list that matches <span class="monoText">"Mike"</span>. How
		long it takes to find a match depends on how we search for the match.
	</p>
	<p>
		Whenever we think about algorithms, we must always keep this rule in mind:
	</p>
	<figure class="math-display">
		<div class="rule">
			<p>A computer can only &#8220;look&#8221; at one thing at a time.</p>
		</div>
	</figure>
	<p>
		In other words, a computer does not have the benefit of looking at many
		things and pinpointing the match. In the example above, we have that
		benefit. We can clearly see that <span class="monoText">"Mike"</span> is not
		in the list.<sup></sup>
	</p>
	<div class="note">
		<p>
			Of course, our benefit of seeing many things at once effectively
			disappears once we have to deal with a list of a thousand, or a million
			things.
		</p>
	</div>
	<p>
		Accordingly, when we write our algorithms, we must always follow the
		fundamental constraint that a computer can only &#8220;look&#8221; at one
		thing at a time. That said, what are some possible algorithms to search for
		a match?
	</p>
	<p>
		One possible algorithm is a <span class="term">linear search</span>. In a
		linear search, the computer goes through each element in the list, one by
		one, checking if there's a match. In the case above, the computer would go
		through all 5 strings just to return a
		<span class="monoText">false</span> (i.e., &#8220;False, there is no
		"Mike".&#8221;). If the list contained ${n}$ elements and there was no
		<span class="monoText">"Mike"</span>, then the algorithm takes a total of
		${n}$ steps.
	</p>
	<p>
		This is where we make a distinction in measuring time complexity. Time
		complexity is not necessarily measured in units of time. Instead, we
		generally measure time complexity in terms of the number of
		<span class="italicsText">steps</span> a computer must take to complete the
		task. We will explore why we measure time in this manner in later sections.
	</p>
	<p>
		Because a linear search algorithm, in the worst-case scenario, might take
		${n}$ steps to complete its task, we say that the linear search algorithm
		has a complexity in the order of ${O(n).}$ However, note that when we
		conduct a complexity analysis, we always want to also look at the code. For
		example, the linear search algorithm might be implemend like this (in
		pseudocode):
	</p>
	<figure class="math-display">
		<pre class="language-pseudo"><code>
			for (i = 0; i < arr.length; i++) {
				if i == "Mike" {
					return true;
				} else {
					return false;
				}
			}
		</code></pre>
	</figure>
	<p>
		In the code above, we have a loop that executes ${n}$ times. However, we
		also have instructions inside the loop: (1) a variable initialization, which
		occurs exactly once, <span class="monoText">i = 0</span>; (2) a comparison
		check <span class="monoText">i < arr.length</span>, (3) a
		<span class="monoText">arr.length</span> computation (which, depending on
		the language, may be called at each iteration); (2) an equality check
		<span class="monoText">==</span>, and an increment
		<span class="monoText">i++</span>. These are are all distinct instructions,
		that may result in the number of steps being something along the lines of
		${an + b,}$ where ${a}$ and ${b}$ are constants. This does not, however,
		change our previous result, ${O(n).}$ In complexity analysis, we focus on
		the <span class="italicsText">leading terms</span>. In other words, the
		&#8220;biggest&#8221; terms in the expression. Accordingly, this requires
		dropping constants.
	</p>

	<h3>Space Complexity</h3>
	<p>
		The same analysis for time complexity applies to
		<span class="term">space complexity</span>. The difference: With space
		complexity, we're concerned with how much space a given data structure, or
		algorithm, uses up. For example, with a linked list of ${n}$ nodes, we have
		a space complexity of ${O(2n).}$ Why ${2n?}$ Because every node contains two
		parts &mdash; the value stored and a pointer. Of course, the actual result
		might be something more than ${2n,}$ since an operation on the list may
		require more memory. However, that amount is more than likely a constant, or
		a variable that has no affect on the leading term, ${n.}$ as we said
		earlier, we are only focused on the leading term. Accordingly, the data
		structure has a space complexity of ${n.}$
	</p>
	<p>
		With space complexity, there are a few general rules we can keep in mind:
	</p>
	<figure class="math-display">
		<div class="rule">
			<p>
				<span class="topic">Rule 1.</span> Primitive types take up constant
				space ${O(1).}$ For example, <span class="monoText">int</span>,
				<span class="monoText">char</span>,
				<span class="monoText">bool</span> and others take up ${O(1)}$ space.
				Granted, a more detailed analysis would be that something like
				<span class="monoText">int</span> takes up ${O(4),}$ since an
				<span class="monoText">int</span> takes up 4 bytes in memory. But again,
				we reduce ${O(4)}$ to ${O(1).}$
			</p>
			<p>
				<span class="topic">Rule 2.</span> Strings are of linear space
				complexity ${O(n),}$ where ${n}$ is the length of the string.
			</p>
			<p>
				<span class="topic">Rule 3.</span> Arrays are or linear space
				complexity, ${O(n),}$ where ${n}$ is the length of the array.
			</p>
		</div>
	</figure>
</section>

<section id="alg_intro">
	<h2>Algorithms: Prelude</h2>
	<p>
		An <span class="term">algorithm</span> is a solution to a
		<span class="term">computational problem</span>. As a solution, it is a
		sequence of computational steps that consumes a set of
		<span class="term">inputs</span> and produces a set of
		<span class="term">outputs</span>.
	</p>

	<p>
		Because of this definition, the way a problem is stated determines the
		relationship between inputs and outputs. We will use certain notations to
		state problems. For example, if the problem asks, "Sort the numbers into
		non-decreasing order," we would write:
	</p>

	<figure class="math-display">
		<div class="rule">
			<p>${s \coloneqq}$ ${\lang a_1, a_2, \ldots, a_n \rang}$</p>
			<p>
				${f(s) \coloneqq}$ ${\lang a_1', a_2', \ldots, a_n' \rang}$ : ${a_1'
				\leq a_2' \leq \ldots \leq a_n'}$
			</p>
		</div>
	</figure>

	<p>
		In the notation above, we use ${s}$ to denote the input. Of course, we are
		free to use whatever variable we would like. We use ${f(s)}$ to denote the
		output, where ${f}$ is the actual algorithm itself. We can use any other
		variable. For example, ${r}$ and ${t(r),}$ ${g}$ and ${h(g).}$ The symbol
		${\coloneqq}$ reads "defined as," and the colon is read "such that."
		Finally, the brackets ${\lang \ldots \rang}$ indicate a sequence.
	</p>

	<p>
		The problem above is an example of a
		<span class="term">sorting problem</span>. This is a particularly important
		problem in computer science, because numerous algorithms rely on its
		solution (for example, many
		<span class="italicsText">search algorithms</span> assume values are
		sorted).
	</p>

	<p>
		When we write algorithms, we want our algorithms to be
		<span class="italicsText">correct</span>. An algorithm is correct if, and
		only if, it halts at the correct output (in other words, the algorithm stops
		the moment it finds a correct output). An incorrect algorithm is one that
		halts at the incorrect output, or one that never halts.
	</p>

	<p>
		<span class="topic">Performance is currency.</span> At the end of the day,
		we study algorithms because of performance. With efficient and correct
		algorithms, we can obtain other things &mdash; security and features. Some
		programmers prefer Python over C, even if C is so much faster. Why? For a
		variety of reasons &mdash; Python is more readable, more user-friendly, more
		widely used, more abstracted, and so on. The cost, however, is performance.
		Python's memory management algorithms may not be the best approach for our
		problem. Python's garbage collection algorithms may be not how we would like
		our data handled. However, we may be willing to take on those costs because
		we've written well-performing algorithms.
	</p>
</section>

<section id="efficiency">
	<h2>Efficiency</h2>
	<p>
		One of the core concerns for algorithms is
		<span class="term">efficiency</span>. For now, let's consider efficiency in
		terms of time. Accordingly, the term efficiency refers to how long it takes
		an algorithm to solve a given problem. How do we measure this length of
		time?
	</p>
	<p>
		The most obvious way is to write some sort of stop watch program, where the
		timer begins once the algorithm commences, and stops when the algorithm
		finishes. We then compare that time to other algorithm times.
	</p>
	<p>
		But what's the problem with this approach? The metrics would be entirely
		machine dependent! The time it takes to perform algorithm ${x}$ on a Macbook
		Pro would be different from the time it takes to perform ${x}$ on a
		Raspberry Pi, even if we're measuring the same algorithm and using the same
		data set. Because of this limitation, we must measure &#8220;speed&#8221;
		with a different metric.
	</p>
	<p>
		Instead of measuring speed with time, we assume that every step a computer
		takes consumes a constant amount of time, then we count how many operations,
		or steps, the algorithm takes. In doing so, an algorithm's efficiency is not
		so much a measure of <span class="italicsText">how long</span> the algorithm
		takes to solve a problem, but
		<span class="italicsText">how many steps</span> the algorithm takes.
	</p>
	<p>
		To measure the number of steps an algorithm takes to solve a given problem,
		we perform <span class="term">asymptotic analysis</span>. The subsequent
		comments provide a big-picture view of asymptotic analysis.
	</p>

	<section id="asymptotic_complexity">
		<h3>Asymptotic Complexity</h3>
		<p>
			Asymptotic complexity is the tool used by computer scientists and
			mathematicians for measuring algorithmic efficiency. The formal
			definition:
		</p>
		<figure class="math-display">
			<div class="rule">
				<p>
					<span class="topic">Asymptotic Complexity.</span> A function ${f(x)}$
					runs on the order of ${g(x)}$ if there exists some ${x}$ value ${x_0}$
					and some constant ${C}$ for which ${f(x) \leq C \cdot g(x)}$ for all
					${x > x_0.}$
				</p>
			</div>
		</figure>
		<p>
			At its core, asymptotic complexity is a measure of how fast a program's
			runtime grows asymptotically. This is equivalent to asking: As the size of
			${f(x)}$'s domain (its inputs) tends towards infinity, how does the
			runtime of the program grow? Consider the example of counting the number
			of characters in this string:
		</p>
		<figure class="math-display">
			<div>
				<p><span class="monoText">"hegel"</span></p>
			</div>
		</figure>
		<p>
			If we used the algorithm of counting each character one at a time, we
			would count ${(h, 1),}$ ${(e, 2),}$ ${(g, 3),}$ ${(e, 4),}$ and ${(l,
			5).}$ We say that this algorithm runs on
			<span class="italicsText">linear time</span> with respect to the number of
			characters. Why? Because it takes a constant amount of time to count one
			character, and as such, the number of characters is directly proportional
			to the amount of time it takes to count the number of characters in the
			string. As the number of characters increases, the runtime increases
			linearly with the number of characters (the input). Denoting the number of
			characters with the variable ${n,}$ we say that the character-counting
			algorithm has a complexity of order ${O(n).}$
		</p>
	</section>

	<section id="classes_or_runtime">
		<h3>Functions of Runtime</h3>
		<p>
			There are several classes of runtime that we're most concerned with the
			analysis of algorithms. We cover each of them below.
		</p>

		<section id="constant_time">
			<p>
				<span class="topic">Constant Time: ${O(1).}$</span> If an algorithm
				always takes a certain amount of time regardless of the size of its
				input, we say that the algorithm's running time is
				<span class="term">constant</span>.
			</p>
		</section>

		<section id="logarithmic_time">
			<p>
				<span class="topic">Logarithmic Time: ${O(\log n).}$</span> If an
				algorithm's running time grows logarithmically with increasing inputs,
				we say that the algorithm's running time is
				<span class="term">logarithmic</span>. Logarithmic time is most often
				found with algorithms that solve a large problem by dividing it into a
				series of smaller and smaller problems.
			</p>
		</section>

		<section id="linear_time">
			<p>
				<span class="topic">Linear Time: ${O(1).}$</span> If an algorithm's
				running time increases in direct proportion to its inputs, we say that
				the algorithm's running time is <span class="term">linear</span>. Linear
				time is most often found with algorithms that must perform some
				processing on every input one by one. For example, if the number of
				inputs is ${n = 1000,}$ then the algorithm takes ${1000}$ operations to
				complete. Such an algorithm runs on linear time.
			</p>
		</section>

		<section id="linearithmic_time">
			<p>
				<span class="topic">Linearithmic Time: ${O(n \log n).}$</span> Some
				algorithms break problems into smaller problems, solve them
				individually, then combine the solutions. These algorithms' running time
				is <span class="term">linearithmic</span>.
			</p>
		</section>

		<section id="quadratic_time">
			<p>
				<span class="topic">Quadratic Time: ${O(n^2).}$</span> Algorithms that
				require a nested loop will almost always have
				<span class="term">quadratic</span> running times. Needless to say,
				quadratic time is not the most efficient, and it is generally restricted
				to small problems.
			</p>
		</section>

		<section id="cubic_time">
			<p>
				<span class="topic">Cubic Time: ${O(n^3).}$</span> Like quadratic time,
				algorithms requiring loops nested within nested loops almost assuredly
				have <span class="term">cubic</span> running times. Like algorithms of
				order ${O(n^2),}$ algorithms of order ${O(n^3)}$ are generally relegated
				to small problems.
			</p>
		</section>

		<section id="exponential_time">
			<p>
				<span class="topic">Exponential Time: ${O(2^n).}$</span> Algorithms with
				<span class="term">exponential</span> running times are those where
				every input results in an exponential increase in running time. For
				example, where ${n = 2,}$ the running time is ${4,}$ and where ${n =
				20,}$ the running time is ${1,000,000.}$ Exponential running times are
				most often found with brute-force algorithms.
			</p>
		</section>
	</section>
</section>
{% endblock %}
