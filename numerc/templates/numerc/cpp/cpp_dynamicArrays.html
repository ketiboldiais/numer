{% extends '../layout.html' %} {% load static %} {% block description %}
<meta name="description" content="Dynamic arrays, sets, and sequences." />
{% endblock %} {% block title %}
<title>Sequences</title>
{% endblock %} {% block content %}

<h1>Arrays</h1>
<section id="intro">
	<p>
		<span class="drop">T</span>he first set of abstract data types we explore is
		the <span class="term">sequence</span>. Sequences allow us to store ${n}$
		things in a specific order. Those things could be strings, integers,
		floating point values, or even other abstract data type objects.
	</p>
	<p>
		The sequence data type consists of two subtypes: (a) the
		<span class="term">tuple</span
		><label for="tuple" class="margin-toggle"><sup></sup></label>
		<input type="checkbox" id="tuple" class="margin-toggle sidenote-number" />
		<span class="marginnote"
			>The tuple is also called a
			<span class="italicsText">static sequence</span>.</span
		>
		and (b) the <span class="term">list</span>.<label
			for="list"
			class="margin-toggle"
			><sup></sup
		></label>
		<input type="checkbox" id="list" class="margin-toggle sidenote-number" />
		<span class="marginnote"
			>The list data type is also called a
			<span class="italicsText">vector</span> or
			<span class="italicsText">dynamic sequence</span>.</span
		>
		The tuple type is a sequence with a fixed size. I.e., the number of elements
		in the sequence does not change. The list type is a sequence with a dynamic
		size; the sequence can grow or shrink through the insertion, shifting, or
		removing of elements in the sequence.
	</p>
	<p>
		Sequences have several useful properties and operations. Below is an API:
	</p>
	<figure class="table">
		<table class="api">
			<thead>
				<th>Identifier</th>
				<th>Description</th>
			</thead>
			<tbody>
				<tr>
					<td colspan="2" style="font-family: var(--serif)">Properties</td>
				</tr>
				<tr>
					<td><span class="monoText">arraySpace</span></td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">size</span></td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">length</span></td>
					<td></td>
				</tr>
				<tr>
					<td colspan="2" style="font-family: var(--serif)">Operations</td>
				</tr>
				<tr>
					<td><span class="monoText">display()</span></td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">append()</span></td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">insert(${i}$, ${x}$)</span></td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">remove(${i}$)</span></td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">search(${x}$)</span></td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">get(${i}$)</span></td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">set(${i}$, ${x}$)</span></td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">max()</span></td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">min()</span></td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">reverse()</span></td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">iterSeq()</span></td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">shift()</span></td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">rotate()</span></td>
					<td></td>
				</tr>
			</tbody>
		</table>
	</figure>
</section>

<section id="static_lists">
	<h2>Tuples</h2>
	<p>
		We turn our attention first to the tuple. Because tuples are of fixed size,
		whenever we talk about tuples, we talk about some ${n-}$tuple. For example,
		2-tuple is could a <span class="italicsText">double</span>. A 3-tuple is
		called a <span class="italicsText">triple</span>. A 4-tuple is a
		<span class="italicsText">quadruple</span>, and so on. Because tuples are of
		fixed size, they necessarily have a smaller API.
	</p>
	<figure class="table">
		<table class="api">
			<thead>
				<th>Identifier</th>
				<th>Description</th>
				<th>Comments</th>
			</thead>
			<tbody>
				<tr>
					<td colspan="3" style="font-family: var(--serif)">Properties</td>
				</tr>
				<tr>
					<td><span class="monoText">size</span></td>
					<td>Returns how many elements the sequence can hold</td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">length</span></td>
					<td>Returns how many elements are in the sequence</td>
					<td></td>
				</tr>
				<tr>
					<td colspan="3" style="font-family: var(--serif)">Operations</td>
				</tr>
				<tr>
					<td><span class="monoText">build(${x_0, \ldots, x_n}$)</span></td>
					<td>
						Given the arguments ${x_0, \ldots x_n,}$ build a sequence ${\lang
						x_0, \ldots, x_n \rang}$
					</td>
					<td>Two approaches to building: At compile-time, and at runtime.</td>
				</tr>
				<tr>
					<td><span class="monoText">print()</span></td>
					<td></td>
					<td>${O(n)}$ &mdash; linear time</td>
				</tr>
				<tr>
					<td><span class="monoText">search(${x}$)</span></td>
					<td></td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">get(${i}$)</span></td>
					<td></td>
					<td>${O(1)}$ &mdash; constant time</td>
				</tr>
				<tr>
					<td><span class="monoText">set(${i}$, ${x}$)</span></td>
					<td></td>
					<td>${O(1)}$ &mdash; constant time</td>
				</tr>
				<tr>
					<td><span class="monoText">max()</span></td>
					<td></td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">min()</span></td>
					<td></td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">reverse()</span></td>
					<td></td>
					<td></td>
				</tr>
				<tr>
					<td><span class="monoText">iterSeq()</span></td>
					<td></td>
					<td>${O(n)}$ &mdash; linear time</td>
				</tr>
			</tbody>
		</table>
	</figure>
	<p>
		Examining these data and properties, we proceed to examining implementation
		approaches. The first implementation approach we consider is the
		<span class="italicsText">static array</span>. We'll first write this
		program in C, then proceed to implementing it in C++.
	</p>

	<p>
		<span class="topic">Printing a Tuple.</span>
		First, the <span class="monoText">print()</span> operation. To print a
		tuple, we must iterate over each of the tuple's element, displaying each
		element to the console (i.e., with something lik
		<span class="monoText">cout</span> or <span class="monoText">printf</span>).
		Because we're iterating over each element, the
		<span class="monoText">print()</span> operation has a time complexity of
		${O(n)}$ given the ${n-}$tuple. This is linear time.
	</p>
	<p>
		<span class="topic">Appending an Element to a Tuple.</span> Second, the
		<span class="monoText">append(${x}$)</span> operation, where ${x}$ is an
		element. To append an element ${x,}$ we insert the element ${x}$ at the very
		end of the tuple. In other words, &#8220;Insert this element ${x}$ at the
		first instance of free space.&#8221; Thus, given an array of length 10 and
		size 5, <span class="monoText">append(${x}$)</span> inserts the element
		${x}$ at ${i = 6.}$ For <span class="monoText">append(${x}$)</span>, we can
		think of this as performing three distinct operations &mdash; accessing the
		array at ${i = \textit{length},}$ assigning the element ${x}$ to that
		particular index, and updating the ${\textit{length}}$ property. Hence,
		${O(3).}$ Applying complexity analysis, this is really ${f(n) = 3.}$
		Rewriting this as ${f(n) = 3n^0,}$ we have ${O(n^0),}$ which reduces to
		${O(1).}$ I.e., constant time.
	</p>
	<p>
		<span class="topic">Inserting an Element to a Tuple.</span> With the
		<span class="monoText">insert(${i}$, ${x}$)</span> operation, we're
		inserting an element ${x}$ at the index ${i.}$ For example, suppose we have
		the tuple ${(0, 1, 2, 3, 4, \_, \_, \_).}$ This is a tuple of size 8, with
		length 5. If we write <span class="monoText">insert(${2}$, ${9}$)</span>, we
		would have the tuple ${(0, 9, 1, 2, 3, 4, \_, \_).}$ Notice that this causes
		a shift. To perform this shift, have to use a loop. In pseudocode:
	</p>
	<figure class="math-display">
		<div>
			<pre class="language-pseudo"><code>
				insert(index, x) {
					for (i = length; i > index, i--) {
						A[i] = A[i - 1];
					}
					A[index] = x;
					length++;
				}
			</code></pre>
		</div>
	</figure>
	<p>
		The operation of inserting an element into a tuple provides an opportunity
		to examine the different forms of complexity analysis. If we insert the
		element at the very end of the tuple &mdash;
		<span class="monoText">insert(${length}$, ${x}$)</span> &mdash; no shifting
		occurs. Accordingly, this operation has a time complexity of order ${O(1),}$
		constant time. However, if we insert the element at the very beginning
		&mdash; <span class="monoText">insert(${0}$, ${x}$)</span> &mdash; then we
		must shift <span class="underlineText">all</span> the elements. Given a
		tuple of length ${n,}$ this operation would require iterating over all ${n}$
		elements. This operation's time complexity is of order ${O(n).}$
	</p>
	<p>
		Examining these possibilities, we have a best-case scenario and a worst-case
		scenario. The best-case scenario is ${\Omega(1),}$ constant time, which
		occurs only if we insert at the very end. The worst-case scenario is
		${O(n),}$ linear time, which occurs if we insert at the origin, ${i = 0.}$
	</p>
	<p>
		<span class="topic">Deleting an Element from a Tuple.</span> The operation
		<span class="monoText">deleteElementAt(${i}$)</span> deletes, or removes, an
		element at the index ${i.}$ Suppose we have the tuple ${(1, 5, 8, 3, 2, \_,
		\_, \_).}$ In this example, there are three cases for deletion:
		<span class="monoText">deleteElementAt(${0}$)</span>,
		<span class="monoText">deleteElementAt(${\textit{length}}$)</span>, and
		<span class="monoText">deleteElementAt(${n}$)</span>, where ${0 < n <
		\textit{length}.}$ The two problematic cases are the first and the third.
		They are problematic because if we delete an element other than the last,
		then we would have gaps in the tuple:
	</p>
	<figure class="math-display">
		<div>
			<p>
				<span class="monoText">deleteElementAt(0)</span> ${ \implies (\_, 5, 8,
				3, 2, \_, \_, \_)}$
			</p>
			<p>
				<span class="monoText">deleteElementAt(3)</span> ${ \implies (0, 5, 8,
				\_, 2, \_, \_, \_)}$
			</p>
		</div>
	</figure>
	<p>
		These gaps lead to all sorts of problems, particularly with respect to
		iteration. One way to avoid these problems is to manually check if a given
		index has an element. This is not a good approach. The better approach is to
		get right to the root of the problem &mdash; &#8220;holes&#8221; in the
		tuple. We can prevent these holes by shifting elements to the right whenever
		one of the two cases above occurs. Thus:
	</p>
	<figure class="math-display">
		<div>
			<p>
				<span class="monoText">deleteElementAt(3)</span> ${ \implies (0, 5, 8,
				\_, 2, \_, \_, \_) \implies (0, 5, 8, 2, \_, \_, \_, \_)}$
			</p>
		</div>
	</figure>
	<p>
		To understand how to perform this shifting, we consider the example
		<span class="monoText">deleteElementAt(3)</span>. Here, ${i = 3.}$ So, we
		start by removing the element at ${i = 3,}$ then shift the element at ${i +
		1}$ to ${i = 3.}$ This creates a hole at ${i = 4,}$ so we set ${i = 4,}$
		then shift the element at ${i + 1}$ to ${i = 4.}$ This creates a hole at ${i
		= 5,}$ so we shift the element at ${i + 1}$ to ${i = 6,}$ and so on, up
		until we reach the value of ${i = \textit{length} - 1}$ (the second to last
		element). In pseudocode:
	</p>
	<figure class="math-display">
		<div>
			<pre class="language-pseudo"><code>
				deleteElementAt(index):
					element = A[index];
					for (i = index; i < tuple.length - 1; i++):
						A[i] = A[i + 1];
					length--;
			</code></pre>
		</div>
	</figure>
	<p>
		The time complexity for this algorithm depends on the value of
		<span class="monoText">index</span>. If
		<span class="monoText">index = length</span>, then no shifting occurs, in
		which case we only have to perform two operations: Initializing
		<span class="monoText">element</span> and decrementing
		<span class="monoText">length</span>. This yields a complexity of ${O(2),}$
		or simply ${O(1)}$ &mdash; constant time.
	</p>
	<p>
		If, however, <span class="monoText">index = 0</span>, then we have to
		perform ${n}$ shifts. This yields ${O(n + 2),}$ or ${O(n).}$ This is linear
		time. Accordingly, deleting an element from a tuple, in the best case
		scenario, is ${\Omega(1),}$ and in the worst case scenario, ${O(n).}$
	</p>

	<h3>Searching a Tuple</h3>
	<p>
		Searching for a particular element ${x}$ in a tuple depends on the search
		algorithm employed. There are two algorithms available: (1)
		<span class="italicsText">linear search</span> and (2)
		<span class="italicsText">binary search</span>. We consider linear search
		first.
	</p>

	<section id="linear_search">
		<h4>Linear Search</h4>
		<p>
			Under
			<span class="term">linear search</span>, we iterate over each of the
			tuple's elements, checking if the element matches our query. Say we had
			the following tuple:
		</p>
		<figure class="math-display">
			<div>
				<p>${A = (8, 9, 4, 7, 6, 5, 10, 2, 11)}$</p>
			</div>
		</figure>
		<p>
			With the linear search approach, we go through each of the elements of
			${A,}$ checking if the element matches the key. Suppose the key is the
			integer ${5.}$ We start with ${A[0],}$ and ask, &#8220;${A[0] =
			5?}$&#8221; False. So we continue to ${A[1].}$ Again we ask, &#8220;${A[1]
			= 5?}$&#8221; False again. So we go to ${A[2].}$ We continue all the way
			up to ${A[5],}$ wherein ${A[5] = 5}$ is true. Because our key was found,
			we say that the search was <span class="italicsText">successful.</span>
		</p>
		<p>
			Now, what if the key was the integer ${22?}$ In that case, we go all the
			way up to ${A[\textit{length}],}$ or ${A[8],}$ to determine that ${22}$ is
			not in the tuple. This is an example of an
			<span class="italicsText">unsuccessful search</span>.
		</p>
		<p>
			Implementing linear search is straightforward. We are simply traversing
			the tuple. We start with ${A[0],}$ then continue to the last element of
			the list, ${A[\textit{length}].}$
		</p>
		<figure class="math-display">
			<div>
				<pre class="language-pseudo"><code>
				tupleLinearSearch(x) {
					for (i = 0; i < length; i++) {
						if (key == A[i]){
							return i;
						}
					}
					return -1;
				}
			</code></pre>
			</div>
		</figure>
		<p>
			What is the time complexity for linear search? Well, in the worst-case
			scenario, the tuple does not contain our key. In which case we would have
			to perform the comparison operation
			<span class="monoText">key == A[i]</span> on all the tuple's elements.
			Thus, given an ${n-}$tuple not containing our key, we have time complexity
			of order ${O(n).}$ This is linear time.
		</p>
		<p>
			In the best-case scenario, ${A[0]}$ is our key, in which case only one
			comparision operation is performed. Accordingly, in the best-case
			scenario, linear search has a time complexity of order ${\Omega(1)}$
			&mdash; constant time.
		</p>
		<p>
			What about the average-case scenario? To determine the average-case
			scenario, we ask, how many comparisons are needed to determine whether ${i
			= 1}$ matches our key? We must perform 1 comparison. We again ask, how
			many comparisons are needed to determine whether the element at ${i = 2}$
			matches our key? We must perform 2 comparisons. We ask the same question
			for ${i = 3,}$ ${i = 4,}$ ${i = 5,}$ and so on. The sum of comparisons is
			the sum of arithmetic sequence of the positive integers:
		</p>
		<figure class="math-display">
			<div>
				<p>${1 + 2 + 3 + \ldots + n = \dfrac{n(n + 1)}{2}}$</p>
			</div>
		</figure>
		<p>
			Thus, the total amount of time for all possible cases is
			${\dfrac{n(n+1)}{2}.}$ Dividing this by the total number of cases, ${n,}$
			provides us with the average case:
		</p>
		<figure class="math-display">
			<div>
				$$ \begin{aligned} \dfrac{ \dfrac{n(n + 1)}{2} }{n} &= \dfrac{
				\dfrac{\cancel{n}(n + 1)}{2} }{\cancel{n}} \\[1.2em] &= \dfrac{n + 1}{2}
				\end{aligned} $$
			</div>
		</figure>
		<p>
			Hence, in the average-case scenario, linear search has a complexity of
			${\Theta\left(\dfrac{n+1}{2}\right).}$ Or, asymptotically, ${\Theta(n).}$
			Notice that both the average-case runtime and the worst-case runtime are
			the same. With linear search, we're almost always running on linear time.
			We can, however, optimize the linear search algorithm.
		</p>
		<p>
			<span class="topic">Transposition.</span> The first method to improving
			linear search is with <span class="term">transposition</span>. The idea
			behind transposition is best understood via analogy. Suppose you're
			conducting research, and you know there's a particular book relevant to a
			topic. You proceed to the shelf and retrieve the book. Instead of
			returning the book to the original place, you might leave the book on a
			particular table, or place it somewhere closer to your work space. Why?
			Because there's a high likelihood of you needing to use the same book
			again.
		</p>
		<p>
			This same idea extends to linear search. In search for some key ${k,}$
			there's a likelihood of searching for ${k}$ again, so we bring it closer
			to ${i = 0.}$ For example, say we have the tuple:
		</p>
		<figure class="math-display">
			<div>
				<p>${T = (8, 9, 4, 6, 7, 3, 2, 11)}$</p>
			</div>
		</figure>
		<p>
			and our key is ${6.}$ A linear search for 6 returns a successful search,
			so we swap 6 and 4 (swap ${T[3]}$ and ${T[2]}$):
		</p>
		<figure class="math-display">
			<div>
				<p>${T = (8, 9, 6, 4, 7, 3, 2, 11)}$</p>
			</div>
		</figure>
		<p>Search for 3 again, and we swap ${T[1]}$ and ${T[2]}$:</p>
		<figure class="math-display">
			<div>
				<p>${T = (8, 6, 9, 4, 7, 3, 2, 11)}$</p>
			</div>
		</figure>
		<p>Search 3 one more time, and we swap ${T[1]}$ and ${T[0]}$:</p>
		<figure class="math-display">
			<div>
				<p>${T = (6, 8, 9, 4, 7, 3, 2, 11)}$</p>
			</div>
		</figure>
		<p>
			Notice that 6 is now at the head of the tuple. Now whenever we search for
			${6,}$ we have a runtime of ${O(1)}$ &mdash; constant time. Of course, 6
			will only get to the tuple's head if we search for it enough times.
		</p>
		<figure class="math-display">
			<div>
				<pre class="language-pseudo"><code>
				tupleLinearSearch(x) {
					for (i = 0; i < length; i++) {
						if (key == T[i]){
							swap(T[i], T[i-1])
							return i - 1;
						}
					}
					return -1;
				}
			</code></pre>
			</div>
		</figure>
		<p>
			<span class="topic">Move-to-head.</span> With transposition, we do not
			reach ${O(1)}$ unless we search for the key ${k}$ ${n}$ times, where ${n}$
			is the index of ${k.}$ This means that the key ${k}$ can take a fair
			amount of time to get to the list's head. Moreover, if ${k}$ is not
			searched, it slowly moves towards the tail-end. To get to the head faster,
			we can use the <span class="term">move-to-head</span> approach. With
			move-to-head, after a successful search, we immediately place the key at
			${i = 0.}$
		</p>
		<figure class="math-display">
			<div>
				<ol class="numd">
					<li>${T = (8, 9, 4, 6, 7, 3, 2, 11)}$</li>
					<li><span class="monoText">tupleLinearSearch${(6)}$</span></li>
					<li>${T = (6, 9, 4, 8, 7, 3, 2, 11)}$</li>
				</ol>
			</div>
		</figure>
		<p>In pseudocode:</p>
		<figure class="math-display">
			<div>
				<pre class="language-pseudo"><code>
					tupleLinearSearch(x) {
						for (i = 0; i < length; i++) {
							if (key == T[i]){
								swap(T[i], T[0])
								return 0;
							}
						}
						return -1;
					}
				</code></pre>
			</div>
		</figure>
	</section>

	<section id="binary_search">
		<h4>Binary Search</h4>
		<p>
			The <span class="term">binary search</span> algorithm is correct only if
			the tuple is <span class="underlineText">sorted</span>. There are a whole
			host of sorting algorithms, so we will cover them in a separate section.
			For now, suppose we have the following sorted tuple:
		</p>
		<figure class="math-display">
			<div>
				$$ \begin{matrix} T = [& 4 & 8 & 10 & 15 & 18 & 21 & 24 & 27 & 29 & 33 &
				34 & 37 & 39 & 41 & 43 &] \\ i = & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9
				& 10 & 11 & 12 & 13 & 14 & \end{matrix} $$
			</div>
		</figure>
		<p>
			Suppose our key is ${k = 18.}$ We see that 18 is at index ${i = 4.}$ With
			linear search, we must perform 5 comparisions before ${k}$ is found. Let's
			see how binary search compares.
		</p>
		<p>
			First, with binary search, we need three variables: ${\ell,}$ ${h,}$ and
			${M.}$ These variables represent three variants: ${\ell}$ represents the
			lower bound (an index), ${h}$ represents the upper bound (also an index),
			and ${M}$ represents the index exactly between ${\ell}$ and ${h.}$
			Accordingly, the value of ${M}$ is given by:
		</p>
		<figure class="math-display">
			<div>
				<p>${M = \lfloor \dfrac{\ell + h}{2} \rfloor}$</p>
			</div>
		</figure>
		<p>
			This simply the arithmetic mean, but we will floor the result to account
			for the possibility of an odd number of elements in the tuple. For
			example, if the tuple has 5 elements, then ${\ell = 0,}$ and ${h = 5.}$
			Calculating ${M:}$ ${(0 + 5) / 2 = 2.5.}$ Obviously an index cannot be
			${2.5,}$ so we floor the result: ${M \equiv i = 2.}$
		</p>
		<p>
			That said, let's trace our program. We know ${k = 18.}$ So, we first
			initialize ${\ell}$ and ${h:}$
		</p>
		<figure class="math-display">
			<div>
				<p>${\ell = 0}$</p>
				<p>${h = 14}$</p>
			</div>
		</figure>
		<p>Then we compute ${M:}$</p>
		<figure class="math-display">
			<div>
				<p>${M = \dfrac{0 + 14}{2} = 7}$</p>
			</div>
		</figure>
		<p>
			Thus, ${M}$ is ${i = 7.}$ We then ask, is ${T[7] = k?}$ If yes, the search
			is successful, and we return the index 7. Otherwise, we ask, is ${T[7] <
			k}$ or is ${T[7] > k?}$ Here, ${T[7] < k.}$ Because ${T[7] < k,}$ ${k}$
			must be on the left-hand side of the tuple. I.e., ${\ell \leq k < M.}$ Why
			can we conclude this? Because the tuple ${T}$ is sorted. If ${T[7] > k,}$
			then ${k}$ must be on the right-hand side (${M < k \leq h}$).
		</p>
		<p>
			In this case, since ${T[7] < k,}$ ${\ell}$ remains as ${i = 0,}$ but we
			change ${h}$ to ${i = 6.}$ Now we're working the left-hand side, which is
			${T[0] < k \leq T[6].}$ Now we have a new mean: ${(0 + 6) / 2 = 3.}$ Thus,
			now the middle element is ${T[3] = 15.}$ We again ask, is ${k = T[3]?}$
			No. Is ${k < T[3]}$ or is ${k > T[3]?}$ Here, ${18 > 15,}$ so we're
			looking at the right-hand side of the tuple.
		</p>
		<p>
			Because ${T[M] > k,}$ we again change the lower bounds and upper bounds.
			${\ell = 4,}$ and ${h = 4.}$ The mean is now ${M = (4 + 4) / 2 = 4.}$
			Thus, the middle element is now ${T[4] = 18.}$ We now ask, is ${k =
			T[4]?}$ Yes. We have found our key. Notice that we found our key in a
			total of 4 comparisons, compared to 5 comparisons with linear search.
		</p>
		<p>
			Let's try another key. ${k = 34.}$ We set the upper bounds and lower
			bounds. ${\ell = 0}$ and ${h = 14.}$ Then we compute the mean index. ${(0
			+ 14) / 2 = 7.}$ Again we have ${T[7] = 27.}$ Is ${T[M] = k?}$ No. Is
			${T[M] > k}$ or is ${T[M] < k?}$ ${27 < 34.}$ Thus, we're now restricting
			our search to the right-hand side.
		</p>
		<p>
			We change the upper and lower bounds. ${\ell = M + 1 = 7 + 1 = 8.}$ And
			${h = 14.}$ Then compute the mean index: ${(8 + 14) / 2 = 11.}$ Thus, the
			middle element is ${T[M] = T[11] = 37.}$ Is ${T[M] = k?}$ No. Is ${T[M] >
			k}$ or ${T[M] < k?}$ ${37 > 34,}$ so we're looking at the left-hand side.
		</p>
		<p>
			We again change the upper and lower bounds. ${\ell = 8}$ and ${h = 11 - 1
			= 10.}$ Computing the mean: ${(10 + 8) / 2 = 9.}$ Is ${(T[9] = 33) = 34?}$
			No. Is ${33 > 34}$ or is ${33 < 34?}$ ${34}$ is less than ${33.}$ So, the
			number is on the right-hand side. Once more we modify the upper and lower
			bounds.
		</p>
		<p>
			${\ell = M + 1 = 9 + 1 = 10,}$ and ${h = 10.}$ We compute the mean: ${(10
			+ 10) / 2 = 10.}$ Is ${T[10] = 34?}$ Yes. We've found our key in 4
			comparisons. Compare that with linear search, which would have taken 10
			comparisions.
		</p>
		<p>
			Let's try another key, ${k = 25.}$ Set the upper and lower bounds: ${\ell
			= 0}$ and ${h = 14.}$ Compute the mean: ${M = (0 + 14) = 7.}$ Compare
			${T[7]}$ to ${k.}$ ${k}$ is not equal to ${T[7].}$ ${k < T[M],}$ so we're
			looking at the left-hand side.
		</p>
		<p>
			Change the upper and lower bounds. ${\ell = 0,}$ and ${h = 7 - 1 = 6.}$
			Compute the mean: ${M = (0 + 6)/2 = 3.}$ ${T[3]}$ is not equal to ${k.}$
			Because ${k > T[M],}$ we're looking at the right-hand side. So we change
			the upper and lower bounds.
		</p>
		<p>
			${\ell = 3 + 1 = 4,}$ and ${h = 6.}$ We compute the mean: ${(4 + 6) / 2 =
			5.}$ The middle element is ${T[5].}$ ${T[5]}$ is not equal to ${25,}$ ${k
			> T[5].}$ So again we're looking at the left-hand side. Once more we
			modify the lower and upper bounds.
		</p>
		<p>
			${\ell = 5 + 1 = 6,}$ and ${h = 6.}$ The mean: ${(6 + 6) / 2 = 6.}$
			${T[6]}$ is not equal to ${25.}$ ${25 > T[6],}$ so we modify the upper and
			lower bounds.
		</p>
		<p>
			${\ell = 6 + 1 = 7,}$ and ${h = 6.}$ This is where we stop. The moment
			${\ell > h,}$ we've arrived at an unsuccessful search, so we return
			${-1.}$ Notice that it took 4 comparisons to reach an unsuccessful search,
			compared to the 15 comparisions it would've taken with linear search.
		</p>
		<p>Implementing binay search in pseudocode:</p>
		<figure class="math-display">
			<div>
				<pre class="language-pseudo"><code>
					BinarySearch(k, low, high) {
						while (low <= high) {
							middle = floor((low + high) / 2);
							if (k == T[mean]) {
								return mean;
							}
							else if (k < T[mean]) {
								high = mean - 1;
							}
							else {
								low = mean + 1;
							}
						}
						return -1;
					}
				</code></pre>
			</div>
		</figure>
		<p>
			Alternatively, we can implement the binary search algorithm with tail
			recursion:
		</p>
		<figure class="math-display">
			<div>
				<pre class="language-pseudo"><code>
					BinarySearch(k, low, high) {
						if (low <= high) {
							mean = floor((low + high) / 2);
							if (k == T[mean]) {
								return mean;
							}
							else if (key < T[mean]) {
								return BinarySearch(low, mean - 1, key);
							}
							else {
								return BinarySearch(mean + 1, high, key);
							}
						}
						return -1;
					}
				</code></pre>
			</div>
		</figure>
		<p>
			To understand the time complexity for binary search, the crucial point is
			that we're cutting the tuple in half each time. However, before any of
			that splitting, we checked if the middle element is in fact the key.
			Accordingly, in the best-case scenario, binary search has a time
			complexity of ${\Omega(1)}$ &mdash; constant time.
		</p>
		<p>
			However, if the middle element was not the key, then we proceed to
			checking if the left- or right-hand side contained the key. Suppose it's
			the left-hand side. If it's the left-hand side, then ${\ell = 0}$ and ${h
			= 6,}$ yielding ${M = 3.}$ If it's on the right-hand side, then ${\ell =
			8}$ and ${h = 14,}$ yielding ${M = 11.}$ This is halving the tuple and
			restricting our search to just the relevant half. If the middle element in
			either half is the key, then we've found our key in 2 comparisons.
		</p>
		<p>
			If neither of the halves contains the middle element, then we again
			perform the split. For the left-hand side of the left-hand side, ${\ell =
			0}$ and ${h = 2,}$ yielding ${M = 1.}$ If it's the right-hand side of the
			left-hand side, then ${\ell = 4}$ and ${h = 6,}$ yielding ${M = 5.}$ For
			the left-hand side of the right-hand side, we have ${\ell = 8}$ and ${h =
			10,}$ yielding ${M = 9.}$ For the right-hand side of the right-hand side,
			we have ${\ell = 12}$ and ${h = 14,}$ yielding ${M = 13.}$ In this case,
			the key can be found in 3 comparisons. Visualizing the tree:
		</p>
		<figure>
			<img
				src="{% static 'images/binary_search_tree.svg' %}"
				alt="The binary search tree's middle element computations"
				loading="lazy"
			/>
		</figure>
		<p>
			Splitting further, the key can be found in 4 comparisions, then 5, then 6,
			and so on. Modelling binary search as a tree, the height of the tree is
			${\lceil\log_{2}(n + 1)\rceil,}$ where ${n}$ is the number of elements in
			the tuple. Why ${\log_{2}(n + 1)?}$ Because the tree's height is directly
			proportional to the number of times we divide ${n}$ by 2 such that we only
			have a remainder of 1. And as we know from mathematics, the answer to
			&#8220;How many times can I divide ${n}$ by 2 to reach a remainder of
			1?&#8221; is a logarithm of base 2. Successive multiplication yields a
			power, and successive division yields a logarithm. Thus, in terms of
			asymptotic analysis, the binary search algorithm has a runtime of order
			${O(\lg n)}$ &mdash; logarithmic time.
		</p>
		<p>
			What about the average-case? For the average-case, we're asking for the
			total time for all the possible cases, divided by the number of cases.
			Revisiting the tree diagram above, we see that at the first level, there
			is only 1 case, with only 1 comparison. Let ${c}$ be the number of cases:
		</p>
		<figure class="math-display">
			<div>
				<p>${c = 1}$</p>
			</div>
		</figure>
		<p>On the second level, there are 2 cases, each requiring 1 comparison:</p>
		<figure class="math-display">
			<div>
				<p>${c = 1 + (1 \cdot 2)}$</p>
			</div>
		</figure>
		<p>On the third level, there are 4 cases, each requiring 2 comparisons:</p>
		<figure class="math-display">
			<div>
				<p>${c = 1 + (1 \cdot 2) + (2 \cdot 4)}$</p>
			</div>
		</figure>
		<p>On the fourth level, there are 8 cases, each requiring 3 comparisons:</p>
		<figure class="math-display">
			<div>
				<p>${c = 1 + (1 \cdot 2) + (2 \cdot 4) + (3 \cdot 8)}$</p>
			</div>
		</figure>
		<p>We can see pattern in this series:</p>
		<figure class="math-display">
			<div>
				<p>${c = 1 + (1 \cdot 2^1) + (2 \cdot 2^2) + (3 \cdot 2^3)}$</p>
			</div>
		</figure>
		<p>We can then express this pattern as:</p>
		<figure class="math-display">
			<div>
				<p>${\sum\limits_{i = 1}^{3} i \cdot 2^i}$</p>
			</div>
		</figure>
		<p>
			That 3 is intresting &mdash; it's the tree's height. Accordingly, our
			series can be generalized as:
		</p>
		<figure class="math-display">
			<div>
				<p>
					${\sum\limits_{i = 1}^{\log n} i \cdot 2^i = \log n \cdot 2^{\log n}}$
				</p>
			</div>
		</figure>
		<p>
			But, we must divide this sum by ${n,}$ the number of elements (and by
			implication, the number of possible cases):
		</p>
		<figure class="math-display">
			<div>
				<p>${\dfrac{\log n \cdot 2^{\log n}}{n} = \log n}$</p>
			</div>
		</figure>
		<p>
			Hence, the average-case running time is ${\Theta(\log n)}$ An algorithm's
			average-case, however, may be different depending on the conditions. For
			example, with respect to binary search, there's an average-case running
			time for a successful search, and an average-case running time for an
			unsuccessful search.
		</p>
		<p>
			A successful search occurs if the element is found. In the tree diagram
			above, the number of possible cases for a successful search is equivalent
			to the number of internal nodes (i.e., all of the nodes other than the
			nodes on the fifth level). If we examine the tree closely, the number of
			edges from the root to a given node represents the number of comparisons
			needed to reach the given node. For example, to find the ${k = 8,}$
			binary-search requires 3 comparisons. Accordingly, the number of
			comparisons to reach a given node containing ${k}$ is ${e + 1,}$ where
			${e}$ is the number of edges leading to the node from the root (we add
			${1}$ because a comparison must be performed for the root).
		</p>
		<p>
			The number of possible cases for an unsuccesful search, in contrast, is
			found from the number of nodes on the fifth level. These are called
			<span class="term">external nodes</span>. It is on these external nodes
			that binary search terminates. Like the successful search, the number of
			comparisons needed to reach an unsuccessful search is the number of edges
			leading to an external node plus 1.
		</p>
		<p>
			With these basic facts, suppose that ${I}$ represents the sum of all the
			paths leading to internal nodes and ${E}$ represents the sum of the paths
			leading to external nodes. It follows then that:
		</p>
		<figure class="math-display">
			<div>
				<p>${E = I + 2n,}$ where ${n}$ is the number of internal nodes</p>
			</div>
		</figure>
		<p>
			Why ${+ 2n?}$ Because each of the internal nodes have 2 children (since
			the binary search algorithm works by successively dividing the problem by
			2). For example, here's a simple binary tree:
		</p>
		<figure>
			<img
				src="{% static 'images/internal_v_external_nodes.svg' %}"
				alt="Internal versus external nodes"
				loading="lazy"
				class="sixty-p"
			/>
		</figure>
		<p>
			Notice that ${n = 3.}$ Thus, ${2n = 6.}$ Then, notice that ${I = 2.}$
			Hence, ${E = 2 + 6 = 8.}$ Indeed, there are 8 paths from the root to the
			external nodes.
		</p>
		<p>
			Alongside the facts above, we also know that the number of external nodes
			is the number of internal does plus 1: ${e = n + 1,}$ where ${e}$ is the
			number of external nodes, and ${n}$ is the number of internal nodes.
			Knowing all of this, the average-case runtime for a successful search on
			an ${n}$ element tuple is the sum of the paths to internal nodes divided
			by the number of nodes (the sum of all possible cases divided by the
			number of cases) plus 1 (including the comparison for the root node):
		</p>
		<figure class="math-display">
			<div class="rule">
				<p>
					${\Omega\left(1 + \dfrac{I}{n}\right)}$, where ${I}$ is the sum of all
					paths to internal nodes and ${n}$ is the number of nodes.
				</p>
			</div>
		</figure>
		<p>Similarly, the average-case runtime for an unsuccesful search is:</p>
		<figure class="math-display">
			<div class="rule">
				<p>
					${\Omega\left( \dfrac{E}{n + 1} \right),}$ where ${E}$ is the sum of
					all paths to external nodes and ${n}$ is the number of nodes
				</p>
			</div>
		</figure>
		<p>
			Note that the sum of all paths to external nodes is roughly the height of
			the tree &mdash; ${n \log n.}$ Accordingly we can generalize the
			average-case runtime for an unsuccessful search as:
		</p>
		<figure class="math-display">
			<div class="rule">
				<p>${\Omega\left( \dfrac{n \log n}{n + 1} \right)}$</p>
			</div>
		</figure>
		<p>Generalizing even further:</p>
		<figure class="math-display">
			<div class="rule">
				<p>${\Omega(\log n)}$</p>
			</div>
		</figure>
		<p>
			Using what we know about ${E,}$ we can use substitution to find a more
			generalized version of the average-case runtime for a successful search.
			Where ${A_s(n)}$ is the average-case runtime for a successful search:
		</p>
		<figure class="math-display">
			<div>
				$$ \begin{aligned} A_s(n) &= 1 + \dfrac{I}{n} \\[1em] &= 1 + \dfrac{E -
				2n}{n} \\[1em] &= 1 + \dfrac{E}{n} - 2 \\[1em] &= 1 + \dfrac{n \log
				n}{n} - 2 \\[1em] &= 1 + \log n + 2 \end{aligned} $$
			</div>
		</figure>
		<p>
			Applying asymptotic analysis, the average-case runtime for a successful
			search is:
		</p>
		<figure class="math-display">
			<div>
				<p>${\Omega (\log n)}$</p>
			</div>
		</figure>
	</section>

	<section id="getting">
		<p>
			<span class="topic">Accessing an Array Element.</span> In the API above,
			we specified an operation called <span class="monoText">get(${i}$)</span>,
			where ${i}$ is an index. This operation returns the tuple's element at
			index ${i.}$
		</p>
		<p>
			Really, there are only two operations we must perform for
			<span class="monoText">get(${i}$)</span>: First, we must ensure the index
			argument ${i}$ is an integer not less than 0. It wouldn't make sense to
			pass an ${i}$ of 2.178 or an ${i}$ of -1. Second, we must ensure the index
			is less than <span class="monoText">length</span>. If the ${i >
			\texttt{length},}$ then we risk (a) getting some garbage value or (b)
			receiving an out-of-bounds error. The implementation:
		</p>
		<figure class="math-display">
			<div>
				<pre class="language-pseudo"><code>
					get(int index) {
						if ( (index >= 0) ∧ (index < length) ) {
							⟹ return A[index];
						} 
					}
				</code></pre>
			</div>
		</figure>
		<p>
			Because only two operations are required for
			<span class="monoText">get(${i}$)</span>, the
			<span class="monoText">get(${i}$)</span> operator has a time complexity of
			${O(1)}$ &mdash; constant time.
		</p>
	</section>

	<section id="setting">
		<p>
			<span class="topic">Replacing an Array Element.</span> The
			<span class="monoText">set(${x}$, ${i}$)</span> operation will set the
			element ${x}$ and the index ${i.}$ Effectively, this will replace the
			element ${y}$ occupying the index ${i.}$ Like the
			<span class="monoText">get(${i}$)</span> operator, we must ensure that the
			index passed is an integer and that ${i < \texttt{length}.}$ In
			pseudocode:
		</p>
		<figure class="math-display">
			<div>
				<pre class="language-pseudo"><code>
					get(int x, int index) {
						if ( (index >= 0) ∧ (index < length) ) {
							⟹ A[index] = x;
						} 
					}
				</code></pre>
			</div>
		</figure>
		<p>
			Once more, because there are really only two operations performed
			(ensuring the requirements are met and assignment), replacing an element
			in an array has running time of ${O(1)}$ &mdash; constant time.
		</p>
	</section>

	<section id="maximum">
		<p>
			<span class="topic">Finding the Maximum in an Array.</span> Finding the
			maximum element in a given array is an instance of a
			<span class="term">peak finding</span> problem. For example, suppose we
			had the following array:
		</p>
		<figure class="table">
			<table class="array">
				<tbody>
					<tr>
						<td>${u = }$</td>
						<td>8</td>
						<td>3</td>
						<td>9</td>
						<td>15</td>
						<td>6</td>
						<td>10</td>
						<td>7</td>
						<td>2</td>
						<td>12</td>
						<td>4</td>
					</tr>
					<tr>
						<td>${i = }$</td>
						<td>0</td>
						<td>1</td>
						<td>2</td>
						<td>3</td>
						<td>4</td>
						<td>5</td>
						<td>6</td>
						<td>7</td>
						<td>8</td>
						<td>9</td>
					</tr>
				</tbody>
			</table>
		</figure>
		<p>
			The <span class="italicsText">peak</span>, or
			<span class="italicsText">maximum</span>, in the array above is the
			element ${15.}$ We will see in later sections that peak finding extends to
			many other problem domains. For now, we focus on the array data structure.
		</p>
		<p>
			How do we find the maximum element? It depends on whether the array's
			elements are sorted. Yet another example of why sorting algorithms are so
			important. For unsorted lists, we have no recourse other than to check
			each of the elements one by one. To do so, we first suppose that the
			maximum is the first element in the array &mdash; ${max = u_0 = 8.}$ We
			then compare ${u_0}$ with ${u_1.}$ If ${u_1 > u_0,}$ then ${max = u_1.}$
			Otherwise, we compare ${u_0}$ with ${u_2.}$ If ${max = u_1,}$ then we
			compare ${u_1}$ with ${u_2.}$ If ${u_2 > u_1,}$ then ${max = u_2.}$
			Otherwise, we compare ${u_2}$ with ${u_3.}$ We keep doing so until we
			reach the very last element. In pseudocode:
		</p>
		<figure class="math-display">
			<div>
				<pre class="language-pseudo"><code>
					max() {
						max = u[0];
						for (int i = 1; i < length; i++) {
							if (u[i] > max) {
								max = u[i];
							}
						}
						return max;
					}
				</code></pre>
			</div>
		</figure>
		<p>
			With this approach, we're effectively performing an instance of a linear
			search. We must traverse the entire array, checking each element (starting
			with the second element) all the way up to the last. Counting the number
			of operations, we have:
		</p>
		<figure class="math-display">
			<div>
				<pre class="language-pseudo"><code>
					max() {
						max = u[0]; <span class="redText">&larr; 1 operation</span>
						for (int i = 1; i < length; i++) { <span class="redText">&larr; n operations</span>
							if (u[i] > max) { <span class="redText">&larr; n - 1 operations</span>
								max = u[i];
							}
						}
						return max; <span class="redText">&larr; 1 operation</span>
					}
				</code></pre>
			</div>
		</figure>
		<p>
			Thus, there are a total of ${1 + 1 + n + (n - 1)}$ operations, or ${2n +
			1.}$ Accordingly, because the time function is ${f(n) = 2n + 1,}$ the
			above approach has a time complexity of ${O(n)}$ &mdash; linear time.
		</p>
		<p>
			On the other hand, if the list is sorted, then the last element is the
			maximum.
		</p>
	</section>

	<section id="finding_minimum">
		<p>
			<span class="topic">Finding a Minimum.</span> The
			<span class="monoText">min()</span> operator returns the minimum element
			in the array. Finding a minimum is an instance of a
			<span class="term">valley-finding problem</span>. We can think of it as
			the inverse of finding a maximum. Like peak-finding, how efficiently we
			can find a minimum depends on whether the array's elements are sorted. If
			the elements are unsorted, then we have no recourse other than to use a
			linear search approach.
		</p>
		<p>Supppose we had the same array from the previous section:</p>
		<figure class="table">
			<table class="array">
				<tbody>
					<tr>
						<td>${u = }$</td>
						<td>8</td>
						<td>3</td>
						<td>9</td>
						<td>15</td>
						<td>6</td>
						<td>10</td>
						<td>7</td>
						<td>2</td>
						<td>12</td>
						<td>4</td>
					</tr>
					<tr>
						<td>${i = }$</td>
						<td>0</td>
						<td>1</td>
						<td>2</td>
						<td>3</td>
						<td>4</td>
						<td>5</td>
						<td>6</td>
						<td>7</td>
						<td>8</td>
						<td>9</td>
					</tr>
				</tbody>
			</table>
		</figure>
		<p>
			The valley here is the element ${u_7 = 2.}$ To find this element, we start
			by assuming the first element, ${u_0,}$ is the minimum. We then check if
			${u_1 < u_0.}$ If it is, then the minimum is ${u_1,}$ and we compare that
			to ${u_2.}$ If it isn't, then the minimum remains ${u_0,}$ and we compare
			it against ${u_2.}$ We repeat this process all the way up to the last
			element. In pseudocode:
		</p>
		<figure class="math-display">
			<ol class="alg">
				<li>min():</li>
				<ol>
					<li>
						min = u[0] <span class="greyText">${\text{1 operation}}$</span>
					</li>
					<li>
						for (i = 1; i < length; i++):
						<span class="greyText">${n \text{ operations} }$</span>
					</li>
					<ol>
						<li>
							if (u[i] < min):
							<span class="greyText">${n - 1 \text{ operations}}$</span>
						</li>
						<ol>
							<li>min = u[i]</li>
						</ol>
					</ol>
					<li>
						return min <span class="greyText">${\text{1 operation}}$</span>
					</li>
				</ol>
			</ol>
		</figure>
		<p>
			Examining the operation count above, we again see that the running time
			function is ${f(n) = 1 + n + (n - 1) + 1 = 2n + 1.}$ Accordingly, this
			approach also has a time complexity of ${O(n)}$ &mdash; linear time.
		</p>
	</section>

	<section id="summing_elements">
		<p>
			<span class="topic">Summing Elements.</span> The
			<span class="monoText">sum()</span> operator will return the sum of all
			the elements in the array. Summing the elements of an array requires
			traversing the entire element, adding each element to some variable ${t}$
			representing the sum. In pseudocode:
		</p>
		<figure class="math-display">
			<ol class="alg">
				<li>sum():</li>
				<ol>
					<li>
						total = 0; <span class="greyText">${\text{1 operation}}$</span>
					</li>
					<li>
						for (i = 0; i < length; i++):
						<span class="greyText">${n + 1 \text{ operations}}$</span>
					</li>
					<ol>
						<li>
							total += array[i]
							<span class="greyText">${n \text{ operations}}$</span>
						</li>
					</ol>
				</ol>
				<li>
					return total <span class="greyText">${\text{1 operation}}$</span>
				</li>
			</ol>
		</figure>
		<p>
			From the operation count above, we see that this approach has a running
			time function of ${f(n) = 2n + 3.}$ Accordingly, this algorithm has a time
			complexity of ${O(n),}$ which is linear time.
		</p>
		<p>Alternatively, we can apply a recursive implementation:</p>
		<figure class="math-display">
			<ol class="alg">
				<li>sum(array):</li>
				<ol>
					<li>if (array.length - 1 > 0):</li>
					<ol>
						<li>return 0;</li>
					</ol>
					<li>else:</li>
					<ol>
						<li>return sum(A, array.length - 1) + A[array.length]</li>
					</ol>
				</ol>
			</ol>
		</figure>
		<p>
			The recursive implementation also has a time complexity of ${O(n).}$ The
			difference, however, is that each recursive call will result in a new
			stack generation, leading to a space complexity of ${O(n).}$ This is in
			contrast to the iterative approach, which has a space complexity of
			${O(1).}$
		</p>
	</section>

	<section id="averaging">
		<p>
			<span class="topic">Computing the Arithmetic Mean.</span> A useful
			operator to have when working with arrays is returning the
			<span class="italicsText">arithmetic mean</span>. This is done with the
			<span class="monoText">ArithmeticMean()</span> operator. The
			implementation is straightforward. We simply compute the sum of all the
			elements, and divide it by the number of elements.
		</p>
		<figure class="math-display">
			<ol class="alg">
				<li>average():</li>
				<ol>
					<li>
						total = 0; <span class="greyText">${\text{1 operation}}$</span>
					</li>
					<li>
						for (i = 0; i < length; i++):
						<span class="greyText">${n + 1 \text{ operations}}$</span>
					</li>
					<ol>
						<li>
							total += array[i]
							<span class="greyText">${n \text{ operations}}$</span>
						</li>
					</ol>
				</ol>
				<li>
					return total / length
					<span class="greyText">${\text{2 operations}}$</span>
				</li>
			</ol>
		</figure>
		<p>
			As we can see, the time complexity is no different, save for the extra
			operation in the last line of dividing by the length. Accordingly, this
			approach has a time complexity of ${O(n)}$ &mdash; linear time.
		</p>
	</section>

	<section id="reversing">
		<h4>Reversing the Elements of an Array</h4>
		<p>
			The
			<span class="monoText">reverse()</span> operator performs exactly what it
			sounds like: It reverses all the original elements of the array. For
			example, given the array:
		</p>
		<figure class="table">
			<table class="array">
				<tbody>
					<tr>
						<td>${A = }$</td>
						<td>8</td>
						<td>3</td>
						<td>9</td>
						<td>15</td>
						<td>6</td>
						<td>10</td>
						<td>7</td>
						<td>2</td>
						<td>12</td>
						<td>14</td>
					</tr>
					<tr>
						<td>${i = }$</td>
						<td>0</td>
						<td>1</td>
						<td>2</td>
						<td>3</td>
						<td>4</td>
						<td>5</td>
						<td>6</td>
						<td>7</td>
						<td>8</td>
						<td>9</td>
					</tr>
				</tbody>
			</table>
		</figure>
		<p><span class="monoText">reverse()</span> returns the array:</p>
		<figure class="table">
			<table class="array">
				<tbody>
					<tr>
						<td>${A = }$</td>
						<td>14</td>
						<td>12</td>
						<td>2</td>
						<td>7</td>
						<td>10</td>
						<td>6</td>
						<td>15</td>
						<td>9</td>
						<td>3</td>
						<td>8</td>
					</tr>
					<tr>
						<td>${i = }$</td>
						<td>0</td>
						<td>1</td>
						<td>2</td>
						<td>3</td>
						<td>4</td>
						<td>5</td>
						<td>6</td>
						<td>7</td>
						<td>8</td>
						<td>9</td>
					</tr>
				</tbody>
			</table>
		</figure>
		<p>
			To reverse an array, there are two methods we can use: (1)
			<span class="italicsText">reversing in place</span> and (2) using an
			<span class="italicsText">auxiliary array</span>. We first consider the
			latter.
		</p>

		<section id="auxiliary_array">
			<p>
				<span class="topic">Reversing with an Auxiliary Array.</span> With the
				auxiliary array approach, we use an additional array to place the
				reversed elements. The idea is straightforward: We start at the last
				element of the array and traverse to the first element, copying each
				element to the auxiliary array. After copying all of the elements into
				the auxiliary array, we then replace the corresponding elements in the
				original array with the elements in the auxiliary array.
			</p>
			<p>Implementing this approach in pseudocode:</p>
			<figure class="math-display">
				<ol class="alg">
					<li>reverse():</li>
					<ol>
						<li>for (i = length - 1, j = 0; i >= 0; i--, j++):</li>
						<ol>
							<li>auxiliaryArr[j] = originalArr[i];</li>
						</ol>
						<li>for (i = 0; i < lenght; i++):</li>
						<ol>
							<li>originalArr[i] = auxiliaryArr[j];</li>
						</ol>
					</ol>
				</ol>
			</figure>
			<p>
				What is this approach's time complexity? Copying the elements from the
				original array to the auxiliary array takes ${n}$ operations, since we
				must traverse through ${n}$ elements. Then, copying the elements from
				the auxiliary array back to the original array takes ${n}$ operations,
				again because we must traverse through ${n}$ elements. Accordingly, this
				approach has a time function of roughly ${f(n) = 2n.}$ Asymptotically,
				the approach has a time complexity of order ${O(n)}$ &mdash; linear
				time.
			</p>
		</section>

		<section id="reversing_in_place">
			<p>
				<span class="topic">Reversing in Place.</span> With the reverse-in-place
				approach, we swap the first and the last elements, then the second and
				the second to last, then the third and the third to last, and so on.
				This is done by assigning two indexing variables: ${i}$ to track indices
				offsetting from and including the first element, and ${j}$ to track
				indices offsetting from and including the last element. We continue the
				process until either ${i}$ and ${j}$ have arrived at the same index, or
				when ${i > j.}$ The implementation:
			</p>
			<figure class="math-display">
				<ol class="alg">
					<li>reverse():</li>
					<ol>
						<li>for (i = 0, j = length - 1; i < j; i++, j--):</li>
						<ol>
							<li>temp = A[i];</li>
							<li>A[i] = A[j];</li>
							<li>A[j] = temp;</li>
						</ol>
					</ol>
				</ol>
			</figure>
			<p>
				With the implementation above, we can see that this approach also
				requires traversing through the array's ${n}$ elements. Accordingly,
				this approach, too, has a running time of ${O(n);}$ linear time.
			</p>
		</section>
	</section>

	<section id="left_shift">
		<h4>Shifting & Rotating</h4>
		<p>
			The next operations we consider are
			<span class="monoText">leftShift()</span>,
			<span class="monoText">rightShift()</span>,
			<span class="monoText">leftRotate()</span>, and
			<span class="monoText">rightRotate()</span>. Each of these operations
			involves moving elements to the left or right &mdash; particularly useful
			actions when implementing other algorithms (e.g., filling in holes in the
			array).
		</p>
		<p>
			<span class="topic">Shifting an Element.</span> Suppose we had the
			following array:
		</p>
		<figure class="table">
			<table class="array">
				<tbody>
					<tr>
						<td>${A = }$</td>
						<td>6</td>
						<td>3</td>
						<td>8</td>
						<td>5</td>
						<td>9</td>
					</tr>
					<tr>
						<td>${i = }$</td>
						<td>0</td>
						<td>1</td>
						<td>2</td>
						<td>3</td>
						<td>4</td>
					</tr>
				</tbody>
			</table>
		</figure>
		<p>
			The operation of left-shifting is to shift all of the elements in the
			array to the left:
		</p>
		<figure class="table">
			<table class="array">
				<tbody>
					<tr>
						<td>${A = }$</td>
						<td>3</td>
						<td>8</td>
						<td>5</td>
						<td>9</td>
						<td></td>
					</tr>
					<tr>
						<td>${i = }$</td>
						<td>0</td>
						<td>1</td>
						<td>2</td>
						<td>3</td>
						<td>4</td>
					</tr>
				</tbody>
			</table>
		</figure>
		<p>
			Notice that in doing so, we lose the element 6. This is the natural
			consequence of left-shifting. We're shifting everything to the left,
			losing whichever element is at index ${i = 0}$ before the left-shift.
		</p>
		<p>
			The same idea extends to right-shifting. The difference, however, is that
			we're shifting elements to the right. Starting with the same previous
			array:
		</p>
		<figure class="table">
			<table class="array">
				<tbody>
					<tr>
						<td>${A = }$</td>
						<td>6</td>
						<td>3</td>
						<td>8</td>
						<td>5</td>
						<td>9</td>
					</tr>
					<tr>
						<td>${i = }$</td>
						<td>0</td>
						<td>1</td>
						<td>2</td>
						<td>3</td>
						<td>4</td>
					</tr>
				</tbody>
			</table>
		</figure>
		<p><span class="monoText">rightShift()</span> results in:</p>
		<figure class="table">
			<table class="array">
				<tbody>
					<tr>
						<td>${A = }$</td>
						<td></td>
						<td>6</td>
						<td>3</td>
						<td>8</td>
						<td>5</td>
					</tr>
					<tr>
						<td>${i = }$</td>
						<td>0</td>
						<td>1</td>
						<td>2</td>
						<td>3</td>
						<td>4</td>
					</tr>
				</tbody>
			</table>
		</figure>

		<p>
			<span class="topic">Rotating an Array.</span> Suppose performed a
			left-shift, pushing the element ${6}$ out of the array. What if we wanted
			to keep the ${6}$ somehow? To do so, we use the
			<span class="monoText">leftRotate()</span> operator. Starting with this
			array:
		</p>
		<figure class="table">
			<table class="array">
				<tbody>
					<tr>
						<td>${A = }$</td>
						<td>6</td>
						<td>3</td>
						<td>8</td>
						<td>5</td>
						<td>9</td>
					</tr>
					<tr>
						<td>${i = }$</td>
						<td>0</td>
						<td>1</td>
						<td>2</td>
						<td>3</td>
						<td>4</td>
					</tr>
				</tbody>
			</table>
		</figure>
		<p><span class="monoText">leftRotate()</span> yields:</p>
		<figure class="table">
			<table class="array">
				<tbody>
					<tr>
						<td>${A = }$</td>
						<td>3</td>
						<td>8</td>
						<td>5</td>
						<td>9</td>
						<td>6</td>
					</tr>
					<tr>
						<td>${i = }$</td>
						<td>0</td>
						<td>1</td>
						<td>2</td>
						<td>3</td>
						<td>4</td>
					</tr>
				</tbody>
			</table>
		</figure>
		<p>
			Notice that the element ${6}$ moves to ${i = length}$ when the left-shift
			pushes it out of the array. The same operation extends to rotating to the
			right &mdash; <span class="monoText">rightRotate()</span>. Starting with:
		</p>
		<figure class="table">
			<table class="array">
				<tbody>
					<tr>
						<td>${A = }$</td>
						<td>6</td>
						<td>3</td>
						<td>8</td>
						<td>5</td>
						<td>9</td>
					</tr>
					<tr>
						<td>${i = }$</td>
						<td>0</td>
						<td>1</td>
						<td>2</td>
						<td>3</td>
						<td>4</td>
					</tr>
				</tbody>
			</table>
		</figure>
		<p><span class="monoText">rightRotate()</span> yields:</p>
		<figure class="table">
			<table class="array">
				<tbody>
					<tr>
						<td>${A = }$</td>
						<td>9</td>
						<td>6</td>
						<td>3</td>
						<td>8</td>
						<td>5</td>
					</tr>
					<tr>
						<td>${i = }$</td>
						<td>0</td>
						<td>1</td>
						<td>2</td>
						<td>3</td>
						<td>4</td>
					</tr>
				</tbody>
			</table>
		</figure>
		<p>
			Because shifting and rotating an unsorted array requires traversing
			through all the array's elements, both shifting and rotating has a time
			complexity of ${O(n).}$
		</p>
	</section>

	<section id="inserting_into_a_sorted_array">
		<p>
			<span class="topic">Inserting into a Sorted Array.</span> Suppose we had
			the following <span class="underlineText">sorted</span> array:
		</p>
		<figure class="table">
			<table class="array">
				<tbody>
					<tr>
						<td>${A = }$</td>
						<td>4</td>
						<td>8</td>
						<td>13</td>
						<td>16</td>
						<td>20</td>
						<td>25</td>
						<td>28</td>
						<td>33</td>
						<td></td>
						<td></td>
					</tr>
					<tr>
						<td>${i = }$</td>
						<td>0</td>
						<td>1</td>
						<td>2</td>
						<td>3</td>
						<td>4</td>
						<td>5</td>
						<td>6</td>
						<td>7</td>
						<td>8</td>
						<td>9</td>
					</tr>
				</tbody>
			</table>
		</figure>
		<p>
			When inserting into a sorted array like the one above, we often want to
			keep array's sorted property. In other words, we want to insert the
			element in a sorted position. In this case,
			<span class="monoText">insert(18)</span> should place the element ${18}$
			after ${A_{3} = 16.}$ This requires placing ${18}$ at ${i = 4,}$ which is
			presently occupied by ${A_{4} = 20.}$ Accordingly, we must make space for
			the element. This is done by shifting the element at 7, then 6, then 5,
			then 4.
		</p>
		<p>
			How might this shifting be implemented? Well, let's consider a simpler
			case. If we instead wrote <span class="monoText">insert(34)</span>, then
			no shifting would occur, since 34 would be the largest element. If,
			however, we wrote <span class="monoText">insert(32)</span>, then 33 would
			have to be shifted, since ${32 > 33.}$ Accordingly, when we shift
			elements, we keep right-shifting, starting from the last element, until we
			reach the <span class="underlineText">first</span> element less than the
			argument passed to <span class="monoText">insert()</span> (i.e., the
			element we want to insert).<label for="insert" class="margin-toggle"
				><sup></sup
			></label>
			<input
				type="checkbox"
				id="insert"
				class="margin-toggle sidenote-number"
			/>
			<span class="marginnote"
				>This algorithm only works because the array's elements are
				sorted.</span
			>
		</p>
	</section>
</section>

<section id="list">
	<h2>Lists</h2>
	<p>
		The <span class="term">list</span> data type is sequence of variable size.
		The data type may also be called a
		<span class="term">dynamic sequence</span> or
		<span class="term">vector</span>. With variable size, we suddenly have a
		much larger API. This shouldn't be all that surprising &mdash; the more
		properties we can mutate, the more operations we can implement.
	</p>
</section>

<section id="linked_lists">
	<p>
		<span class="topic">Linked Lists.</span> We can implement the dynamic
		sequence type above with a <span class="term">linked list</span>. In a
		linked list, elements in the sequence are stored in
		<span class="italicsText">nodes</span>. Each node consists of: (1) the
		sequence element and (2) a pointer to the next node.
	</p>
	<p>
		With linked lists, there are two points of particular interest: the
		<span class="italicsText">head</span> of the linked list (the first node in
		the linked list) and the <span class="italicsText">tail</span> of the list
		(the last node in the linked list). Importantly, the tail consists of a node
		whose pointer points to nothing (i.e., a
		<span class="italicsText">null pointer</span>).
	</p>
</section>

<section id="array_addressing">
	<h2>Array Addressing</h2>
	<p>
		Whenever we write expressions that index into an array, e.g.,
		<span class="monoText">A[2] = 4</span>, the compiler must translate the
		expression into an address. Because arrays are a fundamental data structure,
		it's worth knowing how this translation is done. Put simply, the compiler
		makes performs this translation by applying a particular formula. For
		example, suppose we initialized the following array:
	</p>
	<pre class="language-c"><code>
		int main() {
			int A[3] = {8, 3, 5};
			return 0;
		}
	</code></pre>
	<p>
		As we know, the identifier <span class="monoText">A</span> is an identifier
		for array's base address &mdash; the address of
		<span class="monoText">A[0]</span>. Suppose the address of
		<span class="monoText">A[0]</span> is <span class="monoText">200</span>.
		This means that the address of <span class="monoText">A[1]</span> is
		<span class="monoText">204</span> (since an
		<span class="monoText">int</span> takes 4 bytes), an the address of
		<span class="monoText">A[2]</span> is <span class="monoText">208</span>.
		Abstracting this computation, we can think of the compiler performing the
		following when it encounters <span class="monoText">A[2]</span>:
	</p>
	<figure class="math-display">
		<div>
			$$ \begin{aligned} \textit{address-of}(A[2])&= \textit{address-of}(A[0]) +
			(2)(4) \\ &= 200 + 8 \\ &= 209 \end{aligned} $$
		</div>
	</figure>
	<p>We can abstract this computation into a formula:</p>
	<figure class="math-display">
		<div>
			<p>${\alpha(a_i) = \alpha(a_0) + i \omega}$</p>
		</div>
	</figure>
	<p>
		In the formula above, ${\alpha(n)}$ is a function that returns the memory
		address of some data ${n}$. Thus, ${\alpha(a_i)}$ returns the address of the
		${i^{\text{\scriptsize{th}}}}$ element of the array, and ${\alpha(a_0)}$
		returns the address of the first element. The variable ${i}$ represents the
		index of the element, and the variable ${\omega}$ represents the size of the
		data type (e.g., the size of the data type
		<span class="monoText">int</span> is 4 bytes).
	</p>
	<p>
		As an aside, many older languages like Fortran use 1-based indexing. C came
		after Fortran, so why do so many languages use 0-based indexing? One reason
		is because of the hardware limitations at the time languages like C, BCPL,
		and Fortran were implemented. With 1-based indexing, we would have to use a
		different formula in determining the address of a given element:
	</p>
	<figure class="math-display">
		<div>
			<p>${\alpha(a_i) = \alpha(a_1) + (i-1)\omega}$</p>
		</div>
	</figure>
	<p>
		There's an additional computation with this formula &mdash; a decrement.
		This additional operation proves to be costly on older machines &mdash;
		there are 3 separate computations. Compare that with the previous formula,
		which calls for only 2 computations. Given that a core goal of C's design
		was efficiency, it made sense to opt for the more efficient implementation.
	</p>
	<p>
		Of course, modern computers have surpassed many of these limitations. The
		additional decrementing operation doesn't make much of a difference for most
		applications. So why then do recent languages go with zero-based indexing?
		Because zero-based indexing has become the norm. C inspired a whole host of
		languages (e.g., C++ and Objective-C), which in turn inspired many others
		(e.g., C to C++ to Java to Kotlin, C to Objective-C to Swift). Given that
		language designers are programmers first and foremost, it isn't all that
		surprising to see a designer sticking with what they're familiar with.
	</p>
	<p>
		Moreover, if often makes more sense to use zero-based indexing over
		one-based indexing. We can appreciate this idea by recognizing that
		<span class="italicsText">indexing</span> is
		<span class="underlineText">not</span> the same as
		<span class="italicsText">counting</span>. Instead, indexing is much more
		akin to <span class="italicsText">offsetting</span> from a given point. For
		example, given the array <span class="monoText">int A[3] = {1, 2, 3}</span>,
		the starting point is <span class="monoText">1</span>. If we designate that
		as the element with index 0 (<span class="monoText">A[0]</span>, the first
		element), then the second element, <span class="monoText">A[1]</span>, is
		the element 1-off the first element. The third element,
		<span class="monoText">A[2]</span>, is 2-off the first element. And so on
		and so forth.
	</p>
</section>

{% endblock %}
