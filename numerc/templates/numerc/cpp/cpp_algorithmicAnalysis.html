{% extends '../layout.html' %} {% load static %} {% block description %}
<meta
	name="description"
	content="How to perform complexity analysis: An overview of Big O notation, theta notation, omega notation, time complexity, and space complexity."
/>
<meta name="author" content="Ketib Oldiais" />
<meta
	name="keywords"
	content="complexity analysis, C++, Big O notation, theta notation, omega notation, time complexity, space complexity, complexity analysis exercises"
/>
{% endblock %} {% block title %}
<title>Complexity Analysis</title>
{% endblock %} {% block content %}
<h1>Algorithmic Analysis</h1>
<section id="intro">
	<p>
		<span class="drop">T</span>his section provides a general overview of
		algorithmic analysis in programming terms. We present the motivating
		question, its answer, as well as illustrations of the answer applied.
		The materials that follow assume a basic understanding of functions,
		limits, sequences, and series (i.e., comfort with ideas from a basic
		calculus and discrete mathematics course). For a thorough mathematical
		justification of algorithmic analysis,
		<a href="{% url 'numerm:disc_algorithms' %}"
			><i>see</i>
			<em>Review of Mathematics: Analysis of Algorithms</em>.</a
		>
	</p>
	<p>
		One concern for analyzing algorithms is
		<span class="italicsText">efficiency</span>. While this is the most
		common concern, it is second to another &mdash;
		<span class="italicsText">correctness</span>. If an algorithm is
		incorrect, its efficiency is unimportant.<sup></sup>
		In these materials, we suppose that we've gotten over the correctness
		hurdle, and focus on efficiency.
	</p>
	<div class="note">
		<p>
			For the most part. In later sections, we will see how analyzing the
			efficiency of an incorrect algorithm can lead to insight elsewhere.
		</p>
	</div>
	<p>
		The most common procedure for analyzing algorithms is determining the
		amount of <span class="italicsText">time</span> an algorithm takes to
		execute. In essence, how long it takes a given algorithm to solve a
		particular problem. To do so, however, we need some agreed-upon method
		for quantifying time. We can't rely on units like seconds, because
		every machine is different. Solving some differential equation on a
		small mobile phone will be slower than running the same search on a
		super computer. Although it may be useful to know how fast the search
		runs on one device over another, it's not the principal concern in the
		analysis of algorithms. The search algorithm is the same for both the
		small mobile and the super computer. What we want to know is how the
		algorithm compares to another search algorithm,
		<span class="italicsText">regardless of device</span>.
	</p>
	<div class="mainIdea">
		<p>
			Time complexity is the way we measure how much time an algorithm
			takes to execute (the algorithm's
			<span class="italicsText">runtime</span>) according to its input
			size.
		</p>
	</div>
	<p>
		Occasionally, analyzing algorithms requires determining the amount of
		<span class="italicsText">space</span> an algorithm takes to execute.
		For example, one algorithm ${A}$ might be faster than algorithm ${B}$
		but requires a significant amount of memory. Determining space
		complexity can often be immensely. Perhaps algorithms ${C}$ and ${D}$
		take the same amount of time, but ${C}$ consumes less memory.
	</p>
	<p>
		So how do we determine the amount of time or space an algorithm takes?
		The first step is to count the number of
		<span class="underlineText">fundamental operations</span> performed as
		a function of ${n,}$ where ${n}$ is the number of input values. In the
		pseudocode below, each line in the program on the left presents a
		unique operation. On the right are the time complexities for each line,
		along with comments (click on the comment to expand, or program line to
		see the accompanying comment).
	</p>
	<div class="pseudosource">
		<ol class="alg">
			<li><var>// declaration</var></li>
			<ol>
				<li>let a;</li>
			</ol>
			<li><var>// assignment</var></li>
			<ol>
				<li>let a = 0;</li>
				<li>let b = 1;</li>
				<li>
					let c = (${\texttt{E}_0}$) ? (${\texttt{E}_1}$) :
					(${\texttt{E}_2}$)
				</li>
			</ol>
			<li><var>// arithmetic operations</var></li>
			<ol>
				<li>a - b;</li>
				<li>a + b;</li>
				<li>a * b;</li>
				<li>a / b;</li>
				<li>a % b;</li>
				<li>a++;</li>
				<li>a--;</li>
				<li>a+=b;</li>
				<li>a-=b;</li>
				<li>a*=b;</li>
				<li>a/=b;</li>
				<li>a%=b;</li>
			</ol>
			<li><var>// bitwise operations</var></li>
			<ol>
				<li>a & b;</li>
				<li>a | b;</li>
				<li>a^b;</li>
				<li>~a;</li>
				<li>a&lt;&lt;b</li>
				<li>a&gt;&gt;b</li>
			</ol>
			<li><var>// comparison operations</var></li>
			<ol>
				<li>a > b;</li>
				<li>a &lt; b;</li>
				<li>a >= b;</li>
				<li>a <= b;</li>
				<li>a == b;</li>
				<li>a === b;</li>
				<li>a != b;</li>
			</ol>
			<li><var>// casting operations</var></li>
			<ol>
				<li>(T) a;</li>
			</ol>
			<li><var>// branching structures</var></li>
			<ol>
				<li>if (${\texttt{C}}$) { ${\texttt{E}}$ }</li>
				<li>else if (${\texttt{C}}$) { ${\texttt{E}}$ }</li>
				<li>else { ${\texttt{E}}$ }</li>
			</ol>
		</ol>
		<ol class="algc">
			<li>
				<span class="greyText">
					${\varnothing}$ Comments are typically removed by compilers
					before execution.</span
				>
			</li>

			<li>
				${O(1).}$ At the hardware level, declaration requires the CPU to
				search for available registers. Accordingly, declaration could
				potentially take ${O(\texttt{N}),}$ where ${N}$ is the number of
				registers available for program usage. However, because computer
				systems have a constant number of ${\texttt{N}}$ registers, we say
				that the declaration takes ${O(1)}$ time.
			</li>

			<li>
				<span class="greyText">${\varnothing}$ Comments are ignored.</span>
			</li>

			<li>
				${O(2).}$ We declare <var>a</var>, <em>and</em> assign it
				<var>0</var>. Again, because the CPU must search for available
				registers, initialization can potentially take ${O(\texttt{N} +
				x),}$ where ${\texttt{N}}$ is the number of registers, and ${x}$ is
				the number of machine language instructions the CPU must execute to
				store the value <var>1</var> inside the register. Because both
				${\texttt{N}}$ and ${x}$ are constants, we say that this operation
				takes ${O(2)}$ time, or more generally, ${O(1)}$ time.
			</li>
			<li>${O(2).}$ <i>See</i> previous comment.</li>
			<li>
				${[O(1 + \texttt{T}(\texttt{E}_0) + \texttt{T}(\texttt{E}_1)), O(1
				+ \texttt{T}(\texttt{E}_0) + \texttt{T}(\texttt{E}_2))].}$ Ternary
				operations have variable time complexities. Ultimately, it depends
				on the time complexity of checking the guard clause
				${\texttt{E}_0,}$ and executing either the true-block
				${\texttt{E}_1}$ or false-block ${\texttt{E}_2.}$
			</li>
			<li>
				<span class="greyText">${\varnothing}$ Comments are ignored.</span>
			</li>

			<li>${O(1).}$</li>

			<li>${O(1).}$</li>
			<li>${O(1).}$</li>
			<li>${O(1).}$</li>
			<li>${O(1).}$</li>
			<li>${O(2).}$</li>
			<li>${O(2).}$</li>
			<li>${O(2).}$</li>
			<li>${O(2).}$</li>
			<li>${O(2).}$</li>
			<li>${O(2).}$</li>
			<li>${O(2).}$</li>
			<li>
				<span class="greyText">${\varnothing}$ Comments are ignored.</span>
			</li>

			<li>${O(1).}$</li>

			<li>${O(1).}$</li>
			<li>${O(1).}$</li>
			<li>${O(1).}$</li>
			<li>${O(1).}$</li>
			<li>${O(1).}$</li>
			<li>
				<span class="greyText">${\varnothing}$ Comments are ignored.</span>
			</li>

			<li>${O(1).}$</li>
			<li>${O(1).}$</li>
			<li>${O(2).}$</li>
			<li>${O(2).}$</li>
			<li>${O(1).}$</li>
			<li>${O(2).}$</li>
			<li>${O(1).}$</li>
			<li>
				<span class="greyText">${\varnothing}$ Comments are ignored.</span>
			</li>
			<li>${O(1).}$</li>
			<li>
				<span class="greyText">${\varnothing}$ Comments are ignored.</span>
			</li>
			<li>${O(1 + \texttt{T}(\texttt{C}) + \texttt{T}(\texttt{E})).}$</li>
			<li>${O(1 + \texttt{T}(\texttt{C}) + \texttt{T}(\texttt{E})).}$</li>
			<li>
				${O(c + 1 + \texttt{T}(\texttt{E})),}$ where ${c}$ is the time
				complexity of reaching the else-block. I.e., the time complexity of
				checking all of the preceding <var>if</var> and
				<var>else if</var> conditions.
			</li>
		</ol>
	</div>
	<p>
		We list the operations as exhaustively as we can because in these first
		few sections we will count all of the operations. However, as we
		progress, we will learn that we rarely count every operation. Some
		operations are counted, others ignored. That said, let's consider an
		example. Suppose we had the following algorithm:
	</p>
	<figure>
		<ol class="alg">
			<li>int sum = 0;</li>
			<li>for (int i = 0; i < n; i++):</li>
			<ol>
				<li>sum = sum + i;</li>
			</ol>
			<li>return sum;</li>
		</ol>
	</figure>
	<p>Counting the number of operations:</p>
	<div class="pseudosource">
		<ol class="alg">
			<li>int sum = 0;</li>
			<li>int i = 0;</li>
			<li>i &lt; n;</li>
			<li>sum + i;</li>
			<li>sum = sum;</li>
			<li>i++;</li>
			<li>return sum;</li>
		</ol>
		<ol class="algc">
			<li>${1.}$</li>
			<li>${1.}$</li>
			<li>
				${n + 1.}$ The comparison is performed after each execution of the
				loop's body. Thus, it is true ${n}$ times and false once.
			</li>
			<li>
				${n.}$ The addition operation is performed only if the test
				condition (the preceding comparison operation) returns
				<var>true</var>. Thus, it is performed ${n}$ times.
			</li>
			<li>
				${n.}$ The assignment operation is performed only if the test
				condition returns <var>true</var>. Thus, it is performed ${n}$
				times.
			</li>
			<li>
				${n.}$ The increment is performed only if the test condition
				returns <var>true</var>. Thus, it is performed ${n}$ times.
			</li>
			<li>
				${1.}$ The <var>return</var> operation is performed exactly once.
			</li>
		</ol>
	</div>
	<p>
		Summing the number of executions for each operation, ${\texttt{T}(n),}$
		we get:
	</p>
	<figure>
		$$ \begin{aligned} \texttt{T}(n) &= 1 + 1 + (n+1) + n + n + n \\ &= 1 +
		1 + n + 1 + n + n + n \\ &= 1 + 1 + 1 + n + n + n + n \\ &= 3 + 4n
		\end{aligned} $$
	</figure>
	<p>
		The final expression gives us a function of the algorithm's total run
		time: ${\texttt{T}(n) = 4n + 3.}$ When we conduct complexity analysis,
		we want to express this runtime in terms of Big-O notation. To do so,
		we want to:
	</p>
	<ol>
		<li>Remove the constants.</li>
		<li>Remove the lowest terms.</li>
	</ol>
	<p>
		In this case, the constant is ${5,}$ and the lowest term is ${4.}$
		Dropping the constant and the lowest term, we have some function ${g(n)
		= n,}$ and we express this as ${O(n).}$ We call this
		<span class="term">linear time</span>, and we say that &#8220;The
		algorithm above runs no faster than ${n,}$&#8221; or &#8220;The runtime
		complexity is ${O(n).}$&#8221;
	</p>
	<p>Now, compare the algorithm above with the following:</p>
	<figure>
		<ol class="alg">
			<li>return (n * (n + 1)) / 2</li>
		</ol>
	</figure>
	<p>Counting the number of operations:</p>
	<figure>
		<table class="alg">
			<thead>
				<th>Operation</th>
				<th>Number of Executions</th>
				<th>Comment</th>
			</thead>
			<tbody>
				<tr>
					<td><span class="monoText">n + 1</span></td>
					<td>1</td>
					<td rowspan="4">
						All of these operations perform exactly once.
					</td>
				</tr>
				<tr>
					<td><span class="monoText">n * result(n + 1)</span></td>
					<td>1</td>
				</tr>
				<tr>
					<td><span class="monoText">result(n + 1) / 2</span></td>
					<td>1</td>
				</tr>
				<tr>
					<td><span class="monoText">return result</span></td>
					<td>1</td>
				</tr>
			</tbody>
		</table>
	</figure>
	<p>Summing the number of executions:</p>
	<figure>
		<div>
			$$ \begin{aligned} \texttt{T}(n) &= 1 + 1 + 1 + 1\\[1em] &= 4
			\end{aligned} $$
		</div>
	</figure>
	<p>
		This algorithm has a running time function of ${\texttt{T}(n) = 4.}$
		Whenever the running time function is constant, we say that the
		algorithm runs on
		<span class="term">constant time</span>, and we denote this with
		${O(1).}$ This is a concise way of saying that algorithm's runtime does
		not depend on the input size ${n.}$ We can throw as many ${n}$ as we
		would like at the algorithm, and the runtime will never change. It's
		constant.
	</p>
	<p>
		Compare these two algorithms. Clearly, the first algorithm is slower
		than the second. We have function ${g(n) = n,}$ and ${g(n) = 1.}$ If we
		plot these two functions:
	</p>
	<figure>
		<img
			src="{% static 'images/linear_v_constant.svg' %}"
			alt="Linear versus constant time"
			loading="lazy"
			style="width: 300px"
		/>
		<figcaption>
			where ${n}$ is the size of the input and ${t}$ is time
		</figcaption>
	</figure>
	<p>
		we see that ${g(n)}$ will always grow faster than ${g(1).}$ This
		indicates that ${g(1)}$ will always be faster than ${g(n)}$ &mdash;
		${g(1)}$ never grows to begin with.
	</p>
</section>

<section id="cases">
	<h2>Cases</h2>
	<p>
		How well an algorithm performs depends on its inputs. Consider the
		simple linear search:
	</p>
	<ol class="alg">
		<li>bool LinearSearch(TYPE[] array, TYPE key):</li>
		<ol>
			<li>int n = array.Length();</li>
			<li>for (int i = 0; i &lt; n; i++):</li>
			<ol>
				<li>if (arr[i] == key): return true;</li>
			</ol>
			<li>return false;</li>
		</ol>
	</ol>
	<p>
		If the key is found at <var>arr[0]</var>, the algorithm executes in
		just one iteration. If the key is found at <var>arr[n-1]</var>, the
		algorithm executes in <var>n</var> iterations (with
		<var>n</var> evaluations of the loop's condition). If the key isn't
		found anywhere in the array, the algorithm executes in
		<var>n</var> iterations, with <var>n+1</var> evaluations of the loop's
		condition. Thus, there are three unique scenarios:
	</p>
	<ol>
		<li>Key is found after just one iteration.</li>
		<li>Key is found after ${n}$ iterations.</li>
		<li>Key is not found.</li>
	</ol>
	<p>
		Because of this phenomenon, algorithmic analysis can be performed in
		three different ways:
	</p>
	<ol>
		<li>
			The <b>best-case time</b> of an algorithm is the algorithm's
			<em>smallest possible run-time</em>, over all possible fixed-size
			inputs. The algorithmic analysis for determining an algorithm's
			best-case time is called the <i>best-case analysis</i>. When someone
			gives us an algorithm's best-case time ${t,}$ they're essentially
			telling us that the algorithm has a speed limit ${t}$ &mdash; it
			can't get any faster than ${t.}$
		</li>
		<li>
			The <b>worst-case time</b> of an algorithm is the algorithm's
			<em>largest possible run-time</em>, over all possible fixed-size
			inputs. The algorithmic analysis for determining an algorithm's
			best-case time is called the <i>worst-case analysis</i>. When we're
			given an algorithm's worst-case time ${w,}$ we're being told,
			<q>Thie algorithm can't get any slower than ${w.}$</q>
		</li>
		<li>
			The <b>average-case time</b> of an algorithm is the algorithm's
			<em>average possible run-time</em>, over all possible fixed-size
			inputs. The algorithmic analysis for determining an algorithm's
			best-case time is called the <i>average-case analysis</i>. When we're
			given an algorithm's average-case time ${a}$, we're being told,
			<q>On average, the algorithm has run time of ${a.}$</q>
		</li>
	</ol>
</section>
{% endblock %}
