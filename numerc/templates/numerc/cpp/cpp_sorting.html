{% extends '../layout.html' %} {% load static %} {% block description %}
<meta
	name="description"
	content="Sorting algorithms: Selection sort, insertion sort, bubble sort, merge sort"
/>
{% endblock %} {% block title %}
<title>C++ Sorting Algorithms</title>
{% endblock %} {% block content %}

<h1>Sorting Algorithms</h1>
<section id="sorting_intro">
	<p>Consider the following problem:</p>
	<figure class="math-display">
		<div class="rule">
			<p>
				<span class="topic">Problem.</span> Given an array of ${n}$ elements in
				arbitrary order, return an array of the same numbers sorted from least
				to greatest.
			</p>
			<p>E.g.,</p>
			<figure class="math-display">
				$$ \begin{aligned} x &= [3, 2, 1, 5, 4] \\ f(x) &= [1, 2, 3, 4, 5]
				\end{aligned} $$
			</figure>
		</div>
	</figure>
	<p>
		The problem above is called a <span class="term">sorting problem</span>, and
		it is a prequisite to many other algorithms. For the following analyses, we
		assume there are no duplicates.
	</p>
</section>

<section id="selection_sort">
	<h2>Selection Sort</h2>
	<figure><img src="{% static 'images/comic_insertionSort.png' %}" alt=""></figure>
	<p>
		The simplest sorting algorithm is <span class="term">selection sort</span>.
		<span class="marginnote"
			>The basic idea behind selection sort: Find the smallest
			<span class="underlineText">unsorted element</span>, and add it to the end
			of the sorted list. We repeat this process until we've gone through the
			entire array.</span
		>
		In selection sort, we scan through the entire array. If the element at index
		${i}$ is less than the element at index ${i + 1,}$ we leave the elements in
		place. Otherwise, we swap the elements. If we leave the elements in place,
		we compare ${i}$ with ${i+2.}$ If ${i}$ is less than ${i+2,}$ we leave the
		elements in place. Otherwise, we swap the elements. We continue this process
		until we reach the end of the array. Once we're done comparing ${i,}$ we
		then compare ${i+1,}$ then ${i+2,}$ and so on and so forth. Implementing
		this algorithm:
	</p>
	<pre class="language-cpp"><code>
		#include &lt;iostream&gt;
		using namespace std;
		// function signatures
		void printArray(int arr[], int n);
		void selectionSort(int arr[], int arrLength);
		// main program
		int main() {
			int s = 4;
			int arr[4] = {3, 2, 1, 4};
			printArray(arr, s);
			selectionSort(arr, s); // Call to selectionSort function (see the function)
			printArray(arr, s);
			return 0;
		}
		// Selection Sort function
		void selectionSort(int arr[], int arrLength) {
			for (int i = 0; i < arrLength - 1; i++) {
				int smallest = arr[i];
				int indexOf_smallest = i;
				for (int j = i+1; j < arrLength; j++) {
					if (arr[j] < smallest) {
						smallest = arr[j];
						indexOf_smallest = j;
					}
				}
				// swap
				if (indexOf_smallest != i) {
					int temp = arr[indexOf_smallest];
					arr[indexOf_smallest] = arr[i];
					arr[i] = temp;
				}
			}
		}
		// Print array function
		void printArray(int arr[], int n) {
			cout << "[ ";
			for (int i = 0; i < n; i++) {
				cout << arr[i] << "  ";
			}
			cout << "]" << endl;
		}
	</code></pre>
	<pre class="language-bash"><code>
		[ 3  2  1  4  ]
		[ 1  2  3  4  ]	
	</code></pre>

	<p>Selection sort works fine, but it's not very fast.</p>
</section>

<section id="bubble_sort">
	<h2>Bubble Sort</h2>
	<p>
		Another sorting algorithm we could use is
		<span class="term">bubble sort</span>. In a bubble sort, we compare adjacent
		elements in the array: If the elements are in order, leave them. If they are
		not in order, swap them. The list is in order if a pass is completed without
		making any swaps.
		<span class="marginnote"
			>The name &#8220;bubble sort&#8221; comes from the fact that the largest
			elements appear to &#8220;bubble&#8221; up to the surface.</span
		>
		We can outline this process as such:
	</p>
	<figure class="math-display">
		<div class="rule">
			<ol class="numd">
				<li>Let ${A}$ be an unordered list of length ${n.}$</li>
				<li>Let ${k = A[i].}$</li>
				<li>Let ${j = A[i+1].}$</li>
				<li>
					If ${k > j,}$ swap ${k}$ and ${j.}$ Otherwise, leave the elements as
					is.
				</li>
				<li>
					Once the last element of ${A}$ is reached, the last item is in its
					final position.
				</li>
				<li>
					If a swap is made, continue with the steps above with ${A}$ of length
					${n-1.}$
				</li>
				<li>If a pass is completed without any swaps, the list is sorted.</li>
			</ol>
		</div>
	</figure>
	<p>Here is an implementation:</p>
	<pre class="language-cpp"><code>
		#include &lt;iostream&gt;
		using namespace std;
		
		void printArray(int arr[], int n);
		void bubbleSort(int arr[], int n);
		
		int main() {
			int arr[5] = {3, 1, 5, 4, 2};
			printArray(arr, 5);
			bubbleSort(arr, 5);
			printArray(arr, 5);
		
			return 0;
		}
		
		void bubbleSort(int arr[], int n) {
			for (int i = 0; i < n-1; i++) {
				for (int j = 0; j < n-i-1; j++) {
					if (arr[j] > arr[j+1]) {
						int temp = arr[j];
						arr[j] = arr[j + 1];
						arr[j + 1] = temp;
					}
				}
			}   
		}
		
		void printArray(int arr[], int n) {
			cout << "[ ";
			for (int i = 0; i < n; i++) {
				cout << arr[i] << "  ";
			}
			cout << "]" << endl;
		}
	</code></pre>
	<pre class="language-bash"><code>
		[ 3  1  5  4  2  ]
		[ 1  2  3  4  5  ]
	</code></pre>
</section>

<section id="insertion_sort">
	<h2>Insertion Sort</h2>
	<p>
		We can do better with <span class="term">insertion sort</span>. This
		algorithm works much like how we would sort a hand of playing cards. Suppose
		the cards are: ${\{ 2, 3, 8, Jack, 5 \}}$. We pull one card, and it's an 8.
		Then we pull another card, and it's a 2, so we put it before 8. Then
		another, and it's a Jack, so it goes after 8. Then a 3, so it goes between 2
		and 8. Then a 5, so it goes between 3 and 8. The final result:
	</p>
	<figure class="math-display">
		<div>
			<p>${\lang 2, 3, 5, 8, Jack \rang}$</p>
		</div>
	</figure>
	<p>Here is a rough outline of insertion sort:</p>
	<figure class="math-display">
		<div class="rule">
			<p>Let ${A}$ be an unsorted array of size ${n.}$</p>
			<p>Repeat the following ${n}$ times:</p>
			<ol class="numd">
				<li>
					Let ${k = A[i].}$<sup class="tip"
						><span class="tiptxt"
							>Here we set some variable ${k}$ to be the element at the index
							${i.}$ At the start of each iteration of the
							<span class="monoText">for</span> loop, we assume that the
							smallest element is the element at position ${i.}$ The
							<span class="monoText">while</span> loop will later determine if
							the smallest element remains ${A[i],}$ or if it must be
							swapped.</span
						></sup
					>
				</li>
				<li>
					Let ${j = i - 1}$<sup class="tip"
						><span class="tiptxt"
							>Remember, with insertion sort, we're comparing the current
							element with the previous element. Thus, ${j}$ is the index of the
							previous element.</span
						></sup
					>
				</li>
				<li>
					If (1) ${j \geq 0}$
					<sup class="tip"
						><span class="tiptxt"
							>Since we are decrementing, we want this while loop to stop as
							soon as we go below 0. Note that because ${j = -1}$ on the first
							iteration of the <span class="monoText">for</span> loop, this
							<span class="monoText">while</span> loop is not executed on the
							first iteration.</span
						></sup
					>
					and (2) ${k < A[j]}$<sup class="tip"
						><span class="tiptxt"
							>"If the current element is less than the previous element."</span
						></sup
					>:
				</li>
				<ol>
					<li>
						Let ${A[j + 1] = A[j].}$<sup class="tip"
							><span class="tiptxt"
								>This line executes only if the conditions above are met: (1)
								${j \geq 0,}$ and (2) ${k < A[j].}$ Remember, ${j}$ is the index
								of the previous element (${j = i - 1}$). Thus, ${j + 1}$ returns
								${i.}$ This effectively means, if the current element is less
								than the previous element, then set the position of the current
								element to be the previous element (i.e.,
								&#8220;switch&#8221;).</span
							></sup
						>
					</li>
					<li>
						Decrement ${j}$ by 1.<sup class="tip"
							><span class="tiptxt"
								>This decrements j by 1, so as to terminate the loop.</span
							></sup
						>
					</li>
				</ol>
				<li>
					Else:
					<sup class="tip"
						><span class="tiptxt"
							>I.e., if ${j < 0}$ <span class="italicsText">or</span> ${k}$, the
							current element, is greater than or equal to ${A[j],}$ the
							previous element, do the following.</span
						></sup
					>
				</li>
				<ol>
					<li>
						Set the element after ${j}$ to be the element at ${i.}$
						<sup class="tip"
							><span class="tiptxt"
								>Keep the positions; they're already sorted!</span
							></sup
						>
					</li>
				</ol>
			</ol>
		</div>
	</figure>
	<p>
		The idea behind insertion sort is straightforward. We look at the unsorted
		array ${(A)_1^{n},}$ working from left to right. We then examine each
		element in the unsorted array, comparing it to items on its left. We then
		insert the element in the correct position. This effectively forms two
		portions: The unsorted portion ${(A)_1^{n},}$ and the sorted portion
		${(A)_1^{j-1}.}$
	</p>
	<p>
		The variable ${k}$ is called the <span class="term">key</span>. The idea
		behind insertion sort is that we pull out the key and use it as a value to
		compare against the remaining elements of ${A,}$ the unordered array.
	</p>
	<p>
		Implementing this algorithm in <span class="monoText">C++</span> and
		testing:
	</p>
	<pre class="language-cpp"><code>
		#include &lt;iostream&gt;
		using namespace std;
		
		void printArray(int arr[], int n);
		void insertionSort(int arr[], int n);
		
		int main() {
			int arr[5] = {1, 3, 4, 2, 5};
			printArray(arr, 5);
			insertionSort(arr, 5);
			printArray(arr, 5);
			return 0;
		}
		
		void insertionSort(int arr[], int n) {
			for(int i = 1; i < n; i++) {
				int k = arr[i];
				int j = i;
				while(j > 0 && arr[j-1] > k) {
					arr[j] = arr[j-1];
					j--;
				}
				arr[j] = k;
			}
		}
		
		void printArray(int arr[], int n) {
			cout << "[ ";
			for (int i = 0; i < n; i++) {
				cout << arr[i] << "  ";
			}
			cout << "]" << endl;
		}
	</code></pre>
	<pre class="language-bash"><code>
		[ 1  3  4  2  5  ]
		[ 1  2  3  4  5  ]
	</code></pre>

	<p>
		Let's analyze this algorithm. What is the
		<span class="term">running time</span>? It depends on a variety of factors.
		First, of the ${A}$ is already sorted, then insertion sort has very little
		work to do. This is because every time we go through the loop, the
		<span class="monoText">while</span> loop never executes &mdash; everything
		stays in place. The worst case scenario is if ${A}$ is reverse-sorted.
	</p>

	<p>
		Second, it depends on the input size. If ${A}$ contains numerous elements,
		then it will take a lot longer for insertion sort to work. Accordingly, we
		<span class="italicsText">parameterize</span> the input size. In other
		words, the running time of insertion sort is a function of the input size.
	</p>

	<p>
		Knowing these factors, we almost always want to know the
		<span class="term">upper bound</span> of the running time. Upper bound is
		the maximum time the algorithm would take. This serves as a guarantee to the
		user that the algorithm won't take any longer than ${x}$ amount of time.
	</p>

	<p>
		<span class="topic">Worst-Case Analysis.</span> There are 3 different kinds
		of analyses we can do to assess the running time of an algorithm. One kind
		of analysis is the <span class="term">worst-case analysis</span>. In the
		worst-case analysis, we define ${T(n)}$ to be the
		<span class="italicsText">maximum time</span> on any input of size ${n.}$
		Generally, this means we assume the algorithm takes some gigantic input.
		What about the other factors? With the other factors, we assume the
		worst-possible case for those factors. For example, with insertion sort, the
		worst-case analysis requires us to assume some gigantic array with
		reverse-sorted elements.
	</p>

	<p>
		The worst-case analysis is how we make a guarantee. It allows us to
		determine that an algorithm will always do something, even in the worst
		circumstances. We are effectively stripping the algorithm of all the
		possibilities where it sometimes works well.
	</p>

	<p>
		<span class="topic">Average-Case Analysis.</span> Another kind of analysis
		we can do is the <span class="term">average-case analysis</span>. Here,
		${T(n)}$ is the <span class="italicsText">expected time</span> on any input
		of size ${n.}$ What do we mean by expected time? It's the time taken by
		every input multiplied by the probability that it will be that input.
		Question: How do we know what the probability of an input occurring is? That
		fact is, we do not know. We must assume the statistical distribution of
		inputs. There are several assumptions we can make. For example, assuming
		<span class="term">uniform distribution</span> would mean all inputs are
		equally likely.
	</p>

	<p>
		<span class="topic">Best-Case Analysis.</span> The final kind of analysis is
		the <span class="term">best-case analysis</span>. This analysis essentially
		tells us the smallest possible time it would take the algorithm to halt at
		the correct input. This analysis has its place, but it is generally left to
		more nuanced and specific questions (e.g., the probability of the best-case
		scenario occurring). For most algorithms, the analysis tells us very little
		about an algorithm; conducting the best-case analysis allows us to
		cherry-pick inputs.
	</p>

	<p>
		In measuring the worst-case analysis runtime, we might wonder about the
		other factors. What about the machine? Wouldn't the algorithm run faster on
		a supercomputer compared to a netbook? The answer is unreservedly yes. And
		as one might suspect, determining runtime would be incredibly tedious if we
		had to account for specific machines. We simplify this analysis by
		completely setting aside that consideration throgh
		<span class="term">asymptotic analysis</span>.
	</p>

	<p>
		In an asymptotic analysis, we apply two axioms: (1) Machine dependent
		constants have no effect on the runtime; (2) The runtime is determined by
		examining the <span class="italicsText">growth</span> of the runtime. Before
		we see some examples, we should familiarize ourselves with the analysis's
		notation.
	</p>

	<p>
		The first notation system is <span class="term">${\Theta}$-notation</span>.
		In ${\Theta}$-notation, we drop the leading terms and ignore leading
		constants. For example, if we have the expression ${3n^3+90n^2+5n+6046,}$ we
		have several terms &mdash; ${3n^3,}$ ${90n^2,}$ ${5n,}$ and ${6046.}$
		Mathematically, ${n^3}$ is bigger than ${n^2.}$ More broadly, ${3n^3}$ is
		bigger than all of the other terms (the leading terms). This means we drop
		all of those terms, leaving us with ${3n^3.}$ The term ${3n^3}$ has a
		leading constant, ${3.}$ So, we drop that constant as well, leaving us with
		${n^3.}$ We express this final result as: ${\Theta(n^3).}$
	</p>

	<p>
		Suppose one algorithm is of ${\Theta(n^2)}$ and another algorithm is of
		${\Theta(n^3).}$ Comparing thise algorithms, ${\lim\limits_{n \to
		\infty}\Theta(n^3)}$ will always be larger than ${\lim\limits_{n \to
		\infty}\Theta(n^2).}$ This means that the algorithm of ${\Theta(n^2)}$ will
		always be faster than an algorithm of ${\Theta(n^3).}$ It is irrelevant what
		the leading terms or the leading constants are. It is also irrelevant
		whether you run the algorithm on a supercomputer or on a very slow computer:
		${n^3}$ will always be bigger than ${n^2.}$
	</p>

	<figure>
		<img
			src="{% static 'images/runtime1.svg' %}"
			alt="runtime"
			loading="lazy"
			class="sixty-p"
		/>
	</figure>

	<p>
		If we examine the graph above, ther will always be a point ${n_0,}$
		${\Theta(n^2)}$ will be faster than ${\Theta(n^3).}$ In the real world,
		however, that ${n_0}$ may be such a large input that a computer doesn't have
		enough memory to run the algorithm. For this reason, there are some
		algorithms for which ${\Theta (n^3)}$ and ${\Theta (n^2)}$ are no different.
		This tension between pure mathematics and applied mathematics underlies much
		of algorithm analysis. However, the pure mathematics analysis is always
		performed first &mdash; it provides the backdrop for all further analysis.
	</p>

	<p>
		Returning to insertion sort, the worst case scenario is when every element
		in the array is reverse-sorted. So, to conduct the worst-case analysis, we
		assume that there is some array with ${n}$ elements, all of which are
		reverse-sorted. Now we examine the code and assume that each operation takes
		a constant amount of time. It doesn't matter what that amount of time is
		because we are conducting an asymptotic analysis. One way to think about
		this is to count the amount of times we access memory. The first time we
		access memory is 1, and we increment by 1 each time we access the memory.
		Returning to the code, we have nested loops.
	</p>

	<p>
		First, we have a <span class="monoText">for</span> loop, running from 2 to
		${n.}$ Writing this mathematically:
	</p>

	<figure class="math-display">
		<div>
			<p>$${\sum_{j=2}^{n}}$$</p>
		</div>
	</figure>

	<p>
		Now, we need to look at how many operations are occurring inside each
		iteration. We're looking at the worst case scenario, so every element is
		reversed. This means that the body of the loop occurs ${j}$ times. Thus, we
		conclude:
	</p>

	<figure class="math-display">
		<div>
			<p>$${\sum_{j=2}^{n} \Theta(j)}$$</p>
		</div>
	</figure>

	<p>This is a simple arithmetic series, so we can simplify it further:</p>

	<figure class="math-display">
		<div>
			<p>
				$${\sum_{j=2}^{n} \Theta(j) = \dfrac{(\Theta(n))((\Theta(n)) + 1)}{2}} =
				\Theta(n^2)$$
			</p>
		</div>
	</figure>

	<p>
		Note that we have to be very careful about ${\Theta}$-notation because it is
		a <span class="italicsText">weak notation</span>. This means that the
		notation is more descriptive than it is manipulative. In other notation
		systems like symbolic logic notation or Leibniz notation, we can manipulate
		symbols algebraically. We cannot do the same with ${\Theta}$-notation
		&mdash; we have to think of what exactly the notation is describing. We
		cannot simply algebraically manipulate.
	</p>

	<p>
		That said, we now have the worst-case runtime for insertion sort &mdash;
		${\Theta(n^2).}$ Is it fast? For small ${n,}$ is somewhat fast. But it is
		not fast for large ${n.}$ A much faster algorithm is
		<span class="term">merge sort</span>.
	</p>
</section>

<section id="merge_sort">
	<h2>Merge Sort</h2>
	<p>
		In the <span class="term">merge sort</span> algorithm, we perform three
		steps. Suppose we have an unsorted array ${A}$ of ${n}$ elements. The merge
		sort algorithm operates recursively: (1) If ${n = 1,}$ then the array is
		sorted. (2) Otherwise, merge sort ${(A)_1^{\lceil n/2 \rceil}}$ and the
		array ${(A)_{\lceil(n/2) + 1\rceil}^n.}$ In other words, we are sorting two
		halves of the array ${A.}$ (3) We take the two halves and merge them.
	</p>

	<p>
		The base case, ${n = 1,}$ is just one operation. It takes constant time to
		perform: ${\Theta(1).}$ What about the recusive case? How do we describe
		those operations? Well, the recursive case handles the the two halves of the
		aray, so the runtime is ${2T(n/2).}$ The merge subroutine consists of ${n}$
		steps, so that takes ${\Theta(n)}$ time. We describe merge sort's runtime as
		the following:
	</p>

	<figure class="math-display">
		$$ T(n) = \begin{cases} \Theta(1) &n=1 \\ 2T(n/2) + \Theta(n) &n > 1
		\end{cases} $$
	</figure>
</section>
{% endblock %}
