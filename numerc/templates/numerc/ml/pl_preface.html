{% extends '../layout.html' %} {% load static %} {% block content %}
<section id="ML_intro">
	<h2>Programming Languages</h2>
	<p>
		This volume explores the design and implementation of programming languages.
		In particular, we will analyze the different approaches to creating
		programming languages, as well as the recurring tradeoffs language designers
		make. Roughly, we can think of this volume as an introduction to
		<span class="term">programming language theory</span> (PLT).
	</p>
	<p>
		But why study programming languages? This is a reasonable question, given
		that most programming language courses rely heavily on theory. And as with
		most things reliant on theory, we often want (but don't need) to offer some
		justification.
	</p>
	<p>
		First, studying programming language theory makes learning and understanding
		programming languages much, much easier. While languages wildly vary in
		implementations and features, many languages can be grouped into families
		with shared characteristics. There are a finite number of such families, and
		once we are familiar with them, the variations boil down to syntax and
		different terminology for similar concepts. Recognizing these patterns and
		shared traits hones both a broad and more nuanced understanding of languages
		on first view. How? By building a larger vocabulary for describing the
		language. Often, the hardest part about learning computer science (really,
		any field for that matter) is being able to state our thoughts in words.
		Programming language theory provides the words we need but have yet to
		discover.
	</p>
	<p>
		Second, languages are rapidly developing, but many of the foundational ideas
		have largely remained unchanged. Computers continue to follow the Von
		Neumann architecture, the major programming paradigms are still around, and
		many core problems continue to beset modern computation. Accordingly,
		computer science is a field where we must be in a constant mode of consuming
		new information and retaining it. Studying programming language theory
		allows us to identify trends in computer science development, and perhaps
		even more valuably, the ability to
		<span class="italicsText">effectively skim</span> new information. We don't
		always have the time to thoroughly read the obscure article on applying
		lambda calculus in logic programming, but having a strong foundation in
		programming language theory can help in grokking the main ideas quickly.
	</p>
	<p>
		Third, most programmers engage in some programming language theory after
		learning their second or third languages. A hallmark exercise of programming
		language theory is finding similarities between languages. Learning
		<span class="monoText">C++</span> after ML, we find that
		<span class="monoText">func</span> works like
		<span class="monoText">fun</span>. Going from C to Objective-C to Swift, we
		see how so many features in Swift trace all the way back to C. Learning
		Scheme, or another language without loop constructs, recursion is easier to
		understand in other languages. Part of what makes learning our fifth or
		fifteenth language so easy is drawing upon all the concepts we've learned
		previously: "Oh, this feature ${x}$ in language ${B}$ is just a ${y}$ in
		language ${A.}$" Programming language theory accelerates that process by
		gathering all of those concepts, organizing them, and subjecting the details
		to scrutiny and overview. More importantly, it is the same theory that
		provides the brakes &mdash; sometimes, our analogies are erroneous, and a
		more nuanced comparison is necessary.
	</p>
	<p>
		Last but not least, learning programming language theory is fun. Seeing the
		history behind languages and thinking about the decisions language designers
		made is interesting and often humorous. Computer science tends to gather
		creative and curious people, and wherever those two traits go, there are
		always interesting stories and ideas. The materials will always provide some
		historical background to the material to place things in perspective. Just
		as stepping through the code can help debug a logic error, stepping through
		a language's developmental history can help in understanding why the
		language, or family of languages, is the way it is today. That is a very
		deep level of understanding, and is invaluable to everyday programming.
	</p>
</section>

<section id="evaluating_languages">
	<h4>Evaluating Languages</h4>
	<p>
		Programming languages evaluated through a variety of criteria, but there are
		several factors we consider more often than others: (1)
		<span class="term">readability</span>, (2)
		<span class="term">writability</span>, and (3)
		<span class="term">reliability</span>. By readability, we mean the ease
		reading and understanding programs in that language. Writiability is a
		measure of how easily the language can be used to create programs for
		solving problems in a given domain. Finally, reliability measures how well a
		program performs under any condition.
	</p>
	<p>
		Older languages like Pascal, Algol, and Fortran usually score low on these
		three factors compared to newer languages. This is because languages written
		before 1970 were designed from the perspective of the computer, rather than
		the user.
		<span class="marginnote"
			>On the other hand, this is partly why many older languages are so
			efficient.</span
		>
	</p>
	<p>
		Going into the 1970s, however, researchers began to notice how important
		maintainability was &mdash; programs got bigger, there were more users as
		computers entered the public space, and hardware improved at alarming speed.
		The controlling variables for maintainability? Readability, writability, and
		reliability.
	</p>
	<p>
		Whenever we evaluate the three factors, we always want to consider the
		language's
		<span class="term">problem domain</span>. A language's problem domain is the
		set of problems the language is intended to address. For example, we
		probably would not want to evaluate JavaScript for particle physics
		simulations, a problem domain usually dominated by C,
		<span class="monoText">C++</span>, or Fortran. Each of the factors are
		impacted by several smaller factors.
	</p>
	<p>
		The first such factor is <span class="term">simplicity</span>. The more
		<span class="term">constructs</span> a language has, the more difficult it
		is to learn. By constructs, we mean the ways in which a language natively
		executes a task. For example, a language with seven or eight different
		looping constructs is more difficult to learn than language with just two,
		or arguably, none at all.
		<span class="marginnote"
			>In JavaScript, there are several loops:
			<span class="monoText">for</span>, <span class="monoText">for-in</span>,
			<span class="monoText">for-of</span>, <span class="monoText">while</span>,
			and <span class="monoText">do-while.</span> Compare that with Scheme,
			where there are no loop constructs (iteration is done through a recursive
			implementation). Which is better? It depends on the problem and the
			programmer. Recursion can have a steep learning curve for some, but
			iteration is fairly straightforward.</span
		>
		The phenomenon of having more than one construct for accomplishing a
		particular computation is called
		<span class="term">feature multiplicity</span>. For example, in Java, we can
		increment integers in four different ways:
	</p>
	<pre class="language-java"><code>
		count = count + 1;
		count += 1;
		count++;
		++count;
	</code></pre>
	<p>
		However, a language that is too simple can have its drawbacks. Assembly is a
		very simple language, but it is hardly readable. Brainfuck is overly simple
		language. The language consists of just 8 commands (comprising a data
		pointer and an instruction pointer), all of which are written with just 8
		characters &mdash; <span class="monoText">&gt;</span>,
		<span class="monoText">&lt;</span>, <span class="monoText">+</span>,
		<span class="monoText">-</span>, <span class="monoText">.</span>,
		<span class="monoText">,</span>, <span class="monoText">[</span>,
		<span class="monoText">]</span>. True to its name, Brainfuck programs are
		anything but readable.
		<span class="marginnote"
			>Brainfuck is, however, a
			<span class="italicsText">Turing-complete</span> language, a concept we
			will revisit in later sections.</span
		>
	</p>
	<p>
		Just as feature multiplicity weighs against simplicity, so does
		<span class="term">operator overloading</span> &mdash; a language feature
		where operator symbols have more than one meaning. For example, the symbol
		<span class="monoText">+</span> is one of the most commonly overloaded
		operators. Many languages allow using the symbol to add
		<span class="monoText">float</span> type values with
		<span class="monoText">int</span> type values. In
		<span class="monoText">C++</span> and other languages the overloading goes a
		step further: it can represent arithmetic addition, but also concatenation
		of strings:
	</p>
	<pre class="language-cpp"><code>
		int a = 1 + 2; 
		string s1 = "Hello" + " world!";
	</code></pre>
	<p>
		Operator overloading is not a bad thing. In fact, it is something we
		implicitly expect when writing code. The <span class="monoText">+</span>,
		<span class="monoText">-</span>, <span class="monoText">*</span>, and
		<span class="monoText">/</span> operators should work on all real numbers,
		regardless of whether they are <span class="monoText">ints</span> or
		<span class="monoText">floats</span>. Where operator overloading goes wrong,
		however, is when the language redefines the operators so they do not work
		intuitively for the human reader. Needless to say, implementing good
		operator overloading is tricky because the key element, intuition, depends
		on the language's target audience. Seasoned programmers generally have no
		problem understanding the <span class="monoText">+</span> operator as
		applied to strings, but it may not be intuitive to the novice. Similarly,
		using the <span class="monoText">&lt;</span> and
		<span class="monoText">&gt;</span> operators on strings is not always
		immediately apparent for a purely mathematical audience, but may be for
		programmers familiar with the C language family.
	</p>
	<p>
		<span class="term">Orthogonality</span> is another factor that affects
		simplicity. Orthogonality is a language's ability to combine a small number
		of primitive constructs to create and control large data structures. For
		example, Lisp has high orthogonality &mdash; the language has few primitive
		constructs, but those constructs can be used to create large and complex
		data structures. In contrast, JavaScript is a language with low
		orthogonality, by virtue of the fact that it has numerous primitive
		constructs.
	</p>
	<p>
		In general, the more orthogonality a language has, the less concise the
		language is. For exmple, ALGOL68 is arguably the most orthogonal language,
		but its programs are unbelievably complex because of how many statements are
		necessary. On the other hand, high-orthogonality languages tend to have
		fewer exceptions to its syntax, given how there are so few primitive
		constructs. In contrast, the less orthogonality a language has, the more
		concise the language is, but with the added cost of the language needing
		more exceptions. Many object-oriented languages like Java, C, and
		<span class="monoText">C++</span>, for example, have low orthogonality, due
		to the sheer number of exceptions.
		<span class="marginnote"
			>Functional languages like Lisp, Haskell, and Erlang have traditionally
			been very attractive to academia because they achieve just the right
			amount of orthogonality &mdash; everything boils down to a function call.
			Efficiency, however, has been the primary barrier to their popularity in
			industry.</span
		>
	</p>
	<p>
		<span class="topic">Base Types.</span> The number of meaningful base types
		in a language impacts readability. For example, in Forth, there are no base
		types &mdash; only <span class="italicsText">cells</span>, which is either
		an untyped byte or set of bytes. This provides programmers immense
		flexibility and freedom, but it comes at the cost of readability. What is
		the particular cell representing? Textual data? Boolean? Numeric? As another
		cost, languages with no base types tend to be inefficient as well (not
		necessarily because of the language, but because everything is left up to
		the programmer). Smalltalk is a good example, where there are no &#8220;base
		types&#8221; exposed to the user. Instead, it is up to the user to define
		those types, as well as what operator symbols mean for those types. This
		shifts the burden of optimization to the user, and most programmers aren't
		very good at writing such low-level details. The end result is almost always
		a program that is very, very slow.
	</p>
	<p>
		As one might expect, too many base types risks creating unnecessary
		restrictions on the user as well as increasing program complexity. With more
		base types, the language must define what those base types mean; more
		syntax. And with more syntax, the more symbols and words are taken away from
		the user to use.
		<span class="marginnote"
			>Unsurprisingly, symbols and words are a high commodity in programming. To
			quote Phil Karlton, &#8220;There are only two hard things in computer
			science: cache invalidation and naming things.&#8221;</span
		>
		One way to mitigate this is by allowing overrides, but in doing so, we
		increase complexity even further.
	</p>
	<p>
		With base types, the language designer must also make decisions on
		<span class="term">type checking</span> &mdash; the set of all rules
		determining when and where type errors occur. Type checking is where
		languages are most commonly distinguished &mdash; statically-typed languages
		vs. dynamically-typed languages; strongly-typed languages vs. weakly-typed
		languages. These decisions have a direct impact on writability and
		reliability.
	</p>
	<p>
		<span class="term">Statically-typed languages</span> (usually
		<span class="italicsText">compiled languages</span>) have the benefit of
		catching type-errors during compilation rather than runtime. Running
		programs is costly, and to optimize resource use, we want to ensure that the
		program runs perfectly at run-time. Compilation ensures we never get to
		runtime until we fix minor syntax errors. Better yet, compilation ensures
		that the program runs according to the user's intent. A language's failure
		to type check could result in a program running, but with mysterious output.
	</p>
	<p>
		On the other hand, statically-typed languages are often felt to be
		&#8220;heavy&#8221; and restrictive, at least in comparison to
		<span class="term">dynamically-typed languages</span>. In a
		dynamically-typed language, type checking is done at runtime. With
		interpreted languages, the program simply runs &mdash; there is no
		intermediate stage between writing source code and execution. If there is a
		type-error, the source code doesn't run or outputs unexpected results. The
		problem, however, is that not all errors are type errors. There are also
		syntax errors and logic errors. The cost of freedom in dynamically-typed
		languages is repeatedly using runtime over and over.
	</p>
	<p>
		Languages with type-checking rules are further divided according to the
		level of <span class="term">type safety</span> they provide. Type safety is
		the extent to which a programming language prevents type erros. Some
		languages are <span class="italicsText">strongly-typed</span>, where strict
		rules are implemented to prevent type errors. For example, many compiled
		languages like C, <span class="monoText">C++</span>, Java, and Swift are
		strongly-typed languages, in that they require the user to explicitly state
		the types for variables and functions. Other languages like Python and
		JavaScript, however, are weakly-typed languages, in that no such requirement
		is imposed; the user simply writes variables and functions as she pleases.
	</p>
	<p>
		Again, tradeoffs abound. With more type safety, programs are less likely to
		produce bugs, but at the cost of the user's freedom. With less type safety,
		the more freedom the user has, but at the cost of an increased risk for
		bugs.
		<span class="marginnote"
			>Type safety decisions share similarities to an oft-repeated question in
			American constitutional law: Which do we value more: freedom or
			safety?</span
		>
	</p>
	<p>
		This discussion brings us to yet another factor that affects readability
		&mdash; <span class="term">syntax design</span>.
		<span class="term">Syntax</span> is the set of all rules governing the
		semantics of a language. <span class="italicsText">Syntax design</span> is
		the set of all decisions a language makes to implementing its syntax.
	</p>
	<p>
		Syntax design consists of two subsets: the language's set of
		<span class="term">reserved symbols</span> (also called
		<span class="italicsText">keywords</span>, or
		<span class="italicsText">reserved words</span> &mdash; the set of all
		symbols taken away from the user, for exclusive use by the language) and its
		<span class="term">semantics</span> (the set of all rules defining what is
		meaningful in the language; i.e., what the reserved symbols, and
		combinations of those reserved symbols, mean).
	</p>
	<p>
		Deciding what symbols are reserved can have enormous impact on readability.
		For example, in C, Java, <span class="monoText">C++</span>, JavaScript, and
		many other languages, braces are used to denote scope. In Lisp and
		WebAssembly, its parentheses. In languages like Fortran 95 and Ada, the
		words <span class="monoText">begin</span> and
		<span class="monoText">end</span> is used instead. And in languages like
		Python, no symbols are used &mdash; indentation is king. All of these
		decisions have tradeoffs.
		<span class="italicsText">Curly-bracket languages</span> like JavaScript and
		<span class="italicsText">Parenthesized languages</span> like Scheme are
		less readable because keeping track of brackets is difficult. In
		keyword-grouped languages like Pascal, Algol, Lua, and Basic there is
		greater readability, but less simplicity. And then we have
		<span class="italicsText">off-side rule languages</span> like Python, Curry,
		Elixir, and Haskell, which use indentation to denote grouping. These
		languages are highly readable, but now we have the strange situation where
		invisible things are now significant &mdash; the tab with an extra
		whitespace, the increased risk of code breaking in a merge, and copying and
		pasting code snippets.
		<span class="marginnote"
			>In practice, using whitespace as a scope delimiter is generally not much
			of an issue. IDEs have become much more sophisticated at auto-indentation
			and detecting indentation errors. Perhaps the off-side rule is the
			future.</span
		>
	</p>
	<p>
		With every decision to reserve a symbol, the language designer must decide
		what those symbols mean &mdash; i.e., the language's semantics. Generally,
		we want every symbol to have one, and only one, meaning. The problem occurs
		when a language has too many semantics and not enough syntax. When this
		happens, you have a language where a symbol maps to multiple meanings. To
		differentiate between these meanings, the user is forced to understand
		context; something humans are not always very good at. This ties closely to
		operator overloading, but relates more closely to reserved words.
	</p>
	<p>
		For example, the word <span class="monoText">static</span> in C has a
		meaning dependent on context. A static variable inside a function keeps its
		value between calls to that function. A static global variable is visible
		only inside the file its declared in.
		<span class="monoText">static</span> can also be used inside an array type
		declaration as an argument to a function (e.g., specifying that arguments
		passed to a function must be an array of type
		<span class="monoText">char</span> with at least 10 elements).
	</p>
	<pre class="language-c"><code>
		#include &lt;stdio.h&gt;

		// static on a global variable
		static int a = 1;

		// static on a function variable
		int foo() {
			int a = 2;
			static int b = 1; 
			return a + b;
		}

		// static on a  parameter
		void bar(char arg[static 10]) { 
			printf("valid array");
		}
	</code></pre>
	<p>
		The second problem with semantics is when a symbol doesn't intuitively map
		to its semantics. In PHP, for example, passing
		<span class="monoText">j</span> into the date function returns the day of
		the month without leading zeros. It's not intuitive why
		<span class="monoText">j</span> is used, unless the user is familiar with
		UNIX programming (where <span class="monoText">%j</span> is used).
	</p>
	<p>
		As we explore the following materials, we will revist the factors above
		repeatedly. Thinking about these factors provides a rough estimate of a
		language's <span class="term">cost</span> &mdash; what we are giving up in
		using a particular language or language construct to solve a particular
		problem. In calculating this estimate, the ability to learn new languages is
		a side-effect of the process. The end result, however, is a much more
		valuable asset: the ability to decide between competing languages, or
		constructs within a language, for solving a given problem. This is a
		critical, initial decision every programmer must make before writing a
		program.
	</p>
</section>

{% endblock %}
