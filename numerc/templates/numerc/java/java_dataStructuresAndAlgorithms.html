{% extends '../layout.html' %} {% load static %} {% block content %}
<section id="algorithms">
	<h3>Algorithms in Java</h3>
	<p>
		An algorithm is a sequence of instructions towards completing a particular
		objective. It is an answer to a
		<span class="italicsText">how-to</span> question. This is contrast to a
		mathematical proof, which is an answer to a
		<span class="italicsText">what-is</span> question. However, like
		mathematcial proofs, some algorithms are
		<span class="italicsText">elegant</span> or
		<span class="italicsText">efficient</span>. We use the word
		<span class="italicsText">or</span> because an elegant algorithm need not be
		efficient, nor does an efficient one need be elegant.
	</p>
	<p>
		Here, our primary concern is with <span class="term">efficiency</span>. Some
		algorithms are far more efficient than others. How do we measure efficiency?
		Usually, by how many <span class="italicsText">steps</span> the algorithm
		takes (although, time is a perfectly fine measurement, all things equal). In
		these next few sections, we will assume the unit of measurement is by step.
	</p>
	<p>
		When we analyze an algorithm, we do so in the context of a
		<span class="term">general case</span>, rather than for a specific set of
		inputs. Because we assess efficiency in general, efficiency can be thought
		of in three terms: (1) The best case; (2) the on-average case; and (3) the
		worst case. These cases correspond to the different answers to this
		question: How does the algorithm respond to its inputs? This question
		encapsulates several other questions into a single statement: What happens
		when the inputs are very large? What happens when the inputs are very small?
		What happens for a typical input?
	</p>
	<p>
		<span class="topic">Big-O Notation.</span> The way we measure efficiency in
		computer science is with <span class="term">Big-O Notation</span> &mdash; a
		mathematical notation describing a function's limiting behavior when its
		argument tends towards either (a) zero, (b) some non-zero value, or (b)
		towards infinity. We refer to the efficiency measurement as
		<span class="term">runtime</span>.
	</p>
	<p>Runtimes generally fall into a few categories:</p>
	<ol>
		<li>${O(1)}$ &#8220;Big-O of 1.&#8221;</li>
		<li>${O(\log n)}$ &#8220;Big-O of ${\log n.}$&#8221;</li>
		<li>${O(n)}$ &#8220;Big-O of ${n.}$&#8221;</li>
		<li>${O(n \log n)}$ &#8220;Big-O of ${n \log n.}$&#8221;</li>
		<li>${O(n^2)}$ &#8220;Big-O of ${n^2.}$&#8221;</li>
		<li>${O(2^n)}$ &#8220;Big-O of ${2^n.}$&#8221;</li>
		<li>${O(n!)}$ &#8220;Big-O of ${n}$ factorial.&#8221;</li>
	</ol>
	<p>The graphs for these runtimes:</p>
	<figure>
		<img
			src="images/big_o_comparison.svg"
			alt="compare big-o"
			loading="lazy"
			class="seventy-p"
		/>
	</figure>
	<p>
		Why are these runtimes so important? Because a slow algorithm makes a hard
		problem harder. From the graph above, we can see that there is little to no
		difference when ${x=1;}$ i.e., when the inputs are very, very small.
		However, as we pass larger and larger inputs, the differences are stark. In
		particular, a Big-O of ${n!}$ indicates the algorithm takes a shocking
		amount of computational steps to reach its output.
	</p>
</section>

<section id="data_structures">
	<h3>Data Structures</h3>
	<p>
		A <span class="term">data structure</span> is a collection of data, the
		relationships between such data, and the functions or operations that can be
		applied to such data. Particulary in Java, data structures are created with
		<span class="italicsText">primitive types</span> and
		<span class="italicsText">objects</span>, so as to store and organize data.
		The simplest data structure we have seen is the array. We will now see a
		variety of other data structures.
	</p>
	<p>
		<span class="topic">Lists.</span> After arrays, the next simplest data
		structure is a <span class="term">list</span>.
	</p>
</section>
{% endblock %}
