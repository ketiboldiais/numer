{% extends '../layout.html' %} {% block description %}
<meta name="description" content="Notes on assembly" />
{% endblock %} {% block title %}
<title>Assembly</title>
{% endblock %} {% load static %} {% block content %}
<h1>Assembly</h1>
<section id="intro">
	<p>
		In this module, we examine some basic ideas in assembly language. This
		will provide much of the foundations for later discussions. To do so,
		we will use the <b>6502 Assembly Language</b>, as employed on the Atari
		2600 (<q>the 2600</q>) platform. Such a simple platform is used for
		several reasons: (1) It allows us to avoid having to address the
		enormous stretches of complexity on modern devices, and (2) it captures
		most, if not all, of the most important ideas in assembly and computer
		architecture concisely.
	</p>
	<p>The Atari 2600's hardware at a glance:</p>
	<ul>
		<li>
			The 2600 uses a 6507 processor, running at ${1.19~\text{MHz}}$
			(${1.19 \times 10^6}$ instructions per second). Atari originally
			selected the 6502 processor, but in production, the cheaper version,
			6507, was used. Both architectures are similar, but we'll note a few
			differences as we proceed. Of note, the 6502 powered many computers:
			the Apple II, various Commodore machines, the Tamagotchi, NES, and
			Bender from <i>Futurama</i>.
		</li>
		<li>
			For audio and video, the 2600 uses a TIA (Television Interface
			Adapter) chip.
		</li>
		<li>
			In terms of RAM (<q>Read-Access Memory</q>), the 2600 employs a 6532
			RIOT (<q>RAM Input Output Timer</q>) Chip, capable of storing &mdash;
			wait for it &mdash; 128 bytes. Yes, that's all we get.
		</li>
		<li>
			In terms of ROM (<q>Read-Only Memory,</q> i.e., the game catridge),
			we have ${4~\text{kB}}$ to work with. The ROM is where we will read
			our instructins from for the 2600's processor.
		</li>
		<li>
			For input, we're working with two controller ports, connecting some
			non-keyboard peripheral &mdash; e.g., a joystick.
		</li>
		<li>
			For output, the 2600 uses a <b>cathode-ray tube (CRT) television</b>.
			Note that <i>CRT TVs</i> are different from <i>CRT Monitors</i>.
			We're operating under the assumption of working with a CRT TV, not a
			CRT monitor.
		</li>
	</ul>
	<p>
		We should now see why the Atari 2600 is an ideal medium for exploring
		assembly programming. We're working with extremely tight constraints:
		The processor can only do so much; audio and video have to be manually
		taken care of; the set of possible user inputs is small; and memory is
		unquestionably scarce.
	</p>
</section>

<section id="boolean_logic">
	<h2>Boolean Logic</h2>
	<p>
		In the world of computing, there are only two values: <var>0</var> and
		<var>1.</var> With just <var>0</var> and <var>1</var>, we can perform a
		wide variety of operations.
	</p>
	<p>
		<span class="topic">AND.</span> The logical <var>AND</var> is modeled
		as such:
	</p>
	<figure>
		$$ \begin{array}{c:c:c:c} & \texttt{x} & \texttt{y} & \texttt{AND} \\
		\hline & 0 & 0 & 0 \\ & 0 & 1 & 0 \\ & 1 & 0 & 0 \\ & 1 & 1 & 1
		\end{array} $$
	</figure>
	<p>
		Formally, the logical <var>AND</var> is represented with ${\land.}$
	</p>
	<p>
		<span class="topic">OR.</span> The logical <var>OR</var> is modeled as.
	</p>
	<figure>
		$$ \begin{array}{c:c:c:c} & \texttt{x} & \texttt{y} & \texttt{OR} \\
		\hline & 0 & 0 & 0 \\ & 0 & 1 & 1 \\ & 1 & 0 & 1 \\ & 1 & 1 & 1
		\end{array} $$
	</figure>
	<p>
		In formal logic, the logical <var>OR</var> is represented with the
		symbol ${\lor.}$
	</p>
	<p>
		<span class="topic">NOT.</span> The logical <var>NOT</var> is a
		<i>unary operator</i>. In formal logic, we represent the logical
		<var>NOT</var> with the symbol ${\neg.}$
	</p>
	<figure>
		$$ \begin{array}{c:c:c} & \texttt{x} & \texttt{NOT} \\ \hline & 0 & 1
		\\ & 1 & 0 \end{array} $$
	</figure>
	<p>
		With just these three operations &mdash; <var>AND</var>, <var>OR</var>,
		and <var>NOT</var> &mdash; we can create much more elaborate and
		complex operations.
	</p>

	<section id="boolean_identities">
		<h3>Logical Equivalences</h3>
		<p>
			Like the algebraic identities, Boolean logic also has laws like
			commutativity, associativity, and so on. These laws are called
			<b>logical equivalences</b> because their proofs are established by
			the fact that ${a \equiv b}$ if and only if ${a}$ is logically
			equivalent to ${b.}$
		</p>
		<h4>Identity Laws</h4>
		<p>The identity laws provide that:</p>
		<figure>
			$$ \begin{aligned} x \land 1 \equiv x \\ x \lor 0 \equiv x
			\end{aligned} $$
		</figure>
		<p>
			In other words, the conjunction of some variable signal ${x}$ and a
			signal that's always ${1}$ will always return the variable signal
			${x.}$ Similarly, the disjunction of some variable signal ${x}$ and a
			signal that's always ${0}$ will always return the variable signal
			${x.}$ The proof:
		</p>
		<figure>
			$$ \begin{array}{c:c:c:c:c:c} & x & y & z & x \land y & x \lor z \\
			\hline & 1 & 1 & 0 & 1 & 1 \\ & 0 & 1 & 0 & 0 & 0 \end{array} $$
		</figure>
		<h4>Domination Laws</h4>
		<p>The domination laws state:</p>
		<figure>
			$$ \begin{aligned} x &\lor 1 \equiv 1 \\ x &\land 0 \equiv 0
			\end{aligned} $$
		</figure>
		<p>
			This domination laws provide that if we have a variable signal ${x}$
			and a constant signal ${y,}$ then if ${y = 1,}$ the disjunction
			returns ${1,}$ and if ${y = 0,}$ the conjunction returns ${0.}$ The
			proof:
		</p>
		<figure>
			$$ \begin{array}{c:c:c:c:c} & x & y & x \land y & x \lor y \\ \hline
			& 0 & 0 & 0 & 0 \\ & 1 & 0 & 0 & 1 \\ & 0 & 1 & 0 & 1 \\ & 1 & 1 & 1
			& 1 \\ \end{array} $$
		</figure>
		<h4>Indemponent Laws</h4>
		<p>
			The word
			<q>indemponent</q> is a mathematical term: An operation is
			<i>indemponent</i> if it can be applied multiple times without
			changing the result beyond the initial application. For example, the
			constant function ${f(x) = 2}$ is indempotent. No matter how many
			times we apply it (i.e., passing different values of ${x}$), we will
			always get back ${2.}$ Similarly, the absolute value operator is
			indempotent: ${\lvert x \rvert = \lvert \lvert x \rvert \rvert
			=\lvert \lvert \lvert x \rvert \rvert \rvert.}$ Not matter how many
			times we apply it, we will always get back ${\lvert x \rvert.}$
		</p>
		<p>
			The same phenomenon exists in Boolean logic through the indempotent
			laws:
		</p>
		<dfn>
			<small>Indempotent Laws</small>
			<p>Where ${x}$ is logically equivalent to ${y:}$</p>
			<figure>
				$$ \begin{aligned} x \lor y &\equiv x \\ x \land y &\equiv y
				\end{aligned} $$
			</figure>
		</dfn>
		<p>The proof:</p>
		<figure>
			$$ \begin{array}{c:c:c:c:c} & x & y & x \land y & x \lor y \\ \hline
			& 0 & 0 & 0 & 0 \\ & 1 & 1 & 1 & 1 \\ \end{array} $$
		</figure>
		<p>Note that this implies a corollary:</p>
		<dfn>
			<small>Corollary</small>
			<p>Where ${x}$ is logically equivalent to ${y:}$</p>
			<figure>$$ x \land y \equiv x \lor y $$</figure>
		</dfn>

		<section id="double_negation">
			<h4>Double Negation Laws</h4>
			<p>
				The double negation law is similar to the
				<cite>odd-even sign rule</cite> in mathematics. Recall that the
				odd-even sign rule provides that:
			</p>
			<dfn>
				<small>Odd-Even Sign Rule</small>
				<p>Where ${n \in \Z,}$</p>
				<figure>
					$$ (-1)^n = \begin{cases} 1 ~~~\text{if}~~ n \in \Z_{\text{even}}
					\\ -1 ~~~\text{if}~~ n \in \Z_{\text{odd}} \end{cases} $$
				</figure>
			</dfn>
			<p>
				In other words, ${-1}$ raised to an even number power returns ${1}$
				(e.g., ${-1 \cdot -1 = 1}$) while ${-1}$ raised to an odd number
				power returns ${-1}$ (e.g., ${-1 \cdot -1 \cdot -1 = -1}$). The
				same idea applies to the <var>NOT</var> operator. ${\neg(\neg x)
				\equiv x}$ and ${\neg (\neg (\neg x)) \equiv \neg x.}$
			</p>
			<dfn>
				<small>Double Negation Law</small>
				<p>Where ${x}$ is some signal:</p>
				<figure>$$ \neg (\neg x) \equiv x $$</figure>
			</dfn>
			<p>The proof:</p>
			<figure>
				$$ \begin{array}{c:c:c:c} & x & \neg x & \neg(\neg x) \\ \hline & 0
				& 1 & 0 \\ & 1 & 0 & 1 \end{array} $$
			</figure>
		</section>

		<section id="commutative_law">
			<h4>Commutative Law</h4>
			<p>
				The Boolean commutative law provides that inter-changing the order
				of operands in a Boolean equation does not change its result:
			</p>
			<figure>
				$$ \begin{aligned} (x \land y) &\equiv (y \land x) \\ (x \lor y)
				&\equiv (y \lor x) \end{aligned} $$
			</figure>
			<p>For example, consider the following propositions:</p>
			<ol>
				<li>${n &lt; m}$ and ${m &lt; w}$</li>
				<li>${m &lt; w}$ and ${n &lt; m}$</li>
				<li>${a &lt; b}$ or ${a = b}$</li>
				<li>${a = b}$ or ${a &lt; b}$</li>
			</ol>
			<p>
				The commutative law provides that the propositions (1) and (2) are
				the same. In other words, it doesn't matter if we determine whether
				${n}$ is less than ${m}$ first, or if we determine ${m}$ is less
				than ${w}$ first. This idea is encapsulated in the expression ${n
				&lt; m &lt; w.}$ It also provides that propositions (3) and (4) are
				the same. It doesn't matter whether we determine that ${a}$ is less
				than ${b}$ first, or if we determine that ${a = b}$ first. Hence
				the encapsulating expression ${a \leq b.}$
			</p>
			<p>
				The Boolean commutative law is closely related to the commutative
				law of set theory:
			</p>
			<div class="tripart">
				<figure>
					$$ \begin{aligned} A \cup B &= B \cup A \\ A \cap B &= B \cap A
					\\ \end{aligned} $$
				</figure>
				<figure>
					<img
						src="{% static 'images/A_cup_B.svg' %}"
						alt="A union B"
						loading="lazy"
					/>
				</figure>
				<figure>
					<img
						src="{% static 'images/A_cap_B.svg' %}"
						alt="A cap B"
						loading="lazy"
					/>
				</figure>
			</div>
			<p>
				In the diagram above, ${\text{card}(A) = 6}$ and ${\text{card}(B) =
				5.}$ To determine the cardinality of ${A}$ and ${B}$ combined, it
				doesn't matter if we count the number of elements in ${A}$ first or
				the number of elements in ${B}$ first. We still get:
			</p>
			<figure>
				$$ \begin{aligned} \text{card}(A \cup B) &= \text{card}(B \cup A) =
				\text{card}(A) + \text{card}(B) - \text{card}(A \cap B) \\ &= 6 + 5
				- 2 \\ &= 9 \end{aligned} $$
			</figure>
			<p>The same goes for intersection:</p>
			<figure>
				$$ \begin{aligned} \text{card}(A \cap B) &= \text{card}(A) +
				\text{card}(B) - \text{card}(A \cup B) \\ &= 6 + 5 - 9 \\ &= 11 -
				9\\ &= 2 \end{aligned} $$
			</figure>
			<p>The commutative law's proof:</p>
		</section>

		<section id="associative_law">
			<h4>Associative Law</h4>
			<p>The Boolean associative law provides that:</p>
			<figure>
				$$ \begin{aligned} (x \land (y \land z)) &\equiv ((x \land y) \land
				z) \\ (x \lor (y \lor z)) &\equiv ((x \lor y) \lor z) \\
				\end{aligned} $$
			</figure>
			<p>For example, consider the following propositions:</p>
			<ol>
				<li>${a + b = c}$ and ${p + q = r}$</li>
			</ol>
		</section>
		<section id="distributive_law">
			<h4>Distributive Law</h4>
			<p>The distribute law provides:</p>
			<figure>
				$$ \begin{aligned} (x \land (y \lor z)) &\equiv (x \land y) \lor (x
				\land z) \\ (x \lor (y \land z)) &\equiv (x \lor y) \land (x \lor
				z) \\ \end{aligned} $$
			</figure>
		</section>

		<section id="de_morgan_laws">
			<h4>De Morgan's Laws</h4>
			<p>
				De Morgan's Laws govern how the <var>NOT</var> operator works
				alongside the <var>OR</var> and <var>NOT</var> operator. The laws
				provide:
			</p>
			<figure>
				$$ \begin{aligned} (1)~~\neg (x \land y) &\equiv \neg (x) \lor \neg
				(y) \\ (2)~~\neg (x \lor y) &\equiv \neg (x) \land \neg (y)
				\end{aligned} $$
			</figure>
			<p>
				We can verify these laws via truth table. Verifying the first
				corollary:
			</p>
			<figure>
				$$ \begin{array}{c:c:c:c:c:c:c:c} & x & y & x \land y & \neg (x
				\land y) & \neg x & \neg y & \neg(x) \lor \neg(y) \\ \hline & 0 & 0
				& 0 & 1 & 1 & 1 & 1 \\ & 0 & 1 & 0 & 1 & 1 & 0 & 1 \\ & 1 & 0 & 0 &
				1 & 0 & 1 & 1 \\ & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\ \end{array} $$
			</figure>
			<p>Verifying the second corollary:</p>
			<figure>
				$$ \begin{array}{c:c:c:c:c:c:c:c} & x & y & x \lor y & \neg (x \lor
				y) & \neg x & \neg y & \neg(x) \land \neg(y) \\ \hline & 0 & 0 & 0
				& 1 & 1 & 1 & 1 \\ & 0 & 1 & 1 & 0 & 1 & 0 & 0 \\ & 1 & 0 & 1 & 0 &
				0 & 1 & 0 \\ & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\ \end{array} $$
			</figure>
		</section>

		<section id="absorption_laws">
			<h4>Absorption Laws</h4>
			<p>
				The absorption laws provide a way of dealing with sequences of
				<var>OR</var> operations or sequences of <var>AND</var> operations:
			</p>
			<dfn>
				<small>Absorption Laws</small>
				<p>Where ${x}$ and ${y}$ are variable signals:</p>
				<figure>
					$$ x \lor (x \lor y) \equiv x \\ x \land (x \land y) \equiv x \\
					$$
				</figure>
			</dfn>
			<p>The proof:</p>
			<figure>
				$$ \begin{array}{c:c:c:c:c:c:c} & x & y & x \land y & x \lor y & x
				\land (x \land y) & x \lor (x \lor y) \\ \hline & 0 & 0 & 0 & 0 & 0
				& 0\\ & 1 & 1 & 1 & 1 & 1 & 1\\ \end{array} $$
			</figure>
		</section>

		<section id="negation_laws">
			<h4>Negation Laws</h4>
			<p>
				According to the negation laws, the statement
				<q
					>Mars is Earth's neighbor <em>or</em> Mars is not Earth's
					neighbor</q
				>
				is always true, and the statement
				<q
					>Mars is Earth's neighbor <em>and</em> Mars is not Earth's
					neighbor</q
				>
				is always false. Stated formally:
			</p>
			<dfn>
				<small>Negation Laws</small>
				<p>Where ${x}$ is a variable signal:</p>
				<figure>
					$$ \begin{aligned} x \lor (\neg x) &\equiv 1 \\ x \land (\neg x)
					&\equiv 0 \end{aligned} $$
				</figure>
			</dfn>
		</section>
	</section>

	<section id="boolean_expressions">
		<h3>Boolean Expressions</h3>
		<p>
			Because <var>AND</var>, <var>OR</var>, and <var>NOT</var> are
			operations (much like how addition and subtraction are oeprations),
			the logical operations can be strung together to form
			<b>Boolean expressions</b>. For example, here's a simple Boolean
			expression:
		</p>
		<figure>$$ \neg (0 \lor (1 \land 1)) $$</figure>
		<p>
			Evaluating this expression, we start first with the innermost
			parenthesized expression, ${(1 \land 1).}$ This evaluates to ${1:}$
		</p>
		<figure>
			$$ \begin{aligned} \neg (0 \lor (1 \land 1)) &\equiv \neg (0 \lor 1)
			\end{aligned} $$
		</figure>
		<p>
			Then we evaluate the result, since that's the next parenthesized
			expression:
		</p>
		<figure>
			$$ \begin{aligned} \neg (0 \lor (1 \land 1)) &\equiv \neg (0 \lor 1)
			\\ &\equiv \neg 1 \\ \end{aligned} $$
		</figure>
		<p>The we perform the final operation:</p>
		<figure>
			$$ \begin{aligned} \neg (0 \lor (1 \land 1)) &\equiv \neg (0 \lor 1)
			\\ &\equiv \neg 1 \\ &\equiv 0 \\ \end{aligned} $$
		</figure>
	</section>

	<section id="boolean_functions">
		<h3>Boolean Functions</h3>
		<p>
			Much like algebra, once we have Boolean expressions, we can begin
			creating <b>Boolean functions</b> &mdash; generalized Boolean
			expressions. Boolean functions are precisely how we create the
			additional variety of logical operations. For example, here's a
			Boolean function:
		</p>
		<figure>$$ f(x, y, z) = (x \land y) \lor (\neg (x) \land z) $$</figure>
		<p>
			This function takes three inputs, ${x,}$ ${y,}$ and ${z.}$ Because
			each input is either ${1}$ or ${0,}$ there are ${2^3 = 8}$ possible
			combinations.<sup></sup>
		</p>
		<div class="note">
			<p>
				From formal logic, we know that given ${n}$ statements, there are
				${2^n}$ possible truth value combinations.
			</p>
		</div>
		<figure>
			$$ \begin{array}{c:c:c:c} & x & y & z \\ \hline & 0 & 0 & 0 \\ & 0 &
			0 & 1 \\ & 0 & 1 & 0 \\ & 0 & 1 & 1 \\ & 1 & 0 & 0 \\ & 1 & 0 & 1 \\
			& 1 & 1 & 0 \\ & 1 & 1 & 1 \\ \end{array} $$
		</figure>
		<p>Laying out the possible values)or ${f(x, y, z),}$ we get:</p>
		<figure>
			$$ \begin{array}{c:c:c:c:c:c:c:c} & x & y & z & (x \land y) & \neg x
			& \neg(x) \land z & f(x, y, z) = (x \land y) \lor (\neg (x) \land z)
			\\ \hline & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\ & 0 & 0 & 1 & 0 & 1 & 1 & 1
			\\ & 0 & 1 & 0 & 0 & 1 & 0 & 0 \\ & 0 & 1 & 1 & 0 & 1 & 1 & 1 \\ & 1
			& 0 & 0 & 0 & 0 & 0 & 0 \\ & 1 & 0 & 1 & 0 & 0 & 0 & 0 \\ & 1 & 1 & 0
			& 1 & 0 & 0 & 1 \\ & 1 & 1 & 1 & 1 & 0 & 0 & 1 \\ \end{array} $$
		</figure>
		<p>
			Unlike real functions, Boolean functions have a finite number of
			possible outputs. This makes it easy (for a feasible number of ${n}$
			variables; generally ${n &lt; 4}$) to lay out all the possible
			outputs.
		</p>
	</section>

	<section id="boolean_function_synthesis">
		<h3>Constructing Boolean Functions</h3>
		<p>Suppose we were given the following truth table:</p>
		<figure>
			$$ \begin{array}{c:c:c:c:c} & x & y & z & f \\ \hline & 0 & 0 & 0 & 1
			\\ & 0 & 0 & 1 & 0 \\ & 0 & 1 & 0 & 1 \\ & 0 & 1 & 1 & 0 \\ & 1 & 0 &
			0 & 1 \\ & 1 & 0 & 1 & 0 \\ & 1 & 1 & 0 & 0 \\ & 1 & 1 & 0 & 0 \\
			\end{array} $$
		</figure>
		<p>
			We want to construct a Boolean function that produces this truth
			table. The trick to doing so is to focus on one row at a time: First,
			write a function that particular row, apply it to the rest of the
			values. Second, focus on another row that doesn't match, write a
			function for that particular row, apply it, and so on.
		</p>
		<p>For example, let's focus on the first row:</p>
		<figure>
			$$ \begin{array}{c:c:c:c:c} & x & y & z & f \\ \hline &
			\color{salmon} 0 & \color{salmon} 0 & \color{salmon} 0 &
			\color{salmon} 1 \\ & 0 & 0 & 1 & 0 \\ & 0 & 1 & 0 & 1 \\ & 0 & 1 & 1
			& 0 \\ & 1 & 0 & 0 & 1 \\ & 1 & 0 & 1 & 0 \\ & 1 & 1 & 0 & 0 \\ & 1 &
			1 & 0 & 0 \\ \end{array} $$
		</figure>
		<p>
			A function that produces this row would be: ${(\neg x) \land (\neg y)
			\land (\neg z):}$
		</p>
	</section>
	<figure>
		$$ \begin{array}{c:c:c:c:c:c} & x & y & z & f & (\neg x) \land (\neg y)
		\land (\neg z) \\ \hline & \color{salmon} 0 & \color{salmon} 0 &
		\color{salmon} 0 & \color{salmon} 1 & \color{salmon} 1 \\ & 0 & 0 & 1 &
		0 & \color{salmon} 0 \\ & 0 & 1 & 0 & 1 & \color{salmon} 0 \\ & 0 & 1 &
		1 & 0 & \color{salmon} 0 \\ & 1 & 0 & 0 & 1 & \color{salmon} 0 \\ & 1 &
		0 & 1 & 0 & \color{salmon} 0 \\ & 1 & 1 & 0 & 0 & \color{salmon} 0 \\ &
		1 & 1 & 0 & 0 & \color{salmon} 0 \\ \end{array} $$
	</figure>
	<p>We then consider the next ${1}$ output:</p>
	<figure>
		$$ \begin{array}{c:c:c:c:c} & x & y & z & f \\ \hline & 0 & 0 & 0 & 1
		\\ & 0 & 0 & 1 & 0 \\ & \color{cornflowerblue} 0 &
		\color{cornflowerblue} 1 & \color{cornflowerblue} 0 &
		\color{cornflowerblue} 1 \\ & 0 & 1 & 1 & 0 \\ & 1 & 0 & 0 & 1 \\ & 1 &
		0 & 1 & 0 \\ & 1 & 1 & 0 & 0 \\ & 1 & 1 & 0 & 0 \\ \end{array} $$
	</figure>
	<p>
		A possible function would be: ${(\neg x) \land y \land (\neg z).}$ This
		results in the truth table:
	</p>
	<figure>
		$$ \begin{array}{c:c:c:c:c} & x & y & z & f & (\neg x) \land y \land
		(\neg z) \\ \hline & 0 & 0 & 0 & 1 & \color{cornflowerblue} 0 \\ & 0 &
		0 & 1 & 0 & \color{cornflowerblue} 0 \\ & \color{cornflowerblue} 0 &
		\color{cornflowerblue} 1 & \color{cornflowerblue} 0 &
		\color{cornflowerblue} 1 & \color{cornflowerblue} 1 \\ & 0 & 1 & 1 & 0
		& \color{cornflowerblue} 0 \\ & 1 & 0 & 0 & 1 & \color{cornflowerblue}
		0 \\ & 1 & 0 & 1 & 0 & \color{cornflowerblue} 0 \\ & 1 & 1 & 0 & 0 &
		\color{cornflowerblue} 0 \\ & 1 & 1 & 0 & 0 & \color{cornflowerblue} 0
		\\ \end{array} $$
	</figure>
	<p>
		That takes care of the second ${1}$ output. Now we write a function for
		the third ${1}$ output:
	</p>
	<figure>
		$$ \begin{array}{c:c:c:c:c} & x & y & z & f \\ \hline & 0 & 0 & 0 & 1
		\\ & 0 & 0 & 1 & 0 \\ & 0 & 1 & 0 & 1 \\ & 0 & 1 & 1 & 0 \\ &
		\color{green} 1 & \color{green} 0 & \color{green} 0 & \color{green} 1
		\\ & 1 & 0 & 1 & 0 \\ & 1 & 1 & 0 & 0 \\ & 1 & 1 & 0 & 0 \\ \end{array}
		$$
	</figure>
	<p>
		Here a possible function is ${x \land (\neg y) \land (\neg z),}$
		yielding the truth table:
	</p>
	<figure>
		$$ \begin{array}{c:c:c:c:c} & x & y & z & f & x \land (\neg y) \land
		(\neg z) \\ \hline & 0 & 0 & 0 & 1 & \color{green} 0 \\ & 0 & 0 & 1 & 0
		& \color{green} 0 \\ & 0 & 1 & 0 & 1 & \color{green} 0 \\ & 0 & 1 & 1 &
		0 & \color{green} 0 \\ & \color{green} 1 & \color{green} 0 &
		\color{green} 0 & \color{green} 1 & \color{green} 1 \\ & 1 & 0 & 1 & 0
		& \color{green} 0 \\ & 1 & 1 & 0 & 0 & \color{green} 0 \\ & 1 & 1 & 0 &
		0 & \color{green} 0 \\ \end{array} $$
	</figure>
	<p>Putting it all together, we get:</p>
	<figure>
		$$ \begin{array}{c:c:c:c:c} & x & y & z & f & \color{salmon} (\neg x)
		\land (\neg y) \land (\neg z) & \color{cornflowerblue} (\neg x) \land y
		\land (\neg z) & \color{green} x \land (\neg y) \land (\neg z) \\
		\hline & \color{salmon} 0 & \color{salmon} 0 & \color{salmon} 0 &
		\color{salmon} 1 & \color{salmon} 1 & \color{cornflowerblue} 0 &
		\color{green} 0 \\ & 0 & 0 & 1 & 0 & \color{salmon} 0 &
		\color{cornflowerblue} 0 & \color{green} 0 \\ & \color{cornflowerblue}
		0 & \color{cornflowerblue} 1 & \color{cornflowerblue} 0 &
		\color{cornflowerblue} 1 & \color{salmon} 0 & \color{cornflowerblue} 1
		& \color{green} 0 \\ & 0 & 1 & 1 & 0 & \color{salmon} 0 &
		\color{cornflowerblue} 0 & \color{green} 0 \\ & \color{green} 1 &
		\color{green} 0 & \color{green} 0 & \color{green} 1 & \color{salmon} 0
		& \color{cornflowerblue} 0 & \color{green} 1 \\ & 1 & 0 & 1 & 0 &
		\color{salmon} 0 & \color{cornflowerblue} 0 & \color{green} 0 \\ & 1 &
		1 & 0 & 0 & \color{salmon} 0 & \color{cornflowerblue} 0 & \color{green}
		0 \\ & 1 & 1 & 0 & 0 & \color{salmon} 0 & \color{cornflowerblue} 0 &
		\color{green} 0 \\ \end{array} $$
	</figure>
	<p>
		All that's left to do is to just string these three functions with
		<var>OR</var> operators:
	</p>
	<figure>
		$$ f(x, y, z) = [(\neg x) \land (\neg y) \land (\neg z)] \lor [(\neg x)
		\land y \land (\neg z)] \lor [x \land (\neg y) \land (\neg z)] $$
	</figure>
	<p>
		Although this function is correct, it's fairly complex. A better
		definition would be to simplify the expression. First, examining the
		first two functions:
	</p>
	<figure>
		$$ \begin{aligned} & [(\neg x) \land (\neg y) \land (\neg z)] \\ &
		[(\neg x) \land y \land (\neg z)] \end{aligned} $$
	</figure>
	<p>
		we see that in both of these expressions, we have ${(\neg x)}$ and
		${(\neg z).}$ This means that the only fixed values are ${(\neg x)}$
		and ${(\neg z).}$ The value for ${(\neg y)}$ is captured in both
		expressions. Thus, all we really need to know is ${(\neg x)}$ and
		${(\neg z):}$
	</p>
	<figure>$$ (\neg x) \land (\neg z) $$</figure>
	<p>This reduces our function definition to:</p>
	<figure>
		$$ f(x, y, z) = [(\neg x) \land (\neg z)] \lor [x \land (\neg y) \land
		(\neg z)] $$
	</figure>
	<p>We can then reduce the term:</p>
	<figure>$$ [x \land (\neg y) \land (\neg z)] $$</figure>
	<p>to:</p>
	<figure>$$ [(\neg y) \land (\neg z)] $$</figure>
	<p>resulting in the definition:</p>
	<figure>
		$$ f(x, y, z) = [(\neg x) \land (\neg z)] \lor [(\neg y) \land (\neg
		z)] $$
	</figure>
	<p>From there we can reduce the definition even further:</p>
	<figure>
		$$ f(x, y, z) = (\neg z) \land [(\neg x) \lor (\neg y)] $$
	</figure>
	<p>
		As we might be able to tell, constructing Boolean functions from a
		given truth table is a difficult task. Moreover, finding the shortest
		possible equivalent expression for a given Boolean expression is an
		NP-complete problem. There is no algorithm for finding such an
		expression. This means that, at the moment, a significant aspect of
		designing the logic circuits behind processors depends on human
		ingenuity and labor.
	</p>
	<p>
		What's more remarkable, however, is that we've seen a demonstration of
		the following theorem:
	</p>
	<dfn>
		<small>Canonical Representation Theorem</small>
		<p>
			Any Boolean function can be represented using the operations of
			${\land}$ (<var>AND</var>), ${\lor}$ (<var>OR</var>), and ${\neg}$
			(<var>NOT</var>).
		</p>
	</dfn>
	<p>
		This is a very special theorem. It is because of this theorem that
		computers exist. It turns out, however, that we don't actually need the
		<var>OR</var> operator. We can get away with just <var>AND</var> and
		<var>NOT</var>. This is because the <var>OR</var> operator can be
		expressed with <var>AND</var> and <var>NOT</var>, following De Morgan's
		laws:
	</p>
	<figure>$$ x \lor y \equiv \neg(\neg x) \land (\neg y) $$</figure>
	<p>Accordingly, we have the more generalized form of the theorem:</p>
	<dfn>
		<small>General Canonical Representation Theorem</small>
		<p>
			Any Boolean function can be represented using the operations of
			${\land}$ (<var>AND</var>) and ${\neg}$ (<var>NOT</var>).
		</p>
	</dfn>
	<p>But we can go even a step further:</p>
	<dfn>
		<small>NAND Representation Theorem</small>
		<p>
			Any Boolean function can be represented using the ${\uparrow}$
			(<var>NAND</var>) operator.
		</p>
	</dfn>
	<p>
		The <var>NAND</var> (<var>NOT AND</var>) operator is the result of the
		function:
	</p>
	<figure>$$ f(x,y) = x \uparrow y \equiv \neg (x \land y) $$</figure>
	<p>The truth table:</p>
	<figure>
		$$ \begin{array}{c:c:c:c:c} & x & y & x \land y & \neg(x \land y)
		\equiv x \uparrow y \\ \hline & 0 & 0 & 0 & 1 \\ & 0 & 1 & 0 & 1 \\ & 1
		& 0 & 0 & 1 \\ & 1 & 1 & 1 & 0 \end{array} $$
	</figure>
	<p>
		The proof of the <cite>NAND representation theorem</cite> stems from
		the fact that we can represent <var>AND</var> and <var>NOT</var> with
		<var>NAND</var>. From the
		<cite>General Canonical Representation Theorem</cite>, we know that
		every Boolean function can be be represented using <var>AND</var> and
		<var>NOT</var>, so if we can write these two operations with
		<var>NAND</var>, the <cite>NAND representation theorem</cite> directly
		follows. In this case, we can. The <var>NOT</var> operation is simply:
	</p>
	<figure>$$ \neg x \equiv \neg (x \land x) \equiv x \uparrow x $$</figure>
	<p>and the <var>AND</var> operation is just:</p>
	<figure>
		$$ \begin{aligned} x \land y &\equiv \neg (\neg(x \land y)) \\ &\equiv
		\neg (x \uparrow y) \\ &\equiv (x \uparrow y) \uparrow (x \uparrow y)
		\end{aligned} $$
	</figure>
</section>

<section id="logic_gates">
	<h2>Logic Gates</h2>
	<p>
		A <b>logic gate</b> is a device that implements some functionality that
		can be modeled with Boolean functions. There are two kinds of logic
		gates: <b>elementary gates</b> (gates that implement a strictly Boolean
		operation) and <b>composite gates</b> (gates that a functionality
		beyond a strictly Boolean operation, e.g., addition, multiplication,
		etc.). We begin by covering a few elementary gates.
	</p>
	<p>
		Below are the schematic represenations for the <var>AND</var>,
		<var>NOT</var>, and <var>OR</var> gates:
	</p>
	<figure>
		<img
			src="{% static 'images/elementary_logic_gates.svg' %}"
			alt="Elementary logic gates"
			loading="lazy"
			style="width: 30%"
		/>
	</figure>
	<p>
		The diagrams above are <i>schematic specifications</i> of the gates. We
		can also provide <b>functional specifications</b> and
		<b>truth table specifications</b>. For the <var>AND</var> gate:
	</p>
	<div class="tripart">
		<img
			src="{% static 'images/and_gate.svg' %}"
			alt="Elementary logic gates"
			loading="lazy"
		/>
		<div>
			<pre class="language-pseudo"><code>
				if (a == 1 and b==1)
				then out=1 else out=0
			</code></pre>
		</div>
		<div>
			$$ \begin{array}{c:c:c:c} & \texttt{a} & \texttt{b} &\texttt{out} \\
			\hline & \texttt{0} & \texttt{0} & \texttt{0} \\ & \texttt{0} &
			\texttt{1} & \texttt{0} \\ & \texttt{1} & \texttt{0} & \texttt{0} \\
			& \texttt{1} & \texttt{1} & \texttt{1} \end{array} $$
		</div>
	</div>
	<p>For the <var>OR</var> gate:</p>
	<div class="tripart">
		<img
			src="{% static 'images/or_gate.svg' %}"
			alt="Elementary logic gates"
			loading="lazy"
		/>
		<div>
			<pre class="language-pseudo"><code>
				if (a==1 or b==1)
				then out=1 else out=0
			</code></pre>
		</div>
		<div>
			$$ \begin{array}{c:c:c:c} & \texttt{a} & \texttt{b} &\texttt{out} \\
			\hline & \texttt{0} & \texttt{0} & \texttt{0} \\ & \texttt{0} &
			\texttt{1} & \texttt{1} \\ & \texttt{1} & \texttt{0} & \texttt{1} \\
			& \texttt{1} & \texttt{1} & \texttt{1} \end{array} $$
		</div>
	</div>
	<p>For the <var>NOT</var> gate:</p>
	<div class="tripart">
		<img
			src="{% static 'images/not_gate.svg' %}"
			alt="Elementary logic gates"
			loading="lazy"
		/>
		<div>
			<pre class="language-pseudo"><code>
				if (in==0)
				then out=1 else out=0
			</code></pre>
		</div>
		<div>
			$$ \begin{array}{c:c} & \texttt{in} & \texttt{out} \\ \hline &
			\texttt{0} & \texttt{1} \\ & \texttt{1} & \texttt{0} \\ \end{array}
			$$
		</div>
	</div>
	<h3>NAND Gate</h3>
	<p>
		The <var>NAND</var> gate can be implemented as by wiring together and
		<var>AND</var> gate and a <var>NOT</var> gate:
	</p>
	<figure>
		<img
			src="{% static 'images/nand_gate_schematic.svg' %}"
			alt="Elementary logic gates"
			loading="lazy"
			style="width: 40%"
		/>
	</figure>
	<p>
		The <var>NAND</var> gate representation above is the <b>interface</b>.
		It's what the user actually interacts with. The gates inside the box
		&mdash; the <var>AND</var> and <var>NOT</var> gates comprising
		<var>NAND</var> &mdash; are the <b>implementation</b> of
		<var>NAND</var>.
	</p>
	<p><var>NAND</var>'s specifications are as follows:</p>
	<div class="tripart">
		<img
			src="{% static 'images/nand_gate.svg' %}"
			alt="Elementary logic gates"
			loading="lazy"
		/>
		<div>
			<pre class="language-pseudo"><code>
				if (a==1 and b==1)
				then out=0 else out=1
			</code></pre>
		</div>
		<div>
			$$ \begin{array}{c:c:c:c} & \texttt{a} & \texttt{b} &\texttt{out} \\
			\hline & \texttt{0} & \texttt{0} & \texttt{1} \\ & \texttt{0} &
			\texttt{1} & \texttt{1} \\ & \texttt{1} & \texttt{0} & \texttt{1} \\
			& \texttt{1} & \texttt{1} & \texttt{0} \end{array} $$
		</div>
	</div>

	<section id="circuit_implementations">
		<h3>Circuit Implementations</h3>
		<p>
			We might be wondering, how exactly do these gates work physically?
			This is a fair question, because the gate schematics are really an
			abstraction for a <b>circuit</b>. Accordingly, the implementations
			for elementary gates are actually circuits. For example, the
			<var>AND</var> gate can be represented with a schematic specification
			of its <b>circuit implementation</b>:
		</p>
	</section>
	<figure>
		<img
			src="{% static 'images/and_circuit.svg' %}"
			alt="and circuit"
			loading="lazy"
			style="width: 60%"
		/>
		<figcaption><var>AND</var> circuit</figcaption>
	</figure>
	<p>For the <var>OR</var> gate:</p>
	<figure>
		<img
			src="{% static 'images/or_circuit.svg' %}"
			alt="or circuit"
			loading="lazy"
			style="width: 60%"
		/>
		<figcaption><var>OR</var> circuit</figcaption>
	</figure>
	<p>
		In this volume, we don't deal with these circuit implementations. They
		are presented here purely to satisfy any curiosity we might have about
		how these gates actually work in reality. The design and implementation
		of these circuits falls within the realm of electrical engineering, not
		computer science.
	</p>

	<section id="hardware_description_language">
		<h3>Hardware Description Language</h3>
		<p>
			When we're asked to design a logic gate, we want to always ask for as
			much information as we can. This means we always want to possibly
			construct a truth table. For example, suppose the client asks for a
			gate that:
		</p>
		<figure>
			<p>
				Outputs <var>1</var> if and only if one of its two inputs is
				<var>1</var>.
			</p>
		</figure>
		<p>Specifying this as a truth table:</p>
		<figure>
			$$ \begin{array}{c:c:c:c} & \texttt{a} & \texttt{b} & \texttt{out} \\
			\hline & \texttt{0} & \texttt{0} & \texttt{0} \\ & \texttt{0} &
			\texttt{1} & \texttt{1} \\ & \texttt{1} & \texttt{0} & \texttt{1} \\
			& \texttt{1} & \texttt{1} & \texttt{0} \\ \end{array} $$
		</figure>
		<p>This particular gate is called a <b>XOR gate</b>:</p>
		<figure>
			<img
				src="{% static 'images/xor_gate.svg' %}"
				alt="or circuit"
				loading="lazy"
				style="width: 40%"
			/>
			<figcaption><var>XOR</var> gate</figcaption>
		</figure>
		<p>
			With this information, we can specify this design through
			<b>hardware description language (HDL)</b> &mdash; a computer
			language for describing the structure and behavior of digital logic
			circuits. For the <var>XOR</var> gate, we would write the following
			in an <var>.hdl</var> file:
		</p>
		<pre class="language-pseudo"><code>
			/** Xor gate: out = (a And Not(b)) Or (Not(a) And b)) */
			CHIP Xor {
				IN a, b;
				OUT out;

				PARTS:
				// Implementation goes here
			}
		</code></pre>
		<p>
			In the code above, the implementation (currently noted as
			"Implementation missing") is written in an HDL stub file. To write
			this implementation, we'll need to come up with a way to implement
			<var>XOR</var> using the gates we already have, <var>AND</var>,
			<var>OR</var>, <var>NOT</var>, and <var>NAND</var>.
		</p>
		<p>
			To do so, we take a closer look at the truth table. In this case, we
			see that there are only two cases where the output is <var>1</var>:
		</p>
		<ol>
			<li>
				<var>a</var> is <var>0</var> and <var>b</var> is <var>1.</var>
			</li>
			<li>
				<var>a</var> is <var>1</var> and <var>b</var> is <var>0</var>.
			</li>
		</ol>
		<p>Accordingly, we have the diagram:</p>
		<figure>
			<img
				src="{% static 'images/XOR_implementation.svg' %}"
				alt="or circuit"
				loading="lazy"
				style="width: 80%"
			/>
			<figcaption><var>XOR</var> implementation</figcaption>
		</figure>
		<p>In our <var>hdl</var> file, we write:</p>
		<pre class="language-pseudo"><code>
			/** Xor gate: out = (a And Not(b)) Or (Not(a) And b)) */
			CHIP Xor {
				IN a, b;
				OUT out;

				PARTS:
				Not (in=a, out=nota);
				Not (in=b, out=notb)
				And (a=a, b=notb, out=aAndNotb);
				And (a=nota, b=b, out=notaAndb);
				Or  (a=aAndNotb, b=notaAndb, out=out);
			}
		</code></pre>
		<p>
			The <var>hdl</var> file is really nothing more than a textual
			description of the gate diagram.
		</p>
		<figure>
			<img
				src="{% static 'images/hdl_xor.svg' %}"
				alt="or circuit"
				loading="lazy"
				style="width: 100%"
			/>
		</figure>
	</section>
</section>

<section id="information">
	<h2>What is information?</h2>
	<p>
		We begin by reviewing the concept of <b>information</b> &mdash;
		knowledge communicated or perceived about a particular fact or
		circumstance. In the computer science context, information is
		processed, organized, or structured data.<sup></sup> That data is a
		resolution of uncertainty &mdash; it answers the
		<i>declarative question</i>: <q>What is ${x?}$</q> where ${x}$ is some
		subject. That subject, ${x,}$ could be anything: The location of the
		closest coffee shop, the likelihood of two individuals forming a
		relationship, or more generally, the answer to a particular question
		${q,}$ and so on.
	</p>
	<div class="note">
		<p>
			This is a key difference between <i>data</i> and <i>information</i>.
			Data can be a collection of redundant or meaningless symbols, but
			information is organized data that resolves certainty.
		</p>
	</div>
	<p>
		The less predictable the answer to the declarative question is, the
		more information the answer conveys. This is implied by the fact that
		information is a resolution of uncertainty. Let's say the amount of
		information needed to convey a low probability event A is ${A_I.}$ Then
		let's say the amount of information needed to convey a high probability
		event ${B}$ is ${B_I.}$ It follows that ${A_I &gt; B_I,}$ since ${A}$
		is a more surprising event than ${B.}$ More generally, the more
		uncertain, rare, or surprising-if-it-occurs an event is, the more
		information is needed to represent that particular event.
	</p>
	<p>
		How do we represent this information? One way to do so is with
		<i>bits</i>. Say we had a switch for a lighbulb that goes on or off.
		The switch has ${2}$ possible states &mdash; ${0}$ or ${1.}$ If we
		lined up two of these switches and interpreted them as a unit, we would
		have ${4}$ possible states. In general, if we had ${N}$ switches lined
		up, we would have ${2^N}$ possible states. This in turn means that with
		${N}$ switches, we have ${\log_{2}2^N = N}$ bits at our disposal.
	</p>
	<p>
		We can calculate the amount of information there is in an event ${x}$
		using the probability of the event ${x}$:
	</p>
	<figure>
		$$ \text{information}(x) = - \log_2 (~p(x)~) $$
		<figcaption>
			where ${x}$ is some event, and ${p(x)}$ is the probability of ${x.}$
		</figcaption>
	</figure>
	<p>
		Suppose you're faced with ${n}$ equally probably choices, and our
		friend Jihei tells us a fact ${x}$ that narrows it down to ${m}$
		possible choices. The fact ${x}$ consists of ${I(x)}$ information:
	</p>
	<figure>
		$$ \begin{aligned} I(n) - I(x) &= I(m) \\ I(n) &= I(m) + I(x) \\ I(n) -
		I(m) &= I(x) \\ \end{aligned} $$
	</figure>
	<p>Thus, Jihei gave us:</p>
	<figure>
		$$ \begin{aligned} I(x) &= I(n) - I(m) \\[1em] &= \log_{2} \left(
		\dfrac{n}{m} \right) \end{aligned} $$
	</figure>
	<p>bits of information. The amount of information in one coin flip:</p>
	<figure>$$ I(f) = \log_{2} \left( \dfrac{2}{1} \right) $$</figure>
	<p>which is ${1}$ bit of information. The roll of ${2}$ dice:</p>
	<figure>$$ I(d) = \log_{2} \left( \dfrac{36}{1} \right) $$</figure>
	<p>
		which is ${5.2}$ bits. Once we have an idea about how many bits are
		needed, we need to <b>encode</b>. <i>Encoding</i> is the process of
		assigning representations to information. Choosing an appropriate and
		efficient encoding is a real engineering challenge. There numerous
		factors to consider:
	</p>
	<ul>
		<li>
			<i>Mechanism</i> &mdash; The more mechanisms the process contains,
			the more complex the encoding is, and more difficult it is to improve
			or modify in the future. Too little mechanisms, and we start limiting
			the encodings functionality.
		</li>
		<li>
			<i>Efficiency</i> &mdash; The number of bits used. We want the
			minimal about of bits as possible to represent as much information as
			possible. Too few bits, the less information we can represent.
		</li>
		<li>
			<i>Reliability</i> &mdash; We want little to no <i>noise</i>.
			Essentially, the reliability of the encoding in a variety of
			environments. We do not want the encoding breaking down when it's
			suddenly fed different types of information.
		</li>
		<li>
			<i>Security</i> &mdash; The key issue here is <i>encryption</i>. We
			want the encoding to have maximal security.
		</li>
	</ul>
	<p>
		If all the choices are equally likely, a simple way to encode is
		<b>fixed-length encoding</b>. For example, with the decimal digits,
		there are ${10}$ possible choices:
	</p>
	$$ \{ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 \} $$
	<p>Since we have ${10}$ possible choices, we need:</p>
	<figure>$$ \log_{2}(10) \approx 3.322 $$</figure>
	<p>bits. Thus, to encode the ${10}$ digits, need ${4}$ bits.</p>
	<p>
		For the standard English characters &mdash; uppercase letters (${26}$),
		lowercase letters (${26}$), decimal digits (${10}$), punctuation
		(${11}$), math (${9}$), finance symbols (${4}$) &mdash; there are
		roughly ${86}$ possible choices. Thus, we need:
	</p>
	<figure>$$ \log_{2}(86) \approx 6.426 $$</figure>
	<p>
		bits. Accordingly, we need ${7}$ bits to represent the possible
		choices. Hence the name, <i>7-bit ASCII</i>.
	</p>
</section>

<section id="6502_processor">
	<h2>Overview of the 6502</h2>
	<p>
		Below is a pinout diagram of the 6502 processor. Let's go over some of
		the key features of the pinout.
	</p>
	<figure>
		<img
			src="{% static 'images/6502.svg' %}"
			alt="6502 Diagram"
			loading="lazy"
			width="400px"
			height="400px"
		/>
	</figure>
	<p>
		First, all of the pins above have specific purposes that we'll go over
		in due time. For example, pin 1 is labeled <var>RDY</var>. This is the
		<b>ready pin</b>. It receives signals &mdash; called
		<i>ready signals</i> &mdash; indicating that something has happened
		(e.g., the television set is ready, the joystick is ready, etc.). At
		pin 4, there's a label ${\overline{\texttt{IRQ}}.}$ This pin receives
		<b>interruption requests</b>. Working with assembly, we'll be turning
		back to this diagram repeatedly for reference.
	</p>
	<p>
		Importantly, there are two large swathes of pins called the
		<b>address bus</b> and the <b>data bus</b>. Notice that the data bus
		region consists of eight pins. This corresponds to the fact that there
		<em>${8}$ bits</em> for the data bus. These eight bits are responsible
		for moving values to and from the <b>processor registers</b>.
	</p>
	<p>
		What are processor registers? We can think of them as small places
		inside the processor that store bits &mdash; zeroes and ones. On the
		2600, each register in the processor can only hold eight bits at any
		given time. These days, most machines run on 64-bit processors &mdash;
		processors whose registers can hold sixty-four bits.
	</p>
	<p>
		In addition to the data bus, we have a region called the
		<b>address bus</b>. The pins comprising the address bus are responsible
		for storing <b>memory addresses</b>. We can think of them as trackers.
		They're job is to always know where things are at any given time.
		Notice that there are 16 pins. This is why the 2600 has ${16}$ bits
		&mdash; ${2}$ bytes &mdash; to store a memory address.
	</p>
	<p>
		So, that's the 6502. What about the 6507? Well, the 6507 is essentially
		the 6502 with just a few differences: (1) The pins <var>A13</var>,
		<var>A14</var>, <var>A15</var>, and a few interruption lines are
		inaccessible.
	</p>
	<p>
		The 6507 processes ${1.19}$ million instructions per second. We can
		think of this as though the CPU were a clock, ticking ${1.19}$ million
		times per second. In fact, that analogy wouldn't be too far off &mdash;
		formally, each
		<q>tick</q> is called a <b>clock cycle</b>.
	</p>
	<p>We will explore the processor more deeply in a later section.</p>
</section>

<section id="numbers">
	<h2>Numbers</h2>
	<p>
		Before we examine assembly code, it's worth quickly reviewing how
		information is represented in a computer.
	</p>
	<p>
		We've heard time and time again &mdash; computers only understand ones
		and zeroes. Now that we're working at such a low level, it's worth
		asking, do they actually? It turns out no. They do not. The ones and
		zeroes are just abstractions to aid in representing a binary system
		&mdash; on or off, yes or no, true or false, <i>one</i> or <i>zero</i>.
		What a computer really understands is just that &mdash; two opposite
		states. Or more concretely, two opposite <i>impulses</i>.
	</p>
	<p>
		The impulses can come in a variety of forms. Sometimes, they're
		electric impulses: In the processor, we have a
		<i>low voltage</i> or a <i>high voltage</i>. In other cases, they're
		light impulses: On a CD, the impulses are generated by lasers. If the
		laser's light bounces off of the CD, we have a ${1,}$ if it does not,
		we have a ${0.}$ And yet in others, they're magnetic: On a hard disk
		drive, if the driver's lever is magnetically attracted to some area of
		the disk, we have a ${0,}$ and if it's magnetically repelled, we have a
		${1.}$ We call the abstractions of ${1}$ and ${0}$ <b>bits</b>.
	</p>
	<p>
		Now, how do numbers come in? Well, thanks to some very clever
		electrical engineers, we can group bits together. The ability to group
		bits together is what allows us to represent the many numbers we work
		with on a daily basis.
	</p>
</section>

<section id="registers">
	<h2>Overview: CPU</h2>
	<p>
		The CPU (Central Processing Unit) is a machine for executing programs.
		The two key aspects of the CPU are:
	</p>
	<ol>
		<li>registers, and</li>
		<li>instructions</li>
	</ol>
	<p>
		We can think of registers as variables. For example, on the 8080
		processor, we have five ${8}$-bit registers. From a ${C}$ perspective,
		we can think of those registers as:
	</p>
	<pre class="language-c"><code>
		unsigned char A;
		unsigned char B;
		unsigned char C;
		unsigned char D;
		unsigned char E;
	</code></pre>
	<p>
		Every processor has a component called the <b>program counter (PC)</b>,
		which we can think of as a pointer:
	</p>
	<pre class="language-c"><code>
		unsigned char A;
		unsigned char B;
		unsigned char C;
		unsigned char D;
		unsigned char E;

		unsigned char* PC;
	</code></pre>
	<p>
		For the CPU, program instructions are hexadecimal numbers, and a
		program is just a sequence of hexadecimal numbers. Each assembly
		language instruction corresponds to ${1}$ to ${3}$ bytes.
	</p>
	<p>
		The 6507 processor uses a 28-pin configuration, running at ${1.19
		\text{MHz},}$ or <q>ticking</q> at ${1.19}$ million times per second
		(each tick is called a <b>clock cycle</b>). There are ${13}$ address
		pins and ${8}$ data pins. This means there are ${8}$ bits for data, and
		${13}$ bits for addressing. This totals ${21}$ pins. What about the
		other ${7}$ pins? They're used for the CPU's power, timing clock, rest,
		request bus wait states (e.g., the RDY pin), and read/write commands to
		memory from the CPU (these pins determine whether we're reading or
		writing to memory at any given moment).
	</p>
	<p>
		Importantly, unlike the 6502, we don't get IRQ (interruption request)
		or NMI (non-maskable interrupt) pins. Because these pins are
		unavailable, we cannot perform interrupts on the Atari 2600. This isn't
		pertinent at the moment, but we mention it now to clarify the
		differences between the 6507 and 6502.
	</p>
	<p>
		There are seven main parts to both the 6502 and 6507 processor. A rough
		diagram of the processor is as follows:
	</p>
	<figure>
		<img
			src="{% static 'images/CPU.svg' %}"
			alt="6502 processor"
			loading="lazy"
			width="300px"
			height="300px"
		/>
	</figure>
	<p>
		The dark area above is the processor &mdash; it communicates with both
		the data bus and the address bus. The data bus transfers data to and
		from memory, while the address bus passes an address, sent from the
		CPU, to the ram &mdash; memory.
	</p>
	<p>
		We also have a component called the <b>ALU (Arithmetic Logic Unit)</b>.
		It goes without saying that the processor must perform basic
		computations &mdash; addition, subtraction, determining whether a value
		is less than, greater than, or equal to another, as well as the logical
		deductions from logical operations like AND and OR. Thes are the core
		arithmetic and logical computations, and the ALU, which resides in the
		processor, is responsible for them.<sup></sup>
	</p>
	<div class="note">
		<p>
			More accurately, the ALU only performs bitwise operations on integer
			binary numbers. The notion of arithmetic and logic are simply
			abstractions for the ALU's manipulation of zeroes and ones.
		</p>
	</div>
	<p>
		Next, we have the <b>registers</b>. The 6502 has six addressable
		registers. These registers are:
	</p>
	<ol>
		<li>
			The <b>program counter (PC)</b>, which is responsible for storing the
			address of the next instruction that must be executed. For example,
			if one of our instructions is to load the integer <var>2</var> in
			memory, that instruction will have an address, stored by the PC
			register.
		</li>
		<li>
			The <b>stack pointer (SP)</b>, which points to the top of the
			<i>stack</i>. We've heard of the stack and the heap often; the stack
			pointer always holds the memory address of the current top-most frame
			in the RAM's stack memory.
		</li>
		<li>
			The <b>processor flag register (P)</b> is responsible for storing
			data about what happened in the processor's last execution. For
			example, whether the last computation resulted in an overflow,
			returned negative, or zero.
		</li>
		<li>
			<b>X, Y, & A (Accumulator) Registers.</b> These three registers are
			essentially general-purpose registers. We can use them to store any
			kind of value to make our lives easier. For example, these registers
			can be used to keep track of some loop-counter. The accumulator
			register in particular is what we must use if we want to perform
			computations with the ALU.
		</li>
	</ol>
	<p>
		Because the 6507/6502 is an 8-bit processor, each of the registers
		consists of <em>eight bits</em>. There are, however, two registers that
		are slighly different &mdash; the stack pointer and program counter.
		Because these two registers must keep track of memory addresses, the
		stack pointer and the program counter require ${16}$ bits.
	</p>
	<p>
		Next, the processor status register (the <i>P register</i>), has the
		following layout for its eight bits:
	</p>
	<div class="compare">
		<ol class="array">
			<li>
				<ul>
					<li>n</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>v</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>&ThickSpace;</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>b</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>d</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>i</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>z</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>c</li>
				</ul>
			</li>
		</ol>
	</div>
	<p>
		The <b>c</b> flag stands for <b>carry</b>. If that bit contains a
		<var>1</var>, then the last execution contained a carry.
	</p>
	<p>
		The <b>z</b> flag stands for <b>zero</b>. If that bit contains a
		<var>1</var>, then the last execution resulted in a zero.
	</p>
	<p>
		The <b>i</b> flag stands for <b>IRQ disabled</b>. If the bit is
		<var>1</var>, then the last execution disabled an interrup (i.e.,
		ignoring messages from some hardware). Note that we won't concern
		ourselves with this particular flag since we do not have interrupts.
	</p>
	<p>
		The <b>d</b> flag stands for <b>decimal mode</b>. This particular flag
		will be explored in a section on to itself. In short, the processor can
		perform its executions in either binary or in
		<i>binary-coded decimal</i> (BCD). Performing executions in BCD leads
		to more accurate computations, but it is much slower than performing
		executions in binary.
	</p>
	<p>
		The <b>b</b> flag stands for <b>break instruction</b>. This flag
		indicates that the last execution was interrupted, per another
		instruction.
	</p>
	<p>
		The <b>v</b> flag stands for <b>overflow</b>. A <var>1</var> in this
		bit indicates that the last instruction resulted in an overflow. For
		example, adding two values that resulted in overflowing the ${8}$ bits
		available.
	</p>
	<p>
		The <b>n</b> flag stands for <b>negative</b>. A <var>1</var> in this
		bit indicates that the last execution's result was negative.
	</p>
	<p>
		Notice that there's one idle bit. It's unclear why this bit was left
		unused, but it's more than likely the result of a manufacturing
		decision after price negotiations.
	</p>
</section>

<section id="carry_flag">
	<h2>Processor Status Flags</h2>
	<p>
		Because of how useful processor flags are, we explore them a little
		more deeply. As we know, we only have eight bits to work with. Say we
		load the integer ${255}$ in memory. This results in:
	</p>
	<div class="compare">
		<ol class="array">
			<li>
				<ul>
					<li>1</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>1</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>1</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>1</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>1</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>1</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>1</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>1</li>
				</ul>
			</li>
		</ol>
	</div>
	<p>
		In other words, we're using all ${8}$ bits. Now say we add a ${1}$ to
		this number. In binary, if we add a ${1}$ to this number, we get the
		following:
	</p>
	<figure>
		$$ \begin{aligned} & 1 & 1 & 1 & 1 & 1 & 1 & 1 & & \\ & 1 & 1 & 1 & 1 &
		1 & 1 & 1 & 1 \\ + & & & & & & & & 1 \\ \hline 1 & 0 & 0 & 0 & 0 & 0 &
		0 & 0 & 0 \end{aligned} $$
	</figure>
	<p>
		We keep carrying over and over until we reach the last available bit.
		But what happens to that carryover bit? It gets sent to the
		<i>P register</i>'s <var>c</var> flag:
	</p>
	<div class="compare">
		<ol class="array">
			<li>
				<ul>
					<li>0</li>
					<li>n</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>0</li>
					<li>v</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>0</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>0</li>
					<li>b</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>0</li>
					<li>d</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>0</li>
					<li>i</li>
				</ul>
			</li>
			<li>
				<ul>
					<li>0</li>
					<li>z</li>
				</ul>
			</li>
			<li>
				<ul>
					<li><span class="blueText">1</span></li>
					<li>c</li>
				</ul>
			</li>
		</ol>
	</div>
	<p>
		This is what we mean by a <i>carry</i>, in terms of the processor
		flags. Before consider the other flags, we briefly detour into the
		issue of representing negative numbers in a computer.
	</p>

	<section id="negatives">
		<h3>Representing Negative Numbers</h3>
		<p>
			To represent numbers, computers begin with a foundational premise:
		</p>
		<figure>
			<div>
				<p>All numbers are one of two states: positive or negative.</p>
			</div>
		</figure>
		<p>
			Clearly, this departs from mathematics. We know zero is neither
			positive nor negative, and there are nonreal numbers, i.e., imaginary
			numbers. Computers, however, aren't that smart. They're fast, but
			they aren't Gausses or Eulers.
		</p>
		<p>
			Because of the premise above, one way to represent negative nubers is
			to reserve the last bit to represent the <i>sign</i> of a number.
			This bit is called the <b>sign bit</b>:
		</p>
		<div class="compare">
			<ol class="array">
				<li>
					<ul>
						<li><span class="redText">0</span></li>
					</ul>
				</li>
				<li>
					<ul>
						<li>1</li>
					</ul>
				</li>
				<li>
					<ul>
						<li>1</li>
					</ul>
				</li>
				<li>
					<ul>
						<li>1</li>
					</ul>
				</li>
				<li>
					<ul>
						<li>1</li>
					</ul>
				</li>
				<li>
					<ul>
						<li>1</li>
					</ul>
				</li>
				<li>
					<ul>
						<li>1</li>
					</ul>
				</li>
				<li>
					<ul>
						<li>1</li>
					</ul>
				</li>
			</ol>
		</div>
		<p>
			On most computers &mdash; the 6502/07 included &mdash; a
			<var>0</var> maps to a positive, and a <var>1</var> to a negative.
		</p>
		<p>
			As always, there's a cost to using the sign bit. Because we lose that
			bit, we now only have ${7}$ bits to work with. And given that we only
			have ${7}$ bits, we can only represent numbers in the range
			${(-2^7-1, 2^7-1).}$ In other words, ${-127}$ to ${127.}$
		</p>
		<p>
			This approach to negative number representation is called
			<b>sign-and-magnitude</b>. It is <em>not</em> how most computers
			represent negatives. The most obvious problem with this approach is
			that we end up with two values for zero:
		</p>
		<figure>
			$$ \begin{aligned} \texttt{+0 = 0 0 0 0 0 0 0 0} \\ \texttt{-0 = 1 0
			0 0 0 0 0 0} \\ \end{aligned} $$
		</figure>
		<p>
			Moreover, sign-and-magnitude is cumbersome to implement. Adding and
			subtracting, very basic operations, are needlessly tedious under this
			approach.
		</p>
		<p>
			A much better &mdash; and more clever &mdash; take is
			<b>two's complement</b>.
		</p>
	</section>
</section>

<section id="assembler_flow">
	<h2>The Assembler Flow</h2>
	<p>
		As we mentioned earlier, a program is nothing more than a sequence of
		hexadecimal numbers to the processor. Hexadecimal numbers, however, are
		difficult to write, so use an abstraction &mdash; assembly. Unlike
		other languages like C or Python, Assembly is about as low as we can
		get without having to write code strictly in numerals.
	</p>
	<p>
		But how does the processor understand Assembly instructions? For
		example, how do we tell the processor we want to load the value
		<var>2</var> into the A register? The idea is to send a sequence of
		bits:
	</p>
	<figure>
		$$ \underbrace{\texttt{0101}}_{\normalsize
		\texttt{A}}~~\underbrace{\texttt{1001}}_{\normalsize
		\texttt{9}}~~\underbrace{\texttt{0000}~~
		\texttt{\textcolor{salmon}{0010}}}_{\normalsize 2} $$
	</figure>
	<p>
		to the processor's pins. Above, the hexadecimal <var>A</var> is called
		an <b>operation code (opcode)</b> &mdash; a sequence of bits that the
		processor compares against its hardwired opcode table to determine
		which instruction it must execute. In this case, the processor
		determines the sequence as corresponding to <var>A9</var>,
	</p>
	<p>
		Because we're speaking directly with the processor, we have to remember
		a key fact &mdash; computers are remarkably stupid. They don't
		understand things like nuance and context, unless we define them. At
		higher levels, we might think that computers are quickly approaching
		the singularity, but nothing could be further from the truth.
		Processors cannot read our minds &mdash; we have to explicitly state
		the <em>exact</em> series of steps towards accomplishing a particular
		task.
	</p>
</section>

{% endblock %}
