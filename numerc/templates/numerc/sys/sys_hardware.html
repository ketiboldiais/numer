{% extends '../layout.html' %} {% load static %} {% block description %}
<meta
	name="description"
	content="Overview of hardware; CPU, ALU, RAM, I/O devices, motherboard, and more."
/>
{% endblock %} {% block title %}
<title>Computer Hardware</title>
{% endblock %} {% block content %}
<h1>Hardware: An Overview</h1>
<section id="numbers">
	<p>
		<span class="drop">A</span> <span class="term">computer</span> is an
		information-processing system consisting of two parts: (1)
		<span class="italicsText">software</span>, and (2)
		<span class="italicsText">hardware</span>. The software dictates what
		processing should be done, and the hardware performs the processing in
		response to the software's commands. Many people think the word
		&#8220;hardware&#8221; implies the monitor, mouse, keyboard, motherboard,
		RAM, and all the other pieces we can recall. In reality, the word
		&#8220;hardware&#8221; refers to just one component, the CPU. All the other
		pieces are called <span class="term">peripherals</span>. The combination of
		software, hardware, and peripherals is called a
		<span class="term">computer system</span> &mdash; what many people think of
		when they hear &#8220;computer.&#8221;
	</p>
	<p>
		The computer is a <span class="term">universal computational device</span>.
		To understand what this means, it's helpful to think about other
		computational devices. The abacus is a computational device. So too is the
		slide rule. The wrist watch if also a computational device. They all perform
		some computation: The abacus, addition; the slide rule, logarithms; and the
		wrist watch, time. These devices have their limits. If we wanted to
		calculate square roots with an abacus, we'd best have pencil and paper. We
		can imagine similar limitations for the others.
	</p>
	<p>
		The computer, however, is different. A core premise in modern computer
		science is that anything that can be computed can be computed by a computer.
		This premise originates in <span class="term">Turing's thesis</span>: Every
		computation can be performed by a
		<span class="italicsText">Turing machine</span> &mdash; an abstract machine
		that manipulates symbols on a strip of tape, according to a table of rules.
	</p>
	<p>
		The Turing machine works as such: An infinite piece of tape is fed to a
		machine. The tape is lined with boxes (called
		<span class="italicsText">cells</span>), each either empty or containing a
		symbol. The machine consists of a head, that can
		<span class="italicsText">read</span> and
		<span class="italicsText">write</span>. As the tape is fed to the machine,
		the machine scans each cell. Based on the user's instructions, it either
		writes, erases, or does nothing. Once the cell is processed, the head moves
		to either the previous cell or the next (again depending on the user's
		instructions) and performs the same process. Knowing that computer
		instructions are really just sequences of 1s and 0s, with a little
		imagination on our part, we can roughly outline how this abstract machine
		could perform operations like addition, subtraction, multiplication,
		division, and so on. We could have a machine that performs addition. We
		could then combine that machine with another to perform multiplication,
		resulting in a <span class="italicsText">black box</span>.
	</p>
	<p>
		Turing's key insight, however, is that following this thought process, we
		could create a universal Turing machine that creates Turing machines. It is
		from the universal Turing machine that we have the coup-de-grace: A Turing
		machine can compute anything that can be computed because it is programmable
		&mdash; i.e., as long as we can state the instructions for how to perform a
		computation (a true condition only if the computation can be computed), we
		can feed those instructions to the Turing machine for computation. And
		because a computer is an instance of a Turing machine (they perform a
		computation based on the instructions we feed it), anything that can be
		computed can be computed by a computer. Hence the description
		<span class="italicsText">universal computational device</span>.
	</p>
	<p>
		But how do we go from these abstract machines to a modern-day computer? To
		answer this question, we have to understand how computers work &mdash; the
		very focus of this volume. For now, let's present a very high-level outline.
	</p>
	<p>
		Stage 1: Identify the problem. For example, alphabetizing a list of names,
		or computing the average price of a Big Mac. In doing so, we state the
		problem in natural language: &#8220;Find the most populous city.&#8221;
	</p>
	<p>
		Stage 2: Construct the problem's solution &mdash; an
		<span class="term">algorithm</span>. In computer science and mathematics, an
		algorithms is a sequence of instructions with a strict set of traits: (1) It
		must be <span class="italicsText">finite</span> (the algorithm eventually
		stops); (2) it must be <span class="italicsText">definite</span> (each
		instruction is precisely stated); and (3) it must be
		<span class="italicsText">effective</span> (a computer can perform the
		computation).
	</p>
	<p>
		Stage 3: Translate the algorithm into a computer program. This is the stage
		where we <span class="italicsText">code</span>. The algorithm, in crude
		form, might be a flow chart, a written description, a diagram on a
		whiteboard, or a thought in our mind. When we code, we transform this crude
		medium into a <span class="italicsText">programming language</span>.
	</p>
	<p>
		Stage 4: Translating the source code into instructions the CPU can
		understand. To perform this translation, a program called a
		<span class="italicsText">compiler</span> takes the source code and returns
		instructions according to the machine's
		<span class="term">instruction set architecture</span> (ISA). Roughly, the
		ISA is a description of the machine. It includes: specifications for what
		instructions on the machine look like (i.e., what a particular combination
		of 1s and 0s mean for the machine), what
		<span class="italicsText">registers</span> are available, how wide those
		registers are (e.g., 32-bit, 64-bit, 128-bit, etc.), how data values are
		represented, and what <span class="italicsText">addressing modes</span> are
		supported (i.e., specifications for the mechanisms the processor uses to
		retrieve data); quite literally a set of instructions that effectively
		serves as describing the machine.
	</p>
	<p>
		There are numerous different ISAs. One of the most famous examples is the
		x86, an ISA invented and developed by Intel. Other ISAs include ARM, THUMB,
		POWER, and SPARC.
	</p>
	<p>
		Stage 5: The CPU responds to the instructions. How the CPU responds to the
		instructions received depends on the CPU's
		<span class="term">microarchitecture</span> &mdash; the implementation of
		the machine's ISA. This is where we see variations across CPUs. For example,
		the x86's original implementation was the 8086, but there have been numerous
		implementations since: 80286, 80386, 80486, Pentium IV, Broadwell, then
		Skylake (not including various optimizations).
	</p>
</section>

<section id="moores_law">
	<h2>Moore's Law</h2>
	<p>
		Most of us commonly associate the Intel with processors, but the corporation
		primarily competed in the computer memory market long before its forray into
		the processor space. At the time, however, computer memory wasn't the most
		lucrative market. Intel's cofounder, Gordon Moore, recognized this fact and
		in the early 1960s set the corporation's course towards its modern place in
		the processor industry.
	</p>
	<p>
		At the helm, Moore grew fascinated with the cost of processor manufacturing.
		What was the most cost-effective price for building these chips? Unlike
		other commodities, chips were physically getting smaller and smaller. As
		Moore examined the rate at which the chips shrunk, he observed that every
		eighteen months or so, the optimal price would double. This is an
		exponential rate. Remarkably Moore's observation has held true to this day,
		a self-fulfilling prophecy of sorts. In fact, it's consistency has merited
		its current name,
		<b>Moore's Law</b> &mdash; the number of transistors in a dense integrated
		circuit (i.e., a chip) doubles about every 18 months.
	</p>
	<p>
		Now, why is this a big deal? Well, the more transistors we can fit into a
		single chip, the more features the chip can support. This means that if we
		have twice as many transistors in a single chip today than we did eighteen
		months ago, we can fit the same features we did eighteen months in half of
		the current chip's area. That leaves another half for more features.
	</p>
	<p>
		Fueling this incentive is <b>Dennard Scaling</b>, named after the electrical
		engineer and inventor Robert Dennard, which roughly provides that as
		transistors get smaller and smaller, they get faster. And if the chip
		manufacturer designs correctly, many chip features either improve or stay
		the same.
	</p>
	<p>
		We might be wondering, why don't manufacturers just make larger chip sizes
		<i>and</i> fit more transistors? Wouldn't that provide more features?
		Indeed, it would. In fact, that's precisely what high-end graphics chips
		tend to do. The catch, however, is the push from the processor market's
		dependencies. Chips are only lucrative in so far as they're used to power
		some device, just as car engines are only lucrative if they power a vehicle.
		Currently, consumers want smaller and thinner devices, which in turn pushes
		processor manufacturers to keep their chip sizes down.<sup></sup>
	</p>
	<div class="note">
		<p>
			This push contributes to the fact that larger chips with more transistors
			can easily become prohibitively expensive. Take a look at some of the
			higher-end graphics card prices &mdash; they're easily in the thousands.
		</p>
	</div>
	<p>
		True Moore's law, however, the costs for manufacturing modern chips has
		grown exponentially. Many modern chips easily hit the ${\$100}$ million mark
		in production costs. Much of that cost is spent on engineering labor rather
		than manufacturing &mdash; engineers and scientists typing away at computers
		and drawing on whiteboards. If we're ever amazed at how humans have
		managed to create all of the abstractions behind computers, it's helpful to
		recall that we've reached this point because of hordes of very, very smart
		people working tirelessly together. Just as the pyramids of Giza and the Taj
		Mahal are wonders of human achievement, a lot can be done by a massive labor
		force.
	</p>
</section>

<section id="hardware">
	<h2>The Hardware</h2>
	<p>
		If we drew a rough sketch of how a computer is organized, it would look
		something like this:
	</p>
	<figure>
		<img
			src="{% static 'images/hardware.svg' %}"
			alt="hardware"
			loading="lazy"
		/>
	</figure>
	<p>
		If this picture looks fairly complex, don't worry. One of our goals is to
		fill in the details. To get started, let's just go over a few key parts.
	</p>
	<p>
		When we study computer hardware, it's often helpful to think of the whole
		system as a busy metropolis, where different places rely on one another. The
		whole city is powered by electricity, and its inhabitants are the bits, tiny
		creatures that are either 1 or 0. The bits usually work in groups of 8,
		forming a byte. The city's ultimate goal is facilitating the movement of
		these bits, and more broadly, bytes.
	</p>
	<p>
		<span class="topic">Buses.</span> In the diagram above, there are blue lines
		connecting each of the hardware pieces. These are called
		<span class="term">buses</span>. Despite their name, buses are better
		thought of as lanes, or roads, where bytes travel from one location in
		hardware to the next. The buses are typically designed to transfer one group
		of bytes at a time. That group is called a <span class="term">word</span>.
		On most systems, a word consists of either 4 bytes (32 bits) or 8 bytes (64
		bits).
	</p>
</section>
{% endblock %}
