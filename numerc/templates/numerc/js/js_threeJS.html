{% extends '../layout.html' %} {% load static %} {% block description %}
<meta name="description" content="Notes on ThreeJS" />
{% endblock %} {% block title %}
<title>ThreeJS</title>
{% endblock %} {% block content %}
<h1>ThreeJS</h1>
<section id="threejs">
	<p>
		<u>T</u>hreeJS is a JavaScript library that enables developers to create 3D
		visualizations and experiences for the web. We can think of it as an
		abstraction of <b>WebGL</b> (<q>Web Graphics Library</q>).<label
			for="three"
			class="margin-toggle"
			><sup></sup
		></label>
		<input type="checkbox" id="three" class="margin-toggle sidenote-number" />
		<span class="marginnote"
			>ThreeJS can also work with SVG and CSS, but because it's geared primarily
			at abstracting WebGL, SVG and CSS functionalities remain limited.</span
		>
		WebGL itself is a JavaScript API that renders triangles. Yes &mdash;
		impressive 3D graphics in websites are done via triangles. These triangle
		renderings are displayed in some HTML element (most commonly, a
		<var>canvas</var>). 3D effects result from the fact that these triangles are
		rendered at breathtaking speeds. These speeds are possible because WebGL
		uses the host machine's GPU (Graphic Processing Unit). The advantage to
		using the GPU is because of parallel computation. As we know, JavaScript is
		a single-threaded language. And as a single-threaded language, the vast
		majority of computations &mdash; done by the CPU &mdash; can only be done
		one a time. Those computations are fast, but with graphics, visualizations
		are better rendered with parallel computations &mdash; we want to render a
		gargantuan amount of triangles all at once. For JavaScript, this is only
		feasible through the GPU.
	</p>
	<aside>
		<figure>
			<img
				src="{% static 'images/triangles_cube.svg' %}"
				alt="Triangles form a cube"
				loading="lazy"
				class="sixty-p"
			/>
			<figcaption>Triangles forming a cube</figcaption>
		</figure>
	</aside>
	<p>
		The idea behind rendering triangles is that we position three vertices, and
		state that there are edges connecting them. These instructions are written
		in <b>shaders</b>, a topic we will discuss in later sections, and are
		forwarded to the GPU. The GPU then processes these instructions and displays
		particular colors to the individual pixels bounded by the vertices and
		edges. This is a remarkably fast process. By manipulating the positions of
		various vertices all at once &mdash; which in turn changes the colors of the
		individual pixels all at once &mdash; we can create the effect of a 3D
		dimensional object. The entities that perform these manipulations are
		specified functions. A <i>camera</i>, for example, is one such function. By
		manipulating the vertices in a particular way, we create the effect of
		seeing the 3D object from different angles, when in reality, we're just
		changing the position values for many vertices all at once.
	</p>
	<p>
		A fair question we might have is why not just use WebGL directly then?
		Indeed, we can. Many projects in
		<i>haute graphique</i> &mdash; very high-end graphics work &mdash; are
		written directly in WebGL for better control. And with better control, the
		programmer can optimize for performance as needed. The tradeoff, however, is
		that WebGL is painfully difficult to write. Drawing a single triangle on an
		HTML <var>canvas</var> element would take, at a minimum, a hundred lines of
		code.
	</p>
</section>

<section id="scene">
	<h2>The Scene</h2>
	<p>
		To install ThreeJS, see the
		<a
			href="https://threejs.org/docs/#manual/en/introduction/Installation"
			target="_blank"
			>ThreeJS installation page</a
		>. It's recommended to use ThreeJS via Webpack. With the ThreeJS library
		available, we have access to a variable called
		<span class="monoText"><mark>THREE</mark></span
		>. ThreeJS is structured with classes, and most of these classes are
		accessible via the variable <var>THREE</var>.
	</p>
	<p>
		To actually display ThreeJS renderings, we must have some container that
		houses all of the renderings. This is called a
		<b>scene</b>. More generally, to display renderings, we need, at a minimum,
		the following:
	</p>
	<figure>
		<ol class="checklist">
			<li>A scene</li>
			<li>An object</li>
			<li>A camera</li>
			<li>A renderer</li>
		</ol>
	</figure>
	<p>
		The scene is akin to a movie set. It's where all of the objects, models,
		cameras, lights, and more are placed. After we place everything we need in
		that scene, we ask ThreeJS to <i>render</i> the scene. To create a scene, we
		write the following:
	</p>
	<pre class="language-javascript"><code>
		const scene = new THREE.Scene();
	</code></pre>
	<p>
		Writing the above, we've instantiated a scene. We can then place an
		<b>object</b> in that scene. An object is any geometric object, or more
		generally, whatever it is what we want to render. It might be a cube, a
		sphere, a model for a building, person, video game character, etc. For now,
		we'll start with a basic cube.
	</p>
	<p>
		An object is really an abstract term for a <b>mesh</b>. A mesh is a
		combination of two things: (1) a <b>geometry</b>, and (2) a <b>material</b>.
		We can think of a geometry as defining the object's boundaries in space
		(i.e., it's shape), and the material as defining how the object looks (i.e.,
		it's colors). More accurately, the geometry is an abstraction for writing
		the instruction establishing the vertices discussed earlier. ThreeJS
		provides numerous
		<a
			href="https://threejs.org/docs/#api/en/geometries/BoxGeometry"
			target="_blank"
			>geometries</a
		>
		and
		<a
			href="https://threejs.org/docs/#api/en/materials/LineBasicMaterial"
			target="_blank"
			>materials</a
		>. For our humble cube, we will use the ThreeJS properties
		<var><mark>BoxGeometry</mark></var> and
		<span class="monoText"><mark>MeshBasicMaterial</mark></span
		>:
	</p>
	<pre class="language-javascript"><code>
		const scene = new THREE.Scene();
		const geometry = new THREE.BoxGeometry(1, 1, 1);
		const material = new THREE.MeshBasicMaterial({ color: 'red' });
	</code></pre>
	<p>
		The values <var>(1, 1, 1)</var> can be interpreted as any unit &mdash;
		pixels, kilometers, meters, etc. It's entirely up to us how we interpret
		those units. However, once we establish an interpretation, it's imperative
		that we use that interpretation consistently. This is because
		&#8220;units&#8221; in ThreeJS come down to proportions. If we interpret the
		cube as having a length, width, and height of 1 meter, respectively, then we
		must use that interpretation when we place other objects in relation to the
		cube.
	</p>
	<p>
		For the material, notice that we passed an object. In later sections, we'll
		see that materials can easily increase in complexity. And as we know, we
		want to manage increased complexity with various data structures. In this
		case, we use an object. For
		<var><mark>color</mark></var> property, we used a named color. We can also
		use hexadecimal, RGB, and HSL values. Hexadecimal is the more common color
		format, and we can write hexadecimal values directly:
	</p>
	<pre class="language-javascript"><code>
		const scene = new THREE.Scene();
		const geometry = new THREE.BoxGeometry(1, 1, 1);
		const material = new THREE.MeshBasicMaterial({ color: 0xff0000 });
	</code></pre>
	<p>
		With the geometry and material initialized, we can now initialize the mesh:
	</p>
	<pre class="language-javascript"><code>
		const scene = new THREE.Scene();
		const geometry = new THREE.BoxGeometry(1, 1, 1);
		const material = new THREE.MeshBasicMaterial({ color: 0xff0000 });
		const mesh = new THREE.Mesh(geometry, material);
	</code></pre>
	<p>
		We now have a mesh. To see this mesh, however, we must add it to the scene:
	</p>
	<pre class="language-javascript"><code>
		const scene = new THREE.Scene();
		const geometry = new THREE.BoxGeometry(1, 1, 1);
		const material = new THREE.MeshBasicMaterial({ color: 0xff0000 });
		const mesh = new THREE.Mesh(geometry, material);
		scene.add(mesh);
	</code></pre>
	<p>
		We now need a camera. Without a camera, we have no way of seeing what's in
		the scene. ThreeJS provides
		<a
			href="https://threejs.org/docs/#api/en/cameras/ArrayCamera"
			target="_blank"
			>many kinds of cameras</a
		>. For now, we will use a camera called
		<span class="monoText"><mark>PerspectiveCamera</mark></span
		>:
	</p>
	<pre class="language-javascript"><code>
		const scene = new THREE.Scene();
		const geometry = new THREE.BoxGeometry(1, 1, 1);
		const material = new THREE.MeshBasicMaterial({ color: 0xff0000 });
		const mesh = new THREE.Mesh(geometry, material);
		scene.add(mesh);
		const camera = new THREE.PerspectiveCamera()
	</code></pre>
	<p>
		Above, we initialized a new perspective camera called
		<var>camera</var>. The perspective camera is designed to mimic the way the
		human eye sees. To get the camera to work, we must provide the constructor a
		few parameters:
	</p>
	<figure>
		<div>
			<ul class="syntax">
				<li>
					new THREE.PerspectiveCamera(${fov}$, ${aspect}$, ${near}$, ${far}$)
				</li>
			</ul>
		</div>
	</figure>
	<p>
		We cover these parameters one at a time. First, the ${fov}$ parameter
		(&#8220;Field-of-view&#8221;) sets the vertical vision angle &mdash; how
		much we can see up and down. This might seem odd, as the intuitive response
		is that our field of view is often measured in terms of the horizontal
		visual angle (i.e., our
		<i>peripheral vision</i>). The game development community (often at the
		forefront of computer graphics), however, has found that both development
		and user experience is much more intuitive when working with vertical visual
		angles. <br /><br />
		With a larger field of view, we see everything everywhere, but the things we
		see appear distorted. With a smaller field of view, we see things as if
		we're peering through a scope. There are less things visible, but the things
		that are visible are proportionate and close.
	</p>
	<p>With the above information, we write:</p>
	<pre class="language-javascript"><code>
		const scene = new THREE.Scene();
		const geometry = new THREE.BoxGeometry(1, 1, 1);
		const material = new THREE.MeshBasicMaterial({ color: 0xff0000 });
		const mesh = new THREE.Mesh(geometry, material);
		scene.add(mesh);
		const camera = new THREE.PerspectiveCamera(75)
	</code></pre>
	<p>
		The second parameter, ${asp}$, establishes the
		<i>aspect ratio</i>. This is simply the width of the renderer divided by its
		height. We will see the renderer shortly, but for now, we can think of the
		width and height as the dimensions of the renderer. It's considered best
		practice to initialize the dimension values in an object, then refer to the
		values via object notation:
	</p>
	<pre class="language-javascript"><code>
		const scene = new THREE.Scene();
		const geometry = new THREE.BoxGeometry(1, 1, 1);
		const material = new THREE.MeshBasicMaterial({ color: 0xff0000 });
		const mesh = new THREE.Mesh(geometry, material);
		scene.add(mesh);
		const sizes = {
			width: 800;
			height: 800;
		}
		const camera = new THREE.PerspectiveCamera(
			75, 
			sizes.width / sizes.height
		)
	</code></pre>
	<p>
		We will examine the other parameters in later sections. But with just the
		two parameters above, we now add the camera to the scene:
	</p>
	<pre class="language-javascript"><code>
		const scene = new THREE.Scene();
		const geometry = new THREE.BoxGeometry(1, 1, 1);
		const material = new THREE.MeshBasicMaterial({ color: 0xff0000 });
		const mesh = new THREE.Mesh(geometry, material);
		scene.add(mesh);
		const sizes = {
			width: 800;
			height: 800;
		}
		const camera = new THREE.PerspectiveCamera(
			75, 
			sizes.width / sizes.height
		)
		scene.add(camera);
	</code></pre>
	<p>
		The final step is to <i>render</i> the scene. This is what sets off the
		process for drawing everything we've written. The function that performs the
		render is called a <b>renderer</b>. When the renderer is called, the scene
		is rendered from the camera's point of view, and the result is returned.
		That result is then drawn into the HTML element selected to display the
		result. In our case, we will use a <var>canvas</var> element. We can either
		write the <var>canvas</var> element directly in our HTML document, or have
		ThreeJS do it for us. To simplify things, we'll write the
		<var>canvas</var> element directly:
	</p>
	<pre class="language-html"><code>
		&lt;!DOCTYPE html&gt;
		&lt;html lang=&quot;en&quot;&gt;
		&lt;head&gt;
		&#x9;&lt;meta charset=&quot;UTF-8&quot;&gt;
		&#x9;&lt;title&gt;ThreeJS Lab&lt;/title&gt;
		&lt;/head&gt;
		&lt;body&gt;
		<span class="redText">&#x9;&lt;canvas class=&quot;webgl&quot;&gt;&lt;/canvas&gt;</span>
			&lt;script src=&quot;./three.min.js&quot;&gt;&lt;/script&gt;
			&lt;script src=&quot;./script.js&quot;&gt;&lt;/script&gt;
		&lt;/body&gt;
		&lt;/html&gt;
	</code></pre>
	<p>
		Ignore the class attribute <var>"webgl"</var> for now. We will use it in a
		later section. The main point is that there's a <var>canvas</var> element
		inside the HTML document we're working with. The HTML
		<var>canvas</var> element itself is just an element that allows us to draw
		objects in via JavaScript. By calling the renderer, ThreeJS will use WebGL
		to draw the render inside the <var>canvas</var>.
	</p>
	<p>
		That said, before we add the renderer, we must first create it. Like
		<var>material</var> a renderer can easily grow complex. Accordingly, we use
		an object containing several properties. One of those properties is the
		<var>canvas</var> element. Thus, we must target the
		<var>canvas</var> element and store it as a property:
	</p>
	<pre class="language-javascript"><code>
		const scene = new THREE.Scene();
		const geometry = new THREE.BoxGeometry(1, 1, 1);
		const material = new THREE.MeshBasicMaterial({ color: 0xff0000 });
		const mesh = new THREE.Mesh(geometry, material);
		scene.add(mesh);
		const sizes = {
			width: 800;
			height: 800;
		}
		const camera = new THREE.PerspectiveCamera(
			75, 
			sizes.width / sizes.height
		)
		scene.add(camera);
		const canvas = document.querySelector('.webgl');
		const renderer = new THREE.WebGLRenderer({
			canvas: canvas;
		});
	</code></pre>
	<p>
		If we refresh the HTML page, we'd see black rectangle, no cube in sight. To
		see our cube, we must first give the renderer a size:
	</p>
	<pre class="language-javascript"><code>
		const scene = new THREE.Scene();
		const geometry = new THREE.BoxGeometry(1, 1, 1);
		const material = new THREE.MeshBasicMaterial({ color: 0xff0000 });
		const mesh = new THREE.Mesh(geometry, material);
		scene.add(mesh);
		const sizes = {
			width: 800;
			height: 800;
		}
		const camera = new THREE.PerspectiveCamera(
			75, 
			sizes.width / sizes.height
		)
		scene.add(camera);
		const canvas = document.querySelector('.webgl');
		const renderer = new THREE.WebGLRenderer({
			canvas: canvas;
		});
		renderer.setSize(sizes.width, sizes.height);
	</code></pre>
	<p>
		When we resize the renderer, we resize the canvas. This is why we see the
		size of the black rectangle increase. Next, we must call the
		<var>render()</var> function:
	</p>
	<pre class="language-javascript"><code>
		const scene = new THREE.Scene();
		const geometry = new THREE.BoxGeometry(1, 1, 1);
		const material = new THREE.MeshBasicMaterial({ color: 0xff0000 });
		const mesh = new THREE.Mesh(geometry, material);
		scene.add(mesh);
		const sizes = {
			width: 800;
			height: 800;
		}
		const camera = new THREE.PerspectiveCamera(
			75, 
			sizes.width / sizes.height
		)
		scene.add(camera);
		const canvas = document.querySelector('.webgl');
		const renderer = new THREE.WebGLRenderer({
			canvas: canvas;
		});
		renderer.setSize(sizes.width, sizes.height);
		renderer.render(scene, camera);
	</code></pre>
	<p>
		Even with the code above, we still don't see a cube. Why? Because the camera
		is actually <em>inside</em> the cube. We have to move the camera away from
		the cube. To move the camera &mdash; and really, to move any object &mdash;
		we use the following properties (where ${obj}$ is an object):
	</p>
	<ul class="ruled">
		<li>
			<var><mark>${obj}$.position</mark></var>
		</li>
		<ul>
			<li>
				The <var>position</var> property is an object literal with three
				properties: <span class="monoText"><mark>x</mark></span
				>, <span class="monoText"><mark>y</mark></span
				>, and <var><mark>z</mark>.</var> Each of these properties represents
				the ${x}$-, ${y}$-, and ${z}$-axis values respectively.
			</li>
			<li>
				Note that axis interpretation varies widely across libraries. In
				ThreeJS, the ${x}$-axis is interpreted as left-right movement, the
				${y}$-axis as up-down movement, and the ${z}$-axis is forward-backward
				movement.
			</li>
		</ul>
		<li>
			<var><mark>${obj}$.rotation</mark></var>
		</li>
		<ul>
			<li>
				The <var><mark>rotation</mark></var> property represents an object's
				local rotation, in <em>radians</em>.
			</li>
		</ul>
		<li>
			<var><mark>${obj}$.scale</mark></var>
		</li>
		<ul>
			<li>
				The <var><mark>scale</mark></var> property represents an object's local
				scale.
			</li>
		</ul>
	</ul>
	<p>
		Knowing these properties, we want to move the camera backwards (i.e.,
		towards us):
	</p>
	<pre class="language-javascript"><code>
		const scene = new THREE.Scene();
		const geometry = new THREE.BoxGeometry(1, 1, 1);
		const material = new THREE.MeshBasicMaterial({ color: 0xff0000 });
		const mesh = new THREE.Mesh(geometry, material);
		scene.add(mesh);
		const sizes = {
			width: 800;
			height: 800;
		}
		const camera = new THREE.PerspectiveCamera(
			75, 
			sizes.width / sizes.height
		)
		camera.position.z = 3; // move the camera
		scene.add(camera);
		const canvas = document.querySelector('.webgl');
		const renderer = new THREE.WebGLRenderer({
			canvas: canvas;
		});
		renderer.setSize(sizes.width, sizes.height);
		renderer.render(scene, camera);
	</code></pre>
	<p>
		We should now see a red square. It is actually a cube, we just need to move
		the camera to the right:
	</p>
	<pre class="language-javascript"><code>
		const scene = new THREE.Scene();
		const geometry = new THREE.BoxGeometry(1, 1, 1);
		const material = new THREE.MeshBasicMaterial({ color: 0xff0000 });
		const mesh = new THREE.Mesh(geometry, material);
		scene.add(mesh);
		const sizes = {
			width: 800;
			height: 800;
		}
		const camera = new THREE.PerspectiveCamera(
			75, 
			sizes.width / sizes.height
		)
		camera.position.z = 3; // move the camera back
		camera.position.x = 2; // move the camera right
		scene.add(camera);
		const canvas = document.querySelector('.webgl');
		const renderer = new THREE.WebGLRenderer({
			canvas: canvas;
		});
		renderer.setSize(sizes.width, sizes.height);
		renderer.render(scene, camera);
	</code></pre>
</section>

<section id="transforming_objects">
	<h2>Transforming Objects</h2>
	<p>
		To <b>transform</b> an object is to modify some property of the object.
		Transformation is the first step towards <i>animating</i> the object. In
		ThreeJS, the four properties most relevant to transformation are:
	</p>
	<figure>
		<ul>
			<li>
				<var><mark>position</mark></var>
			</li>
			<li>
				<var><mark>scale</mark></var>
			</li>
			<li>
				<var><mark>rotation</mark></var>
			</li>
			<li>
				<var><mark>quaternion</mark></var>
			</li>
		</ul>
	</figure>
	<p>
		All of these properties originate in a class called
		<var><mark>Object3D</mark>.</var> From the <var>Object3D</var> class we have
		classes like <var>PerspectiveCamera</var> and <var>Mesh</var>.
	</p>
	<section id="position">
		<h3>The Position Property</h3>
		<p>
			As stated earlier, the <var>position</var> property originates in the
			class <var>Object3D</var>. Thus, any object whose ancestor class is
			<var>Object3D</var> has a <var>position</var> property. The
			<var>position</var> property itself is a combination of three properties:
		</p>
		<ul class="ruled">
			<li>
				<var>${Obj}$.x</var>
			</li>
			<ul>
				<li>Sets the ${x}$-axis coordinate.</li>
			</ul>
			<li>
				<var>${Obj}$.y</var>
			</li>
			<ul>
				<li>Sets the ${y}$-axis coordinate.</li>
			</ul>
			<li>
				<var>${Obj}$.z</var>
			</li>
			<ul>
				<li>Sets the ${z}$-axis coordinate.</li>
			</ul>
		</ul>
		<p>For example, using the same code from the previous section:</p>
		<pre class="language-javascript"><code>
			mesh.position.y = 1; // move the mesh up by 1
			mesh.position.y = -1; // move the mesh down by 1

			mesh.position.x = 1; // move the mesh right by 1
			mesh.position.x = -1; // move the mesh down by 1

			mesh.position.z = 1; // move the mesh forward by 1
			mesh.position.z = -1; // move the mesh back by 1
		</code></pre>
		<p>We can also update all three positions in a single line:</p>
		<pre class="language-javascript"><code>
			/*
			 * Move the mesh right, up, forward by 1.
			*/
			mesh.position.set(1, 1, 1); 
		</code></pre>
		<p>When using the single-line approach, the syntax is:</p>
		<figure>
			<ul class="syntax">
				<li>${Obj}$.position.set(${x}$, ${y}$, ${z}$)</li>
			</ul>
		</figure>
		<p>
			Although the comments to the code above are true, they are true insofar as
			we are using a perspective camera. Interpreting the axes entirely depends
			on the camera &mdash; which establishes
			<i>our</i> position when we view the object. If we think about this
			carefully, this makes perfect sense. The word &#8220;right&#8221; and
			&#8220;left&#8221; are entirely subjective; hence the existence of
			questions like &#8220;My left or your left?&#8221; The same goes for
			&#8220;up&#8221; and &#8220;down.&#8221; If van Helsing stands on the
			floor while Dracula suspends from the ceiling, Helsing's
			&#8220;down&#8221; is Dracula's &#8220;up,&#8221; and Dracula's
			&#8220;up&#8221; is Helsing's &#8220;down.&#8221; The words
			&#8220;forward&#8221; and &#8220;backward&#8221; are not immune either. If
			Don Quixote stands in front of a car and the car reverses towards Panza,
			the car is moving &#8220;backward&#8221; from Don Quixote and
			&#8220;forward&#8221; towards Panza.
		</p>
		<p>
			Equally arbitrary is the number we used above,
			<var>1.</var> The distance of ${1}$ unit is entirely determined by our
			interpretation. We can think of it was 1 nanometer, 1 meter, 1 mile, 1
			lightyear, and so on. As stated in the earlier section, distances in
			ThreeJS are measured in terms of proportions. Thus, once we've settled on
			an interpretation, we must keep that interpretation in mind when we use
			numbers for other objects.
		</p>
		<p>
			The <var>position</var> property also inherits from a class called
			<span class="monoText"><mark>Vector3</mark></span
			>. The <var>Vector3</var> class contains numerous methods we will use
			extensively. One such method is
			<span class="monoText"><mark>.length()</mark></span
			>:
		</p>
		<pre class="language-javascript"><code>
			mesh.position.x = 0.7;
			mesh.position.y = -0.6;
			mesh.position.z = 1;
			console.log(mesh.position.length());
		</code></pre>
		<pre class="language-bash"><code>
			1.3601470508735443
		</code></pre>
		<p>
			The output above is effectively how far our mesh is from the scene's
			center. We can also obtain the distance between the object and the camera
			(note that there must be a camera before we asking for the distance to the
			camera):
		</p>
		<pre class="language-javascript"><code>
			mesh.position.x = 0.7;
			mesh.position.y = -0.6;
			mesh.position.z = 1;

			// after the camera initialization

			console.log(mesh.position.distanceTo(camera.position));
		</code></pre>
		<pre class="language-bash"><code>
			2.202271554554524
		</code></pre>
		<p>
			One particularly useful method is
			<span class="monoText"><mark>.normalize()</mark></span
			>. This method will take the length of a particular vector, and reduce it
			to a length of <var>1</var>:
		</p>
		<pre class="language-javascript"><code>
			mesh.position.x = 0.7;
			mesh.position.y = -0.6;
			mesh.position.z = 1;

			mesh.position.normalize();
		</code></pre>
		<section id="position_helper">
			<h4>Axes Helper</h4>
			<p>
				Because axes can get confusing when working with many different cameras,
				ThreeJS provides a tool called
				<span class="monoText"><mark>AxesHelper</mark></span
				>. This is just a visualization of the axes we're working with. To
				create it, we write the code below. Note that because the axes helper is
				an object, it must be added to the scene:
			</p>
			<pre class="language-javascript"><code>
				const axesHelper = new THREE.AxesHelper();
				scene.add(axesHelper);
			</code></pre>
			<p>
				With the axes helper, the axes are color-coded, each with a length of
				<em>1 unit</em>:
			</p>
			<figure>
				<ul>
					<li>
						The
						<span class="redText">red</span> line corresponds to the
						<span class="redText">${\text{$x$-axis}}$</span>
					</li>
					<li>
						The
						<span class="greenText">green</span> line corresponds to the
						<span class="greenText">${\text{$y$-axis}}$</span>
					</li>
					<li>
						The
						<span class="blueText">blue</span> line corresponds to the
						<span class="blueText">${\text{$z$-axis}}$</span>
					</li>
				</ul>
			</figure>
			<p>
				With the perspective camera set to default, we likely do not see the
				${\text{$z$-axis}}$ line. This is because the perspective camera, by
				default, points straight ahead. To see the ${\text{$z$-axis},}$ we must
				change the camera position:
			</p>
			<pre class="language-javascript"><code>
				const axesHelper = new THREE.AxesHelper();
				scene.add(axesHelper);

				// camera initialized

				camera.position.z = 3;
				camera.position.y = 1;
				camera.position.x = 1;
				scene.add(camera);
			</code></pre>
		</section>
	</section>

	<section id="scale">
		<h3>Scale</h3>
		<p>
			Like the position property, the
			<var><mark>scale</mark></var> property is also a <var>Vector3</var>. The
			<var>scale</var> property will change the scale (i.e., the unit
			proportions) of the particular object. For example, the default value of
			each axis is ${1.}$ We can change this default value by modifying the
			<var>scale</var> property:
		</p>
		<pre class="language-javascript"><code>
			mesh.scale.x = 2; // change the x-axis scale to 2
			mesh.scale.y = 3; // change the y-axis scale to 3
			mesh.scale.z = 0.5; // change the z-axis scale to 0.5
		</code></pre>
	</section>

	<section id="rotating_objects">
		<h3>Rotating Objects</h3>
		<p>To rotate objects, we have two properties available for modification:</p>
		<figure>
			<ul>
				<li>
					<span class="monoText">
						<mark>rotation</mark>
					</span>
				</li>
				<li>
					<span class="monoText">
						<mark>quaternion</mark>
					</span>
				</li>
			</ul>
		</figure>
		<p>
			Modifying either of the properties above will achieve a rotation.
			Importantly, updating one will update the other. Let's first consider the
			<var>rotation</var> property.
		</p>
		<p>With <var>rotation</var>, there three properties:</p>
		<ul>
			<li>
				<span class="monoText"><mark>x</mark></span
				>, which sets the ${\text{$x$-axis}}$ angle in radians, the default
				being ${0.}$
			</li>
			<li>
				<span class="monoText"> <mark>y</mark></span
				>, which sets the ${\text{$y$-axis}}$ angle in radians, the default
				being ${0.}$
			</li>
			<li>
				<span class="monoText"><mark>z</mark></span
				>, which sets the ${\text{$z$-axis}}$ angle in radians, the default
				being ${0.}$
			</li>
		</ul>
		<p>
			Each of these properties inherits from a class called
			<span class="monoText"><mark>Euler</mark></span
			>. The idea behind <var>rotation</var> is that the object rotates about a
			stick piercing through the center of the object. Thus, when we use the
			<var>rotation</var> property, we must think about which axis we're
			rotating the object on (i.e., from which direction the stick pierces
			through the object).
		</p>
		<figure>
			<img
				src="{% static 'images/euler_rotation.svg' %}"
				alt="Euler rotation"
				loading="lazy"
				class="sixty-p"
			/>
			<figcaption>
				As always, bear in mind that the axes are entirely dependent on where we
				view the object (i.e., the camera's position).
			</figcaption>
		</figure>
		<p>Stating the rotations follows the usual dot notation:</p>
		<pre class="language-javascript"><code>
			mesh.rotation.x = 0.5 // rotate about the x-axis
			mesh.rotation.y = 0.5 // rotate about the y-axis
			mesh.rotation.z = 0.5 // rotate about the z-axis
		</code></pre>
		<p>
			Because the rotations are measured in radians, a rotation value of ${\pi}$
			results in rotation equivalent to ${{180}^\circ,}$ and a value of ${2
			\pi}$ results in a full revolution, ${{360}^\circ.}$
		</p>
		<pre class="language-javascript"><code>
			// rotate about the y-axis half a circle
			mesh.rotation.y = Math.pi

			// rotate about the z-axis a full circle
			mesh.rotation.z = 2 * MATH.pi 
		</code></pre>
		<section id="gimbal_lock">
			<h4>Gimbal Lock</h4>
			<p>
				We must take care when working with rotations. Whenever we rotate an
				axis, we might unintentionally rotate the other axis. This is because
				the <var>rotation</var> property always follows the order <var>x</var>,
				<var>y</var>, and <var>z</var>. Consider the illustration above. If we
				rotate the object about the ${\text{$z$-axis}}$ by ${\pi / 2,}$ the
				${\text{$y$-}}$ and ${\text{$z$-axes}}$ point in different directions:
			</p>
			<figure>
				<img
					src="{% static 'images/euler_rotation2.svg' %}"
					alt="Euler rotation"
					loading="lazy"
					class="sixty-p"
				/>
			</figure>
			<p>
				Moreover, because these axes are arbitrary (it depends on the camera's
				position), we can mistakenly rotate, say, the ${\text{$x$-axis}}$ when
				we actually intended to rotate the ${\text{$y$-axis}.}$ If we rotate an
				axis far enough, we can get into a situation called a
				<b>gimbal lock</b> &mdash; a phenomenon where it appears as if one of
				the axes is stuck, or doesn't work anymore.
			</p>
			<p>
				One way to avoid this problem is to change the default order with the
				<var><mark>.reorder()</mark></var> method:
			</p>
			<pre class="language-javascript"><code>
				// change the rotation order to y-, x-, z-axis
				mesh.rotation.reorder('yxz')
				// rotations
			</code></pre>
			<p>
				The last comment above is included to indicate that the reordering must
				occur before we rotate.
			</p>
			<p>
				As we can likely tell, it seems tedious to have to write these
				reorderings over and over again before rotations. Because of this
				nuisance, ThreeJS provides another property for rotation,
				<span class="monoText"><mark>quaternion</mark></span
				>. We will cover quaternions in a later section, but the idea behind the
				quaternion is that we can use some algebra to ensure rotations are
				consistent. Because the mathematics is fairly involved, we will devote
				an entire section for quaternions in a later discussion.
			</p>
		</section>
	</section>
	<section id="look_at">
		<h3>Look at a Particular Point</h3>
		<p>
			When working with transformations, it's often helpful to keep the camera
			pointed at a particular point. This way, no matter how we transform the
			object, the camera points at a constant location, allowing us some
			uniformity in evaluating transformations. To do so, we use
			<span class="monoText"><mark>lookAt()</mark></span
			>, a method available to all <var>Object3D</var> instances (e.g.,
			cameras):
		</p>
		<pre class="language-javascript"><code>
			// initialize camera
			camera.lookAt(new THREE.Vector(0, -1, 0));
		</code></pre>
		<p>
			By writing the code above, the camera will always point at the coordinate
			${(0, -1, 0).}$ This coordinate, however, may or may not be on the mesh.
			If we wanted to keep the camera pointed at the mesh, we can use the mesh's
			position property:
		</p>
		<pre class="language-javascript"><code>
			// initialize camera and mesh
			camera.lookAt(mesh.position);
		</code></pre>
		<p>
			With the above code, the camera now always points to the center of the
			mesh.
		</p>
	</section>
	<section id="grouping_objects">
		<h3>Grouping</h3>
		<p>
			From our discussion thus far, it should be obvious that transforming
			objects can lead to many lines of code. Because of this fact, it's best
			practice to <i>group</i> objects whenever possible. For example, 3D
			rendering of a house might consist of many component 3D renderings &mdash;
			the rendering for a door, a table, the walls, a roof, and so on. If the
			house is just one rendering in a bigger rendering of, say, a landscape, we
			do not want to have to move each of the component renderings one-by-one.
			Instead, we should gather all of these component renderings into a group,
			then manipulate the properties for that group. If we want to move the
			house to another position in the landscape, we manipulate the position
			property for the house, rather than the position property for the walls,
			roof, table, lamp, and so on.
		</p>
		<p>
			To make a group, we use the
			<var><mark>group()</mark></var> method:
		</p>
		<pre class="language-javascript"><code>
			// create two cubes
			const cube1 = new THREE.mesh(
				new THREE.BoxGeometry(1,1,1),
				new THREE.MeshBasicMaterial({color: 0xff0000})
			);
			const cube2 = new THREE.mesh(
				new THREE.BoxGeometry(1,1,1),
				new THREE.MeshBasicMaterial({color: 0x00ff00})
			);

			// create the group
			const group1 = new Three.Group();

			// add the cubes to the group
			group1.add(cube1, cube2);

			// add the group to the scene
			scene.add(group1);
		</code></pre>
		<p>
			Notice that we used slightly different syntax to instantiate the meshes.
			We can directly pass the properties into the constructor. Importantly, we
			must remember to always add our groups to the scene. Otherwise, we won't
			see anything displayed.
		</p>
		<p>
			With the two cubes in a group, we can transform them all together by
			transforming the group:
		</p>
		<pre class="language-javascript"><code>
			group.position.y = 1; // move the group by 1 on the y-axis
		</code></pre>
	</section>
</section>

<section id="introduction_to_animations">
	<h2>A Brief Look at Animations</h2>
	<p>
		Animating in ThreeJS &mdash; and JavaScript more generally &mdash; is
		analogous to stop motion animations. The idea is that we move points
		slightly, take a picture, move points again, take a picture, move points
		again, take a picture, and so on. The act of &#8220;take a picture&#8221; is
		rendering. Take many, many pictures and present them one at time very
		quickly &mdash; we create the illusion of movement.
	</p>

	<section id="frame_rates">
		<h3>Frame Rates</h3>
		<p>
			The speed at which the pictures are shown (or, in flipbookese, how fast we
			flip through book), is called a <b>frame rate</b>, and it's usually
			measured in the unit FPS (Frames Per Second). With higher frame rates,
			&#8220;motion&#8221; appears smooth or fluid (think high-end video games
			like <i>Diablo III</i>). With lower frame rates, motion appears choppy
			(think some silly GIF).
		</p>
		<div class="mainIdea">
			<p>
				Frame rate: The frequency at which consecutive images are captured or
				displayed.
			</p>
		</div>
		<p>
			Higher frame rates, however, do not imply better viewing experience. In
			fact, the topic of an &#8220;ideal framerate&#8221; is subject to fiery
			debate. Many of the movies and TV shows we see run at ${24 \text{ FPS}.}$
			For cinematography, productions using frame rates beyond ${24 \text{
			FPS}}$ tend to suffer from a phenomenon called the
			<i>soap opera effect</i> (take a look at clip from a product like
			<i>Days of our Lives</i> &mdash; it has a distinctive &#8220;look.&#8221;)
			Video games, at a minimum, generally run on ${60 \text{ FPS}.}$
		</p>
		<p>
			Furthermore, a viewing experience is also impacted by
			<b>frame time</b> &mdash; how long a frame stays in view. We can have a
			frame rate of ${60 \text{ FPS},}$ but if frame times are inconsistent
			(e.g., one frame lasts for ${2 \text{ mS}}$ and another ${5 \text{ mS},}$)
			we see phenomenon called <b>stuttering</b> &mdash; small
			&#8220;pauses&#8221; or &#8220;stills&#8221; in the motion.
		</p>
		<p>
			The issue with frame rates and animations is that computers have different
			frame rates. Many computers run on ${60 \text{ FPS,}}$ but more powerful
			computers can run on ${120 \text{ FPS}.}$ Because of these variations, if
			we animated a spinning propeller, the propeller might appear to spin much,
			much faster on machines with higher frame rates. This is not what we want.
			Our animations must look the same, regardless of the machine's frame rate.
		</p>
		<p>
			Before we solve this problem, let's talk about how to animate to begin
			with. The JavaScript API provides a method that allows us to take the
			pictures we need:
		</p>
		<figure>
			<ul class="syntax">
				<li>window.requestAnimationFrame(${f_c}$)</li>
			</ul>
			<figcaption>where ${f_c}$ is a <i>callback function</i>.</figcaption>
		</figure>
		<p>
			The procedure for animating is to pass object transformations into the
			<var>requestAnimationFrame()</var> method, perform the renderings therein,
			then call the <var>requestAnimationFrame()</var> in the next call. This is
			a little tricky to think about, but we can clarify by thinking of the
			method's purpose: The purpose of <var>requestAnimationFrame()</var> is to
			call a function provided in the <em>next</em> frame. To perform
			animations, we must always keep this fact in the back of our minds. The
			method <var>requestAnimationFrame()</var>, despite its name, is
			<em>not</em> a function that performs animations; it's a function that
			calls some function in the next frame just once.
		</p>
		<p>
			For example, suppose we've instantiated all of our meshes and cameras, and
			added them to the scene. To animate our mesh, we first need the callback
			function:
		</p>
		<pre class="language-javascript"><code>
			const tick = () => {
				window.requestAnimationFrame(tick);
			};
			tick();
		</code></pre>
		<p>
			By writing the function above, we are telling JavaScript to call the
			<var>tick</var> function on the next frame, then on that frame, call the
			<var>tick</var> function on the next frame, and on the next frame, and so
			on.
		</p>
		<p>
			Because the <var>tick()</var> function is constantly being called at
			different times, we can pass our transformations and renderings into the
			<var>tick()</var> function. Using some ThreeJS methods that update the
			transformations and renderings, we effectively modify object properties
			many, many times:
		</p>
		<pre class="language-javascript"><code>
			const tick = () => {
				// update object properties
				mesh.position.x += 0.01;

				// render
				renderer.render(scene, camera);

				// call the tick function for the next frame
				window.requestAnimationFrame(tick);
			};
			tick();
		</code></pre>
		<p>
			The code above will display the mesh moving along the ${x}$-axis. If we
			instead wrote:
		</p>
		<pre class="language-javascript"><code>
			const tick = () => {
				// update object properties
				mesh.rotation.y += 0.01;

				// render
				renderer.render(scene, camera);

				// call the tick function for the next frame
				window.requestAnimationFrame(tick);
			};
			tick();
		</code></pre>
		<p>
			We would see our mesh rotating about the ${y}$-axis. At this point, we now
			run into our previous problem. On a machine with a higher frame rate, the
			mesh would move or rotate much faster than a machine with a lower frame
			rate. This is because the number of calls to the
			<var>tick()</var> equals the number of frames per second. More frames per
			second &mdash; more calls to <var>tick()</var> &mdash; more updates to the
			properties.
		</p>
		<p>
			There are many ways to solve this problem. A common solution is to
			<i>adapt to the framerate</i>. The idea behind this solution is to use a
			<b>timestamp</b> of the current time, or date. Because the timestamp will
			be the same for all users, regardless of frame rate, we can use the
			timestamp as a common starting point for all viewers. With the timestamp,
			we compare the current timestamp against the previous timestamp. Because
			both timestamps are constant, the difference between the two timestamps is
			constant. We'll call this difference <var>deltaTime</var>. Because
			<var>deltaTime</var> is constant, we can then use it to establish
			uniformity across frame rates.
		</p>
		<p>
			To initialize a timestamp, we use the native method
			<var><mark>Date.now()</mark>:</var>
		</p>
		<pre class="language-javascript"><code>
			let previousTime = Date.now();
			const tick = () => {
				// current time for frame
				const currentTime = Date.now();
				
				// elapsed time
				const deltaTime = currentTime - previousTime;

				// update the previous time for the next tick to use
				previousTime = currentTime;

				// update object properties
				mesh.rotation.y += 0.01;

				// render
				renderer.render(scene, camera);

				// call the tick function for the next frame
				window.requestAnimationFrame(tick);
			};
			tick();
		</code></pre>
		<p>
			Because the <var>deltaTime</var> value is constant, multiplying that value
			with the values for property updates creates uniformity across frame
			rates:
		</p>
		<pre class="language-javascript"><code>
			let previousTime = Date.now();
			const tick = () => {
				// current time for frame
				const currentTime = Date.now();
				
				// elapsed time
				const deltaTime = currentTime - previousTime;

				// update the previous time for the next tick to use
				previousTime = currentTime;

				// update object properties
				mesh.rotation.y += 0.01 * deltaTime;

				// render
				renderer.render(scene, camera);

				// call the tick function for the next frame
				window.requestAnimationFrame(tick);
			};
			tick();
		</code></pre>
		<p>
			Another fix is to use ThreeJS's built-in solution,
			<span class="monoText"><mark>Clock()</mark></span
			>. The <var>Clock</var> object provides a method called
			<var>getElapsedTime()</var>, which essentially computes the
			<var>deltaTime</var> we saw earlier:
		</p>
		<pre class="language-javascript"><code>
			const clock = new THREE.Clock();
			const tick = () => {
				// elapsed time
				const deltaTime = clock.getElapsedTime();

				// update object properties
				mesh.rotation.y = deltaTime;

				// render
				renderer.render(scene, camera);

				// call the tick function for the next frame
				window.requestAnimationFrame(tick);
			};
			tick();
		</code></pre>
		<p>
			Importantly, the <var>getElapsedTime()</var> method returns the elapsed
			time in seconds. Because this is a number value that changes, we can
			assign this number directly to <var>mesh.rotation.y</var>. If we want to
			perform 1 revolution per second:
		</p>
		<pre class="language-javascript"><code>
			const clock = new THREE.Clock();
			const tick = () => {
				// elapsed time
				const deltaTime = clock.getElapsedTime();

				// update object properties
				mesh.rotation.y = deltaTime * Math.PI * 2;

				// render
				renderer.render(scene, camera);

				// call the tick function for the next frame
				window.requestAnimationFrame(tick);
			};
			tick();
		</code></pre>
		<p>
			If we want animations that cycle through various movements, we rely on
			trigonometry. We know that the functions ${y = \sin x,}$ ${y = \cos x,}$
			${y = \tan x,}$ and their inverses are all cyclical functions.
			Accordingly, we can use this fact to perform repetitive animations, like a
			mesh oscillating up and down:
		</p>
		<pre class="language-javascript"><code>
			const clock = new THREE.Clock();
			const tick = () => {
				// elapsed time
				const deltaTime = clock.getElapsedTime();

				// update object properties
				mesh.rotation.y = Math.sin(elapsedTime);

				// render
				renderer.render(scene, camera);

				// call the tick function for the next frame
				window.requestAnimationFrame(tick);
			};
			tick();
		</code></pre>
	</section>
</section>

<section id="cameras">
	<h2>Cameras</h2>
	<p>In this section, we discuss cameras in further detail.</p>
	<section id="camera">
		<h3>Perspective Camera</h3>
		<p>
			The perspective camera renders a scene with perspective. For example, here
			is a camera initialization:
		</p>
		<pre class="language-javascript"><code>
			const sizes = {
				width: 800,
				height: 600
			}
			const camera = new THREE.PerspectiveCamera(100, sizes.width / sizes.height, 1, 3);
			camera.position.x = 2;
			camera.position.y = 2;
			camera.position.z = 2;
		</code></pre>
		<p>
			Click on the underlined parts of the template below for an explanation of
			each argument:
		</p>
		<figure>
			<ul class="syntax">
				<li>
					let cam = new THREE.PerspectiveCamera(<span class="pop">${fov}$</span
					>, <span class="pop">${asp}$</span>,
					<span class="pop">${near}$</span>, <span class="pop">${far}$</span>)
				</li>
				<div class="popText">
					<p>
						This is the
						<span class="boldText">Field of View (FOV)</span> parameter. It's
						measured in degrees, and essentially returns the vertical vision
						angle &mdash; how much we can see up and down.
					</p>
				</div>
				<div class="popText">
					<p>
						This is <span class="boldText">aspect</span> argument. By default,
						it is set to 1. In the code above, we used an object. We can think
						of this as setting how much area the camera covers (i.e., the canvas
						size).
					</p>
				</div>
				<div class="popText">
					<p>
						This is the <span class="boldText">near</span> argument, effectively
						how &#8220;close&#8221; an object is to the camera. Any object
						closer than this value will not be seen.
					</p>
				</div>
				<div class="popText">
					<p>
						This is the <span class="boldText">far</span> argument, how
						&#8220;far&#8221; the object is to the camera. Any object farther
						than this value will not be seen.
					</p>
				</div>
			</ul>
		</figure>
		<p>
			<span class="topic">FOV Parameter.</span>
			The larger the FOV argument, the more likely we are to distort the
			objects. Beyond ${{75}^\circ,}$ objects begin looking very small. Often,
			it's best to stay within the range of ${{45}^\circ}$ through
			${{75}^\circ.}$
		</p>
		<p>
			<span class="topic">Aspect Ratio Parameter.</span>
			The aspect ratio argument is a ratio of width:height. We can think of this
			as setting the width and the height of the render.
		</p>
		<p>
			<span class="topic">Near & Far Parameters.</span> The near and far
			parameters correspond to how close and how far the camera can see. If an
			object is (1) closer than near, or (2) further than far, then the object
			will not be seen.
		</p>
		<p>
			A common mistake with this parameter is setting
			<var>near = 0.0001</var> and <var>far = 99999</var>. Although this largely
			keeps objects within view, it almost always introduces a bug called
			<b>z-fighting</b>. When two objects are too close to one another along the
			${z}$-axis, the GPU has difficulty determining which of the two objects is
			&#8220;seen.&#8221; This leads to the objects coming into conflict.
		</p>
	</section>

	<section id="orthographic_camera">
		<h3>Orthographic Camera</h3>
		<p>
			Unlike the perspective camera, the orthographic camera has no perspective.
			Thus, objects seen through an orthographic camera have the same size,
			regardless of their distance to the camera.
		</p>
		<p>The template for initializing an orthographic camera:</p>
		<figure>
			<ul class="syntax">
				<li>
					const c = new THREE.OrthographicCamera(<span class="pop"
						>${left}$</span
					>, <span class="pop">${right}$</span>,
					<span class="pop">${top}$</span>, <span class="pop">${bottom}$</span>,
					<span class="pop">${near}$</span>, <span class="pop">${far}$</span>);
				</li>
				<div class="popText">
					<p>The ${left}$ parameter indicates how far left the camera sees.</p>
				</div>
				<div class="popText">
					<p>
						The ${right}$ parameter indicates how far right the camera sees.
					</p>
				</div>
				<div class="popText">
					<p>The ${top}$ parameter indicates how far up the camera sees.</p>
				</div>
				<div class="popText">
					<p>
						The ${bottom}$ parameter indicates how far down the camera sees.
					</p>
				</div>
				<div class="popText">
					<p>
						This is the <span class="boldText">near</span> argument, effectively
						how &#8220;close&#8221; an object is to the camera. Any object
						closer than this value will not be seen.
					</p>
				</div>
				<div class="popText">
					<p>
						This is the <span class="boldText">far</span> argument, how
						&#8220;far&#8221; the object is to the camera. Any object farther
						than this value will not be seen.
					</p>
				</div>
			</ul>
		</figure>
		<p>
			With the orthographic camera, we want to think of the camera's field of
			view as a cube. What it sees directly in front of it is a square, and the
			object lies on that square. Compare that with the perspective camera,
			which has a field of view akin to a cylinder. Because of this, the
			orthographic camera must know the how far left, right, top, and bottom we
			render. Anything beyond those points is not seen. Furthermore, just like
			the perspective camera, we can include arguments for the parameters
			${near}$ and ${far.}$
		</p>
		<p>
			When using an orthographic camera, it's often the case that changes to
			near and far can distort the shape. To prevent this from occurring, we
			want to use the aspect ratio &mdash; the ratio of width:height. We then
			multiply this ratio to the ${left}$ and ${right}$ arguments:
		</p>
		<pre class="language-javascript"><code>
			const sizes = {
				width: 400,
				height: 400
			}
			const aspectRatio = sizes.width / sizes.height;
			const camera = new THREE.OrthographicCamera(
				-1 * aspectRatio, 
				1 * aspectRatio, 
				1, 
				-1, 
				0.1, 
				100
			);
		</code></pre>
	</section>

	<section id="custom_camera_controls">
		<h3>Controlling the Camera</h3>
		<p>
			Because ThreeJS deals with both 3D objects and their movements, cameras
			must be mobile. Think of a movie set. The camera operators move the
			cameras here and there as needed. There isn't just one camera, or multiple
			cameras, remaining in place.
		</p>
		<p>
			One way to move the camera is through the cursor. For example, if the
			cursor moves to the left, we want the camera to move left, if the cursor
			moves right, the camera moves right, and so on.
		</p>
		<p>
			To move the camera, we must know the coordinates of the cursor. If we run
			the code below, we'll see that we get back the ${x}$-coordinates of the
			cursor's position in real time:
		</p>
		<pre class="language-javascript"><code>
			window.addEventListener('mousemove', (event) => { 
				console.log(event.clientX);
			});
		</code></pre>
		<p>And if we use the following function:</p>
		<pre class="language-javascript"><code>
			window.addEventListener('mousemove', (event) => { 
				console.log(event.clientY);
			});
		</code></pre>
		<p>
			We get the ${y}$-coordinates of the cursor's position. The values we see
			in the console are pixel values. This is not very useful. If the window
			gets large enough, the values run into the thousands, and if the window
			gets small enough (e.g., on a mobile device), we stay in the hundred
			range. A much more useful metric would be one that keeps these values
			proportional. For example, one metric would be using the values ${0}$
			through ${1.}$ No matter what the window size is, we stay within this
			range.
		</p>
		<p>To begin, we'll store the values we need in an object:</p>
		<pre class="language-javascript"><code>
			const cursor = {
				x: 0,
				y: 0
			};
			window.addEventListener('mousemove', (event) => { 
					cursor.x = event.clientX;
					cursor.y = event.clientY;
			});
		</code></pre>
		<p>
			To keep these values within the range of ${0}$ to ${1,}$ we simply divide
			the values by the size of our viewport:
		</p>
		<pre class="language-javascript"><code>
			const sizes = {
				width: 400,
				height: 400
			};
			const cursor = {
				x: 0,
				y: 0
			};
			window.addEventListener('mousemove', (event) => { 
					cursor.x = event.clientX / sizes.width;
					cursor.y = event.clientY / sizes.height;
			});
		</code></pre>
		<p>
			Note that the <var>width</var> and <var>height</var> values above are not
			necessarily the viewport's dimensions. They might be the dimensions for a
			<var>canvas</var> element. In later sections, we will consider ways to
			obtain the actual viewport's dimensions.
		</p>
		<p>
			With the approach above, we're getting only positive values. What if we
			wanted to have positive and negative values, similar to a Cartesian
			coordinate system? The solution is to just subtract ${0.5:}$
		</p>
		<pre class="language-javascript"><code>
			const sizes = {
				width: 400,
				height: 400
			};
			const cursor = {
				x: 0,
				y: 0
			};
			window.addEventListener('mousemove', (event) => { 
					cursor.x = event.clientX / sizes.width - 0.5;
					cursor.y = event.clientY / sizes.height - 0.5;
			});
		</code></pre>
		<p>
			Why ${0.5?}$ Because the values range from ${0}$ to ${1.}$ By subtracting
			${0.5,}$ the range now becomes ${[-0.5, 0.5].}$ The difference between
			this implementation and the Cartesian coordinate system is the moving the
			mouse towards the top yields negative ${y}$-values, and moving the mouse
			towards the bottom yields positive ${y}$-values. The ${x}$-axis works as
			expected &mdash; moving towards the left, we get negative ${x}$-values,
			moving towards the right, we get positive ${x}$-values.
		</p>
		<p>
			Having these values, we can update the camera through the
			<var>tick()</var> function:
		</p>
		<pre class="language-javascript"><code>
			<span class="greyText">const tick = () => {</span>

				camera.position.x = cursor.x * 2;
				camera.position.y = cursor.y * 2;

				<span class="greyText">renderer.render(scene, camera);</span>

				<span class="greyText">window.requestAnimationFrame(tick);</span>
			<span class="greyText">}</span>
			<span class="greyText">tick();</span>
		</code></pre>
		<p>
			Running all of the relevant code, we might notice that the camera acts
			odd. If we move the cursor left, the camera moves right. If we move the
			cursor right, the camera moves left. And if we move up and down, the
			object appears to follow the camera. This is because of that earlier fact
			we noted: In ThreeJS, the ${y}$-axis is negative towards the top, and
			positive towards the bottom. This means we must invert the
			${y-}$coordinate value:
		</p>
		<pre class="language-javascript"><code>
			<span class="greyText">const sizes = {
				width: 400,
				height: 400
			};
			const cursor = {
				x: 0,
				y: 0
			};
			window.addEventListener('mousemove', (event) => { 
					cursor.x = event.clientX / sizes.width - 0.5;</span>
					cursor.y = -(event.clientY / sizes.height - 0.5);
			<span class="greyText">});</span>
		</code></pre>
		<p>
			If we want to make sure the camera is always looking at the center of the
			cube (while still panning around):
		</p>
		<pre class="language-javascript"><code>
			<span class="greyText">const tick = () => {

				camera.position.x = cursor.x * 2;
				camera.position.y = cursor.y * 2;</span>
				camera.lookAt(new THREE.Vector3());

				<span class="greyText">renderer.render(scene, camera);</span>

				<span class="greyText">window.requestAnimationFrame(tick);</span>
			<span class="greyText">}</span>
			<span class="greyText">tick();</span>
		</code></pre>
		<p>
			Alternatively, we can simply pass the object's position as an argument to
			<var>lookAt()</var>:
		</p>
		<pre class="language-javascript"><code>
			<span class="greyText">const tick = () => {

				camera.position.x = cursor.x * 2;
				camera.position.y = cursor.y * 2;</span>
				camera.lookAt(mesh.position);

				<span class="greyText">renderer.render(scene, camera);</span>

				<span class="greyText">window.requestAnimationFrame(tick);</span>
			<span class="greyText">}</span>
			<span class="greyText">tick();</span>
		</code></pre>
		<p>
			We still, however, have a limitation: We can't see behind the object. To
			remedy this, we want to have the camera rotate around the object. And to
			rotate the camera around the object, we're going to use trigonometry.
		</p>
		<p>
			As we know, the trigonometric functions are cyclical. For both ${y = \sin
			x}$ and ${y = \cos x,}$ the domain is ${\R,}$ and we can see that it is
			cyclical by way of a table:
		</p>
		<figure>
			<div class="func">
				<ul>
					<li>${x}$</li>
					<li>${y}$</li>
				</ul>
				<ul>
					<li>0</li>
					<li>0</li>
				</ul>
				<ul>
					<li>${\dfrac{\pi}{2}}$</li>
					<li>1</li>
				</ul>
				<ul>
					<li>${\pi}$</li>
					<li>0</li>
				</ul>
				<ul>
					<li>${\dfrac{3\pi}{2}}$</li>
					<li>-1</li>
				</ul>
				<ul>
					<li>${2 \pi}$</li>
					<li>0</li>
				</ul>
				<ul>
					<li>${\dfrac{5 \pi}{2}}$</li>
					<li>1</li>
				</ul>
				<ul>
					<li>${3 \pi}$</li>
					<li>0</li>
				</ul>
				<ul>
					<li>${\dfrac{7 \pi}{2}}$</li>
					<li>-1</li>
				</ul>
				<ul>
					<li>${4 \pi}$</li>
					<li>0</li>
				</ul>
			</div>
			<figcaption>The ${x}$ and ${y}$ values of ${y = \sin x.}$</figcaption>
		</figure>
		<p>And for ${y = \cos x}$ we have:</p>
		<figure>
			<div class="func">
				<ul>
					<li>${x}$</li>
					<li>${y}$</li>
				</ul>
				<ul>
					<li>0</li>
					<li>1</li>
				</ul>
				<ul>
					<li>${\dfrac{\pi}{2}}$</li>
					<li>0</li>
				</ul>
				<ul>
					<li>${\pi}$</li>
					<li>-1</li>
				</ul>
				<ul>
					<li>${\dfrac{3\pi}{2}}$</li>
					<li>0</li>
				</ul>
				<ul>
					<li>${2 \pi}$</li>
					<li>1</li>
				</ul>
				<ul>
					<li>${\dfrac{5 \pi}{2}}$</li>
					<li>0</li>
				</ul>
				<ul>
					<li>${3 \pi}$</li>
					<li>-1</li>
				</ul>
				<ul>
					<li>${\dfrac{7 \pi}{2}}$</li>
					<li>0</li>
				</ul>
				<ul>
					<li>${4 \pi}$</li>
					<li>1</li>
				</ul>
			</div>
			<figcaption>The ${x}$ and ${y}$ values of ${y = \cos x.}$</figcaption>
		</figure>
		<p>
			Because of these oscillations, if we use ${\cos x}$ to represent the
			${x}$-coordinate and ${\sin x}$ to represent the ${y}$-coordinate, we
			essentially have coordinates all along a circle. Knowing these facts, the
			question then, is, which axes do we move along? Because we aren't moving
			up and down (we're just trying to see behind the object), the axes we will
			travel are the ${x}$- and ${z}$-axis.
		</p>
		<pre class="language-javascript"><code>
			<span class="greyText">const tick = () => {</span>

				camera.position.x = Math.sin(cursor.x * Math.PI * 2) * 3;
				camera.position.y = cursor.y * 5;
				camera.position.z = Math.cos(cursor.x * Math.PI * 2) * 3;
				camera.lookAt(new THREE.Vector3());

				<span class="greyText">renderer.render(scene, camera);</span>

				<span class="greyText">window.requestAnimationFrame(tick);</span>
			<span class="greyText">}</span>
			<span class="greyText">tick();</span>
		</code></pre>
	</section>

	<section id="built_in_controls">
		<h3>Built-in Controls</h3>
		<p>
			The control we mentioned above is an example of a custom control. ThreeJS
			also provides <b>built-in controls</b>. These built-in controls are:
		</p>
		<figure>
			<ul>
				<li><var>DeviceOrientationControls</var></li>
				<li><var>FirstPersonControls</var></li>
				<li><var>FlyControls</var></li>
				<li><var>OrbitControls</var></li>
				<li><var>PointerLockControls</var></li>
				<li><var>TrackBallControls</var></li>
			</ul>
		</figure>
		<p>We examine each of these controls in turn.</p>

		<section id="device_orientation_control">
			<h4>Device Orientation Controls</h4>
			<p>
				<var>DeviceOrientationControls</var>
				retrieve the device orientation as set on the device, operating system,
				or browser. With this information, we can rotate the camera based on
				orientation.
			</p>
		</section>

		<section id="fly_controls">
			<h4>Fly Controls</h4>
			<p>
				We can think of <var>FlyControls</var> as allowing us to move the camera
				as if we were operating a fighter jet &mdash; we can rotate on all 3
				axes (e.g., performing a barrel roll), as well as moving forward and
				back.
			</p>
		</section>

		<section id="first_person_controls">
			<h4>First Person Controls</h4>
			<p>
				<var>FirstPersonControls</var> are similar to fly controls, but with
				restriction: The <i>up</i> axis is fixed. Where fly controls allow us to
				do &#8220;aerial tricks,&#8221; first person controls are more like a
				Boeing-747 &mdash; it can look down, but there is an upper bound on how
				much it can look up.
			</p>
		</section>

		<section id="pointer_lock_control">
			<h4>Pointer Lock Controls</h4>
			<p>
				<var>PointerLockControls</var> are akin to the controls we see in a game
				like Minecraft. We can look up, down, left, right, from a
				<i>fixed position</i>. Think of this as operating the camera on the the
				Mars rover. We can see in all directions, but only from a
				&#8220;locked&#8221; position.
			</p>
		</section>

		<section id="orbit_control">
			<h4>Orbit Controls</h4>
			<p>
				<var>OrbitControls</var> allow us to move in all directions,
				<em>except</em> below the floor (a vertical angle limit). This is very
				much like operating a drone or satellite. The camera
				&#8220;orbits&#8221; the object.
			</p>
		</section>

		<section id="trackball_controls">
			<p>
				<var>TrackballControls</var> are similar to orbit controls, but do not
				have the vertical angle limit we see with orbit controls.
			</p>
		</section>
	</section>
</section>

<section id="full_screen_and_resizing">
	<h2>Full-screen & Resizing</h2>
	<p>
		As we know, ThreeJS renders its visualizations in a canvas element. We can
		think of the canvas element as the universe where ThreeJS renderings exist.
		Depending on our desires, we often need to shrink, expand, or more generally
		resize the universe. Perhaps we want only want the canvas element to cover a
		portion of the viewport. Or we might want just the opposite, the canvas
		element covering the entire viewport. This all comes down to full-screening
		and resizing.
	</p>
	<div class="mainIdea">
		<p>
			Some important distinctions: The
			<i>viewport</i> is the area within the browser that displays webpage
			content. The <i>window</i> is the entire browser's area (the viewport plus
			the header bar with the URL). The <i>screen</i> is the entire monitor's
			display area.
		</p>
	</div>
	<p>
		Let's say we want our visualization to cover the entire viewport. We can do
		so by obtaining the viewport's height and width:
	</p>
	<pre class="language-javascript"><code>
		const sizes = {
			width: window.innerWidth,
			height: window.innerHeight
		}
	</code></pre>
	<p>
		Note that these properties are found in the
		<var>window</var> object, but they are width and height of the viewport,
		note the window.
	</p>
	<p>
		When we use these sizes for the renderer, the renderer now covers the entire
		viewport. There may, however, be some margins. The trick to fixing this is
		to add some class attribute to the HTML element
		<var>canvas</var> itself, then use the position property in CSS:
	</p>
	<pre class="language-css"><code>
		.webgl {
			position: fixed;
			top: 0;
			left: 0;
			outline: none;
		}
	</code></pre>
	<p>
		The <var>outline: none;</var> styling is used to get rid of any default
		outlines that the browser renders for the <var>canvas</var> element
		(depending on the browser, this might look like a thin blue line around the
		canvas element).
	</p>
	<section id="handle_resizing">
		<p>
			<span class="topic">Handle Resizing.</span> One problem we might have
			noticed with the above changes is that they aren't responsive. If we
			enlarge the window or narrow it, the canvas element does not change unless
			we reload the page.
		</p>
		<p>The fix is to use an event listener:</p>
		<pre class="language-javascript"><code>
			const sizes = {
				width: window.innerWidth,
				height: window.innerHeight
			}
			
			window.addEventListener('resize', () => {
				// update sizes 
				sizes.width = window.innerWidth;
				sizes.height = window.innerHeight;
			});
		</code></pre>
		<p>
			Above, we updated the sizes with the event listener, but this isn't
			enough. The sizes are used by different parts of our rendering, but it
			doesn't actually update the container for our scene. Thus, the next thing
			we must do is update the camera. For the camera, we must update the aspect
			ratio.
		</p>
		<pre class="language-javascript"><code>
			const sizes = {
				width: window.innerWidth,
				height: window.innerHeight
			}
			
			window.addEventListener('resize', () => {
				// update sizes 
				sizes.width = window.innerWidth;
				sizes.height = window.innerHeight;

				// update camera
				camera.aspect = sizes.width / sizes.height;
			});
		</code></pre>
		<p>
			This is still not enough. Even if we update the camera's aspect, ThreeJS
			doesn't know that aspect is being updated dynamically. To be precise,
			updating the camera's aspect property doesn't update the camera's
			<i>projection matrix</i>. Accordingly, we must call the built-in method
			<var>updateProjectionMatrix()</var>:
		</p>
		<pre class="language-javascript"><code>
			const = sizes = {
				width: window.innerWidth,
				height: window.innerHeight
			};
			window.addEventListener('resize', () => {
				// update sizes
				sizes.width = window.innerWidth;
				sizes.height = window.innerHeight;

				// update camera
				camera.aspect = sizes.width / sizes.height;
				camera.updateProjectMatrix();
			})
		</code></pre>
		<p>
			Updating the camera's projection matrix, we see that resizing the window
			causes some sort of resizing, but it's not quite what we want. Expanding
			the window seems to stretch the mesh, and narrowing the window appears to
			contract the mesh.
		</p>
		<p>
			We're seeing this behavior because we're only updating the camera, but not
			the renderer. Accordingly, we must update the renderer's size:
		</p>
		<pre class="language-javascript"><code>
			const = sizes = {
				width: window.innerWidth,
				height: window.innerHeight
			};
			window.addEventListener('resize', () => {
				// update sizes
				sizes.width = window.innerWidth;
				sizes.height = window.innerHeight;

				// update camera
				camera.aspect = sizes.width / sizes.height;
				camera.updateProjectMatrix();

				// update the renderer
				renderer.setSize(sizes.width, sizes.height)
			})
		</code></pre>
		<p>
			Performing this update, we now see our scene expanding and contracting
			responsively.
		</p>
	</section>

	<section id="pixel_ratios">
		<h3>Pixel Ratios</h3>
		<p>
			On some machines, the mesh rendering &mdash; whether through ThreeJS or
			some other graphics software &mdash; might appear blurry or jagged. When
			we see this behavior, the most likely culprit is a screen with a pixel
			ratio greater than ${1.}$
		</p>
		<p>
			The <b>pixel ratio</b> is a ratio of the number of physical pixels there
			are for one pixel unit in software.<label
				for="pixel"
				class="margin-toggle"
				><sup></sup
			></label>
			<input type="checkbox" id="pixel" class="margin-toggle sidenote-number" />
			<span class="marginnote"
				>The word &#8220;pixel&#8221; is a blending of &#8220;picture
				element.&#8221;</span
			>
			Inherent in this definition is the distinction between a
			<i>physical pixel</i> and the <i>logical pixel</i>. The logical pixel is a
			pixel unit, denoted ${\text{px}.}$ It is the software interpretation of a
			pixel. The physical pixel, however, is a single composition of molecules
			between two transparent electrons (i.e., the tiny squares we see if we
			look at an electronic screen with a magnifying glass).
		</p>
		<p>
			Keeping this distinction in mind, there are a few terms we should be clear
			about:
		</p>
		<ul>
			<li>
				<b>Resolution</b> is the number of physical pixels along the width and
				the number of physical pixels along the height. Thus, a resolution of
				${16 \times 12}$ denotes some collection of pixels ${16}$ pixels
				horizontally and ${12}$ pixels vertically. Resolution, more generally,
				is the product:
				<figure>
					<div>
						<p>${width \times height.}$</p>
					</div>
				</figure>
			</li>
			<li>
				<b>Aspect ratio</b> is the reduced form of a resolution. For example,
				with a resolution of of ${16 \times 12,}$ the greatest common factor is
				${4,}$ so the aspect ratio is ${4:3.}$ The aspect ratio, more generally,
				is the ratio:
				<figure>
					<div>
						<p>${width:height.}$</p>
					</div>
				</figure>
				In computer graphics and cinematography, when captured content has a
				wider aspect ratio than the display, we often see black bars at the top
				and bottom of the screen. This is called
				<b>letterboxing</b>. When the captured content has a taller aspect ratio
				than the display, black bars appear at the left and right sides of the
				screen. This is called <b>pillarboxing</b>.
			</li>
			<li>
				A <b>screen size</b> is the diagonal length of the screen, usually
				measured in inches or centimeters. For example, given a resolution of
				${16 \times 12,}$ the screen size can be computed as:
			</li>
			<li>
				<b>Pixels Per Inch (PPI)</b> is a measure of pixel density (i.e., how
				closely packed the physical pixels are), represented as the ratio
				${\dfrac{d}{s}}$ where ${d}$ is the diagonal number of pixels, and ${s}$
				is the screen size. For example, for the ${16 \times 12}$ device, the
				diagonal number of pixels is:
				<figure>
					<div>
						<p>${\text{screen size} = \sqrt{16^2 + 12^2} \approx 20.2731}$</p>
					</div>
				</figure>
				If the screen size is ${8.2 \text{ in.},}$ the device has a PPI of:
				<figure>
					<div>
						<p>${\dfrac{20.2731}{8.2} \approx 2.4723}$</p>
					</div>
				</figure>
				This essentially tells us that the device fits about ${2}$ pixels for
				every inch.
			</li>
			<li>
				The <b>device pixel ratio</b>, or <i>pixel ratio</i> for short, is the
				number of <i>physical pixels</i> for each <i>logical pixel</i>.
			</li>
		</ul>
		<p>
			For many years, all screens had a pixel ratio of ${1.}$ In that bygone
			era, one physical pixel corresponded to one logical pixel. However, some
			very clever hardware and software engineers at places like Apple began
			creating screens that supported a pixel ratio of two (in Apple's case, the
			so-branded <i>Retina&trade; displays</i>). With a pixel ratio of two,
			developers can render ${4}$ pixel units for each physical pixel.
			Eventually, other device manufacturers noticed this opportunity and began
			creating screens supporting even higher pixel ratios &mdash; three (giving
			${9}$ pixels units to render), four (${16}$ pixel units to render), five
			(${25}$ pixel units), and even six (${36}$ pixel units). To get our
			device's pixel ratio, we can use the
			<var><mark>window.devicePixelRatio</mark></var>
			property.
		</p>
		<p>
			Currently, higher pixel ratios are a double-edged sword. On the one hand,
			these higher pixel ratios lead to much more detailed images, and by
			extension, fluid and smooth animations. On the other hand, higher pixel
			ratios are enormously costly on the GPU, and the GPU industry has not
			progressed nearly as fast as its display counterpart. This is a
			significant problem when we consider the fact that we've now arrived at a
			situation where the highest pixel ratios are on the weakest machines
			&mdash; mobile phones. There is no technological reason for having such
			high pixel ratios on mobiles; their GPUs simply cannot exploit these
			increased numbers. In fact, to the naked eye, the mobile device with a
			pixel ratio of three is indistinguishable from devices with higher pixel
			ratios. How did it come to this? Frighteningly clever marketing teams.
		</p>
		<p>
			Because of the variations in pixel ratio, we must write code accounting
			for these differences:
		</p>
		<pre class="language-javascript"><code>
			renderer.setPixelRatio(window.devicePixelRatio);
		</code></pre>
		<p>
			Writing the code above ensures that our renderer conforms to the relevant
			machine's pixel ratio. Recall, however, that there are devices with
			outrageous pixel ratios like five. Because of the GPU cost, we must limit
			our rendering to a particular pixel ratio:
		</p>
		<pre class="language-javascript"><code>
			renderer.setPixelRatio(Math.min(window.devicePixelRatio, 2));
		</code></pre>
		<p>
			The code above reads: If the pixel ratio is less than two, set that as the
			pixel ratio, otherwise, use two. Of note, it's considered best-practice to
			place the pixel ratio setter inside the resize function we used earlier:
		</p>
		<pre class="language-javascript"><code>
			const sizes = {
				width: window.innerWidth,
				height: window.innerHeight
			}
		
			window.addEventListener('resize', () => {
				// update sizes 
				sizes.width = window.innerWidth;
				sizes.height = window.innerHeight;
		
				// update camera
				camera.aspect = sizes.width / sizes.height;
				camera.updateProjectionMatrix();
		
				// update the renderer
				renderer.setSize(sizes.width, sizes.height)
				renderer.setPixelRatio(Math.min(window.devicePixelRatio, 2));
			});
		</code></pre>
		<p>
			We do so because more users today connect smaller devices to larger
			displays (e.g., linking a laptop to a monitor via HDMI). By placing the
			pixel ratio setter inside our resizing function, our rendering can
			self-modify when viewed on a larger resolution device.
		</p>
	</section>
	<section id="full_screen_toggle">
		<h3>Full-screen</h3>
		<p>
			Full-screen visualizations allow us to create more immersive experiences.
			The first step is to implement a way to toggle full-screen. In the
			illustrations below, we'll use a double-click approach, but there are many
			other ways: a button, a keyboard shortcut, a dropdown, etc.
		</p>
		<p>
			Four double-click, we'll use an event listener with a double click event:
		</p>
		<figure>
			<ul class="syntax">
				<li>window.addEventListener('dblclick', ${f_c}$)</li>
			</ul>
			<figcaption>where ${f_c}$ is a callback function</figcaption>
		</figure>
		<p>
			The DOM provides a read-only property called
			<span class="monoText"><mark>${obj}$.fullscreenElement</mark></span
			>, which returns <var>true</var> if some element ${obj}$ is in full-screen
			mode, and <var>false</var> otherwise. We can use this as a test condition
			for toggling between the different states:
		</p>
		<pre class="language-javascript"><code>
			window.addEventListener('dblclick', () => { 
				if (!document.fullscreenElement) {
					console.log('go fullscreen');
				} else { 
					console.log('leave fullscreen');
				}
			})
		</code></pre>
		<p>Double clicking our viewport:</p>
		<pre class="language-javascript"><code>
			go fullscreen
		</code></pre>
		<p>
			It works as expected. Next, we use another DOM method,
			<var><mark>.requestFullscreen()</mark></var> to toggle full-screen mode.
			Because our scene is in a <var>canvas</var> element, we call this method
			on the <var>canvas</var> element. And to exit full-screen mode, we call
			the
			<var><mark>document.exitFullScreen()</mark></var>
			method:
		</p>
		<pre class="language-bash"><code>
			window.addEventListener('dblclick', () => { 
				if (!document.fullscreenElement) {
					canvas.requestFullscreen();
				} else { 
					document.exitFullscreen();
				}
			})
		</code></pre>
		<p>
			Now when the user double clicks on the
			<var>canvas</var> element (the scene), our visualization goes into
			full-screen mode, and on another double-click (or pressing the
			<var>esc</var> key), the user exits full-screen mode.
		</p>
	</section>
</section>

<section id="geometries">
	<h2>Geometries</h2>
	<p>
		Recall that a <b>geometry</b> is the shape, our outline of some object we
		want to render. In our previous examples, we used ThreeJS's
		<span class="monoText"><mark>BoxGeometry</mark></span
		>. We now examine other geometries.
	</p>
	<aside>
		<figure>
			<img
				src="{% static 'images/geometry_cylinder.svg' %}"
				alt=""
				loading="lazy"
				class="sixty-p"
			/>
			<figcaption>The geometry behind a cylinder</figcaption>
		</figure>
	</aside>
	<p>
		To begin, we consider a few additional details about geometries. A geometry
		is composed of <b>vertices</b> and <b>faces</b>. In 3D graphics, a vertex is
		a point coordinate in 3D space. A face is the surface that results from
		joining vertices together with triangles. We can also use geometries for
		particles. When used in this manner, we set aside the faces, and just use
		the vertices. But for now, we will focus on vertices and faces.
	</p>
	<p>
		A geometry can store more than just the vertex coordinates. Every vertex has
		a position, but we can also assign colors, number values, size,
		ThreeJS-specific properties, any kind of data. Before we create our own
		geometries, we should know ThreeJS's native geometries.
	</p>
	<p>
		All native geometries in ThreeJS inherit from the
		<var><mark>BufferGeometry</mark></var> class. Within the
		<var>BufferGeometry</var> class, there are many built-in methods and
		properties we can use. The documentation can be found
		<a
			href="https://threejs.org/docs/#api/en/core/BufferGeometry"
			target="blank"
			>here</a
		>. Some useful methods:
	</p>
	<ul class="ruled">
		<li>
			<span class="monoText"><mark>.rotateX</mark></span
			>, <span class="monoText"><mark>.rotateY</mark></span
			>, <var><mark>.rotateZ</mark></var>
		</li>
		<ul>
			<li>Rotate the geometry (not the mesh) along the relevant axis.</li>
			<li>These methods take a float argument, measured in radians.</li>
		</ul>
		<li>
			<var><mark>.translate(${x}$, ${y}$, ${z}$)</mark></var>
		</li>
		<ul>
			<li>
				Translate the geometry (not the mesh) along the axes, where ${x,}$
				${y,}$ and ${z}$ are floats.
			</li>
		</ul>
	</ul>
	<p>
		The native geometries in ThreeJS are implemented as classes inheriting from
		<var>BufferGeometry</var>. Below are some of the more commonly used
		geometries, but a full list can be found in
		<a
			href="https://threejs.org/docs/#api/en/geometries/BoxGeometry"
			target="_blank"
			>the documentation</a
		>:
	</p>
	<ul class="ruled">
		<li>
			<span class="monoText"
				>const ${c}$ = new THREE.BoxGeometry(${w}$, ${h}$, ${d}$)</span
			>
		</li>
		<ul>
			<li>
				Instantiates a cuboid with the width ${w,}$ height ${h,}$ and depth
				${d.}$
			</li>
		</ul>
		<li>
			<span class="monoText"
				>const ${c}$ = new THREE.PlaneGeometry(${w}$, ${h}$)</span
			>
		</li>
		<ul>
			<li>
				Instantiates a plane with width ${w}$ along the ${x}$-axis, and a height
				${h}$ along the ${y}$-axis.
			</li>
		</ul>
		<li>
			<span class="monoText"
				>const ${c}$ = new THREE.CircleGeometry(${r}$, ${s}$)</span
			>
		</li>
		<ul>
			<li>
				Instantiates a circle of radius ${r,}$ with segments ${s.}$ Because
				everything is drawn via triangles, the integer ${s}$ determines the
				number of triangles used to draw the triangle. A minimum of ${3}$ is
				needed, and the default is ${8.}$ The more triangles are used, the
				&#8220;smoother&#8221; the circle looks.
			</li>
		</ul>
		<li>
			<span class="monoText"
				>const ${c}$ = new THREE.SphereGeometry(${r}$, ${w_s}$, ${h_s}$)</span
			>
		</li>
		<ul>
			<li>
				Instantiates a sphere of radius ${r.}$ The integer argument ${w_s}$ sets
				the number of horizontal segments (default ${32,}$ minimum ${3,}$), and
				the integer argument ${h_s}$ sets the number of vertical segments
				(default ${16,}$ minimum ${2}$).
			</li>
		</ul>
		<li>
			<span class="monoText"
				>const ${c}$ = new THREE.ConeGeometry(${r}$, ${h}$, ${rs}$)</span
			>
		</li>
		<ul>
			<li>
				Instantiates a cone with a base of radius ${r,}$ and a height ${h.}$ The
				integer argument ${rs}$ sets the number of segmented faces around the
				cone's circumference. The default is ${8,}$ higher values render a
				smoother cone.
			</li>
		</ul>
		<li>
			<span class="monoText"
				>const ${c}$ = new THREE.CylinderGeometry(${r_t}$, ${r_b}$, ${h}$,
				${rs}$)</span
			>
		</li>
		<ul>
			<li>
				Instantiates a cylinder. Given that a cylinder has two circle bases, the
				float argument ${r_t}$ sets the radius for the top base, and the float
				argument ${r_b}$ sets the radius for the bottom. The float argument
				${h}$ sets the height, and the integer argument ${r_s}$ sets the number
				of segmented faces around the cylinder's circumference.
			</li>
		</ul>
		<li>
			<span class="monoText"
				>const ${c}$ = new THREE.RingGeometry(${r_i}$, ${r_o}$,
				${\theta_s}$)</span
			>
		</li>
		<ul>
			<li>
				Instantiates a disk with a hole through its center. The float argument
				${r_i}$ sets the inner radius, and ${r_o}$ the outer radius. The integer
				argument ${\theta_s}$ sets the number of segments; a greater integer
				will render a rounder disk.
			</li>
		</ul>
		<li>
			<span class="monoText"
				>const ${c}$ = new THREE.TextGeometry(${s}$, ${p}$)</span
			>
		</li>
		<ul>
			<li>
				Instantiates a text geometry. The text to render is the string argument
				${s,}$ and the parameters are passed via the object-literal ${p.}$
			</li>
		</ul>
	</ul>

	<p>
		From the small sampling of the API above, we can see that there are numerous
		native geometries available at our disposal. Before we consider creating our
		own geometries, we should always examine the documentation to determine
		whether an implementation already exists. For the most complex geometries,
		we will need to use 3D-modelling programs like Blender.
	</p>
	<section id="segments">
		<h3>Segments</h3>
		<p>
			In the previous API table, we saw some parameters related to
			<b>segments</b>. These parameters, at a higher level, impact the number of
			triangles used to draw the geometry's faces. To understand how these
			segments work, let's consider the cube. Recall that to instantiate a cube,
			we write:
		</p>
		<pre class="language-javascript"><code>
			const cube = new THREE.BoxGeometry(1, 1, 1);
		</code></pre>
		<p>
			The full parameter list for <var>BoxGeometry()</var>, however, is the
			following:
		</p>
		<figure>
			<ul class="syntax">
				<li>
					THREE.BoxGeometry(${w}$, ${h}$, ${d}$, ${s_w}$, ${s_h}$, ${s_d}$)
				</li>
			</ul>
		</figure>
		<p>
			The ${s}$ parameters, all integers, correspond to the number of segments.
			${s_w}$ maps to the number of width segments, ${s_h}$ the height segments,
			and ${s_d}$ the depth segments. By default, each of these parameters is
			assigned the value ${1.}$ Focusing on ${s_h,}$ if ${s_h = 1,}$ the cube
			looks like:
		</p>
		<figure>
			<img
				src="{% static 'images/segment_cube1.svg' %}"
				alt="segment of 1 cube"
				loading="lazy"
				class="twenty-p"
			/>
		</figure>
		<p>Notice that there are exactly ${1 \times 2 = 2}$ triangles.</p>
		<p>
			When ${s_h = 2,}$ we draw two segments from the midpoints, generating what
			we can think of as four squares. Each square then forms two triangles,
			yielding ${2 \times 4 = 8}$ triangles.:
		</p>
		<figure>
			<img
				src="{% static 'images/segment_cube2.svg' %}"
				alt="segment of 2 cube"
				loading="lazy"
				class="twenty-p"
			/>
		</figure>
		<p>
			At ${s_h = 3,}$ we have ${3 \times 3 = 9}$ squares, and ${3 \times 2 = 9}$
			triangles. At ${s_h = 4,}$ we have ${4 \times 4 = 16}$ squares and ${16
			\times 2 = 32}$ triangles. Generalizing this pattern:
		</p>
		<figure>
			<div>
				<p>${n = 2s^2}$</p>
			</div>
			<figcaption>
				where ${n}$ is the number of triangles, and ${s}$ is the number of
				segments.
			</figcaption>
		</figure>
		<p>
			Why do we need more triangles? Well, for a simple geometry like a cube,
			the more triangles does not produce any visible difference. However, for
			geometries with curvatures, the number of triangles determines how smooth
			the curvatures are:
		</p>
		<figure>
			<img
				src="{% static 'images/sphere.svg' %}"
				alt="Triangles forming a sphere"
				loading="lazy"
				class="sixty-p"
			/>
		</figure>
		<p>
			For all meshes, we can see the geometry's consisting triangles by
			including in the <var>material</var>'s parameters a
			<var><mark>wireframe</mark></var> property set to <var>true</var>:
		</p>
		<pre class="language-javascript"><code>
			const material = new THREE.MeshBasicMaterial({
				color: 0xff0000,
				wireframe: true
			})
		</code></pre>
	</section>
	<section id="custom_buffer_geometries">
		<h3>Custom Buffer Geometries</h3>
		<p>
			Suppose we wanted to create a triangle on our own, without relying on
			ThreeJS's built-in geometries. To do so, we must create our own buffer
			geometry.
		</p>
		<p>
			To create custom buffer geometries, we must use a
			<var><mark>Float32Array</mark></var> &mdash; a typed array of fixed length
			in the JavaScript language. Specifically, a <var>Float32Array</var> is an
			array that can only store <var>float</var> values.
		</p>
		<p>
			To initialize the <var>Float32Array</var>, we use the following syntax:
		</p>
		<figure>
			<ul class="syntax">
				<li>new Float32Array(${n}$)</li>
			</ul>
			<figcaption>
				where ${n}$ is an <var>int</var> value, corresponding to the length of
				the array.
			</figcaption>
		</figure>
		<p>For example:</p>
		<pre class="language-javascript"><code>
			const positionsArray = new Float32Array(9);
		</code></pre>
		<p>
			With the <var>Float32Array</var> instantiated, we can then assign values
			to the array's positions:
		</p>
		<pre class="language-javascript"><code>
			const positionsArray = new Float32Array(9);
			positionsArray[0] = 0;
			positionsArray[1] = 0;
			positionsArray[2] = 0;

			positionsArray[3] = 0;
			positionsArray[4] = 1;
			positionsArray[5] = 0;

			positionsArray[6] = 1;
			positionsArray[7] = 0;
			positionsArray[8] = 0;
		</code></pre>
		<p>
			Tying this to ThreeJS is the following: Every three positions is the
			coordinate to vertex, with the positions corresponding to the ${x-}$,
			${y}$-, and ${z}$-axis respectively. Given that a triangle consists of
			three vertices, we have the following interpretation:
		</p>
		<pre class="language-javascript"><code>
			const positionsArray = new Float32Array(9);

			// vertex 1
			positionsArray[0] = 0; // x-coordinate
			positionsArray[1] = 0; // y-coordinate
			positionsArray[2] = 0; // z-coordinate

			// vertex 2
			positionsArray[3] = 0;
			positionsArray[4] = 1;
			positionsArray[5] = 0;

			// vertex 3
			positionsArray[6] = 1;
			positionsArray[7] = 0;
			positionsArray[8] = 0;
		</code></pre>
		<p>
			Rather than writing all of the lines above individually, we can accomplish
			the same result in a single line:
		</p>
		<pre class="language-javascript"><code>
			const positionsArray = new Float32Array([
				0, 0, 0,
				0, 1, 0,
				0, 1, 0
			]);
		</code></pre>
		<p>
			Why aren't we just using regular JavaScript arrays? Because computers can
			handle the <var>Float32Array</var> much more easily than a regular array.
			With the standard array, there's a longer checklist the machine must go
			through before returning an output. <var>Float32Array</var> significantly
			shortens that checklist. The array consists entirely of floats, so type
			inference is quick; the array's length is fixed so no calculations are
			necessary; the array is immutable so even fewer calculations must be made.
		</p>
		<p>
			Once we've initialized our positions array, we must convert the
			<var>Float32Array</var> to a
			<var><mark>BufferAttribute</mark></var> object &mdash; essentially a
			container for attribute data (in this case, our positions).
		</p>
		<pre class="language-javascript"><code>
			const positionsArray = new Float32Array([
				0, 0, 0,
				0, 1, 0,
				1, 0, 0
			]);
			const positionsAttribute = new THREE.BufferAttribute(positionsArray, 3);
		</code></pre>
		<p>
			Notice how we passed the <var>positionsArray</var> into the constructor,
			followed by the integer <var>3</var>. This integer essentially tells the
			constructor that each of the floats in <var>positionsArray</var> are
			grouped by three (corresponding to the coordinates of each vertex).
		</p>
		<p>
			With our coordinates now stored in a
			<var>BufferAttribute</var>, we can now create a new geometry using the
			coordinates. We do so by passing appropriate label and the
			<var>positionsAttribute</var> into the geometry's
			<var>.setAttribute()</var> method:
		</p>
		<pre class="language-javascript"><code>
			const positionsArray = new Float32Array([
				0, 0, 0,
				0, 1, 0,
				1, 0, 0
			]);
			const positionsAttribute = new THREE.BufferAttribute(positionsArray, 3);
			const geometry = new THREE.BufferGeometry();
			geometry.setAttribute('position', positionsAttribute);
		</code></pre>
		<p>
			What exactly is the string argument
			<var>'position'</var>? This is the value that ThreeJS's
			<i>shaders</i> will use. We will cover shaders in a later section, but for
			now, think of the <var>'position'</var> argument as telling ThreeJS's
			shaders, &#8220;These are coordinates.&#8221;
		</p>
		<section id="multiple_custom_geometries">
			<h4>Multiple Custom Geometries</h4>
			<p>
				Above, we created a single triangle. How do we create multiple
				triangles? We use the same procedure from last. We'll start with a
				variable, <var>triangleCount</var>, for the number of our triangles we
				want:
			</p>
			<pre class="language-javascript"><code>
				const triangleCount = 50;
			</code></pre>
			<p>
				Then, we'll create a positions array. Each triangle must have three
				coordinates, and each coordinate must have have three positions, so we
				multiply <var>triangleCount</var> by 9:
			</p>
			<pre class="language-javascript"><code>
				const triangleCount = 50;
				const positionsArray = new Float32Array(triangleCount * 3 * 3);
			</code></pre>
			<p>
				Because we're just generating fifty triangles, we can fill the
				<var>positionsArray</var> with random values:
			</p>
			<pre class="language-javascript"><code>
				const triangleCount = 50;
				const positionsArray = new Float32Array(triangleCount * 3 * 3);
				for (let i = 0; i < (count * 3 * 3); i++) { 
						positionsArray[i] = Math.random();
				}
			</code></pre>
			<p>
				Then we store the <var>positionsArray</var> in
				<var>BufferAttribute</var> and add this attribute to a geometry:
			</p>
			<pre class="language-javascript"><code>
				const triangleCount = 50;
				const positionsArray = new Float32Array(triangleCount * 3 * 3);
				for (let i = 0; i < (triangleCount * 3 * 3); i++) { 
					positionsArray[i] = Math.random();
				}
				const positionsAttribute = new THREE.BufferAttribute(positionsArray, 3);
				const geometry = new THREE.BufferGeometry();
				geometry.setAttribute('position', positionsAttribute);
			</code></pre>
			<p></p>
		</section>
	</section>
</section>

<section id="debug_ui">
	<h2>Debug UI</h2>
	<p>
		In many ThreeJS projects, we can see a rectangular interface in the corner
		displaying performance-related metrics and other real-time data. This small
		interface is called a <b>debug UI</b>.
	</p>
	<p>
		Why do so many projects include this feature? Because it allows for quick
		debugging and development. Coding in computer graphics can be tedious, and
		it is inefficient to update values in a file and refreshing over and over
		again. With a debug UI, we can quickly see our modifications. As we work
		through a project, we should always pause and consider whether to add a
		particular control in the debug UI panel.
	</p>
	<p>
		To implement a debug UI, we have two options: (1) create our own, or (2) use
		a library. And with option (2), we again have options:
	</p>
	<ul>
		<li>
			<a href="https://github.com/dataarts/dat.gui" target="_blank">dat.GUI</a>
		</li>
		<li>
			<a href="https://lil-gui.georgealways.com/" target="_blank">lil-gui</a>
		</li>
		<li>
			<a href="https://github.com/freeman-lab/control-panel" target="_blank"
				>control-panel</a
			>
		</li>
		<li>
			<a href="https://github.com/automat/controlkit.js" target="_blank"
				>ControlKit</a
			>
		</li>
		<li><a href="https://github.com/lo-th/uil" target="_blank">Uil</a></li>
		<li>
			<a href="https://cocopon.github.io/tweakpane/" target="_blank"
				>TweakPlane</a
			>
		</li>
		<li><a href="https://github.com/colejd/guify" target="_blank">Guify</a></li>
		<li><a href="https://github.com/wearekuva/oui" target="_blank">Oui</a></li>
	</ul>
	<p>
		To use one of the libraries above: (1) Download the library and place it in
		the relevant folder and import it into the JavaScript file for the
		animations; or (2) use webpack to install the dependency and import. For
		example, if we decided to use lil-gui, we would import it into our
		JavaScript file as follows (followed by console log to ensure it imported
		successfully):
	</p>
	<pre class="language-javascript"><code>
		import * as lil from 'lil-gui'
		console.log(lil);
	</code></pre>
	<pre class="language-bash"><code>
		Module {...}
	</code></pre>
	<p>Once imported, we instantiate the GUI:</p>
	<pre class="language-javascript"><code>
		import * as lil from 'lil-gui'

		// Debugger
		const gui = new lil.GUI();
	</code></pre>
	<p>
		This will display an empty interface, called the
		<i>control panel</i>, on our canvas. Many libraries provide objects we can
		place in the control panel. These objects generally fall into the following
		categories:
	</p>
	<ul>
		<li>A <b>range</b> object for numbers with minimum and maximum values.</li>
		<li>A <b>color</b> for colors with various formats.</li>
		<li>A <b>text</b> for simple text.</li>
		<li>A <b>checkbox</b> for Boolean values.</li>
		<li>A <b>select</b> for a list of selections.</li>
		<li>A <b>button</b> to trigger functions.</li>
		<li>A <b>folder</b> to organize the panel.</li>
	</ul>
	<p>
		For the rest of this section, we'll use lil-gui's syntax. To add an element
		to the control panel, we must first decide what we want the element to
		manipulate. For example, let's say we want the element to manipulate the
		mesh's ${y}$-axis position. To add this control to the panel, we write the
		following <em>after</em> the mesh has been initialized and added to the
		scene:
	</p>

	<pre class="language-javascript"><code>
		// Debug
		gui.add(mesh.position, 'y');
	</code></pre>

	<div class="mainIdea">
		<p>
			By default, adding a debug UI will always display the control panel on our
			rendering. To hide the control panel by default, simply write
			<var><mark>${n}$.hide()</mark></var> after instantiating the debug UI
			object ${n.}$
		</p>
	</div>

	<p>
		Writing the code above, we see a new field labeled
		<var>y</var>, where we can enter numbers to manipulate the mesh's ${y}$-axis
		position. With lil-gui, we can also place additional parameters for the
		minimum and maximum ${y}$-values, as well as the step (allowing the user to
		control precision):
	</p>
	<pre class="language-javascript"><code>
		// Debug
		gui.add(mesh.position, 'y', -3, 3, 0.01);
	</code></pre>

	<p>
		In the code above, <var>-3</var> sets the minimum value, <var>3</var> sets
		the maximum value, and <var>0.01</var> sets the step. With these additional
		parameters, the ${y}$-axis control becomes a slider rather than a simple
		input field. Using the same syntax, we can add controls for the other axes:
	</p>
	<pre class="language-javascript"><code>
		// Debug
		gui.add(mesh.position, 'x', -3, 3, 0.01);
		gui.add(mesh.position, 'y', -3, 3, 0.01);
		gui.add(mesh.position, 'z', -3, 3, 0.01);
	</code></pre>
	<p>
		For greater readability, we can avoid the appearance of magic numbers with
		chaining:
	</p>
	<pre class="language-javascript"><code>
		gui.add(mesh.position, 'y').min(-3).max(3).step(0.01);
	</code></pre>
	<p>And for greater readability, chaining with line breaks:</p>
	<pre class="language-javascript"><code>
		gui
			.add(mesh.position, 'y')
			.min(-3)
			.max(3)
			.step(0.01);
	</code></pre>

	<section id="custom_labels">
		<h3>Custom Labels</h3>
		<p>
			By default, the control's name is the mesh's property name. To use a
			custon label, we use the
			<var><mark>.name()</mark></var> function:
		</p>
		<pre class="language-javascript"><code>
			gui
				.add(mesh.position, 'y')
				.min(-3)
				.max(3)
				.step(0.01)
				.name('elevation');
		</code></pre>
	</section>

	<section id="toggling_visibility">
		<h3>Toggling Visibility</h3>
		<p>
			All meshes have a property called
			<span class="monoText"><mark>.visible</mark></span
			>. This is a Boolean property &mdash; set it to <var>false</var>, and the
			mesh is no longer visible. Because we have access to this property, we can
			include a control for toggling a particular mesh's visibility.
		</p>
		<pre class="language-javascript"><code>
			gui
				.add(mesh, 'visible')
		</code></pre>
		<p>
			Writing the code above, we see a checkbox with the label
			<var>visible</var>. Clicking this checkbox toggles the mesh's visibility.
		</p>
		<p>
			This GUI feature extends to other objects with the
			<var>visible</var> property. For example, we can toggle wireframe
			visibility:
		</p>
		<pre class="language-javascript"><code>
			gui
				.add(material, 'wireframe');
		</code></pre>
	</section>

	<section id="colors">
		<h2>Colors</h2>
		<p>
			To add a control to change our mesh's color, we use the
			<var><mark>.addColor()</mark></var> method. A separate method is used
			because using <var>.add()</var> alone, the <var>gui</var> object has no
			way of differentiating between the string <var>'0xffff'</var> and the
			color <var>0xffff</var>.
		</p>
		<p>
			To use <var>.addColor()</var> we must use an object. This is because we
			cannot access the <var>.color</var> property for
			<var>material</var> through <var>.addColor()</var>.
		</p>
		<pre class="language-javascript"><code>
			const parameters = {
				color: 0xff0000
			};
			gui
				.addColor(parameters, 'color');
		</code></pre>
		<p>
			Writing the code above adds a color meter to the control panel. However
			changing the meter value doesn't render any changes. This is because the
			<var>parameters</var> object has nothing to do with the rest of the scene.
			We must provide code to update the material's color when the meter is
			changed. We can do so with the <var>.onChange()</var> method:
		</p>
		<pre class="language-javascript"><code>
			const parameters = {
				color: 0xff0000
			}
			gui
				.addColor(parameters, 'color')
				.onChange(() => { 
						material.color.set(parameters.color);
				});
		</code></pre>
		<p>
			In the code above, we used the lil-gui method
			<var>.onChange()</var>. This is essentially an event listener. We then
			pass an arrow function as a parameter. If the color meter changes, the
			arrow function invokes <var>material.color.set(parameters.color)</var>.
			The <var>.set()</var> method is the <var>Color</var> class's function for
			setting a color.
		</p>
		<p>
			As an aside, we should always place values hard values like colors and
			other numbers in an object, then use the object to assign values.
			Following this practice keeps code clean and modular.
		</p>
	</section>

	<section id="functions">
		<h2>Triggering Functions</h2>
		<p>
			With more complicated meshes, it's helpful to have a button for triggering
			particular functions. Like the hurdle with changing colors, we cannot pass
			non-object-literals as parameters to the lil-gui methods. Instead, we must
			include the function in an object, and pass the relevant object value into
			the lil-gui method.
		</p>
		<pre class="language-javascript"><code>
			const parameters = {
				color: 0xff0000,
				spin: () => { console.log('spin') }
			}
			gui
				.add(parameters, 'spin');
		</code></pre>
		<p>
			The control panel should now display a button. Clicking that button, we
			see <var>spin</var> in the console. Using <var>gsap</var>, we can replace
			the console log with a rotation animation:
		</p>
		<pre class="language-javascript"><code>
			const parameters = {
				color: 0xff0000,
				spin: () => {
					gsap.to(mesh.rotation, {
							duration: 1,
							y: mesh.rotation.y + 10
					})
				}
			};
			gui
				.add(parameters, 'spin');
		</code></pre>
		<section id="close_button">
			<h3>Close Button</h3>
			<p>As an aside, we can add a close button by writing the following:</p>
			<pre class="language-javascript"><code>
				const gui = new dat.GUI({closed: true});
			</code></pre>
			<p>The code above will add to the control panel an open/close button.</p>
		</section>
	</section>

	<section id="control_panel_width">
		<h3>Control Panel Width</h3>
		<p>
			To change the control panel's width, we include the value in the
			constructor:
		</p>
		<pre class="language-javascript"><code>
			const gui = new dat.GUI({
				closed: true,
				width: 400
			});
		</code></pre>
	</section>
</section>

<section id="textures">
	<h2>Textures</h2>
	<ol class="references">
		<li>
			<a
				href="https://threejs.org/docs/#api/en/textures/Texture"
				target="_blank"
				>ThreeJS, <strong>Documentation: Textures</strong>.</a
			>
		</li>
		<li>
			<a href="https://www.poliigon.com/" target="_blank"
				>Poliigon (resource for textures)</a
			>
		</li>
		<li>
			<a href="https://3dtextures.me/" target="_blank"
				>3dtextures.me (resource for textures)</a
			>
		</li>
		<li>
			<a href="https://ambientcg.com/" target="_blank">ambientCG</a> (resource
			for textures)
		</li>
		<li>
			<a href="https://polyhaven.com/textures" target="_blank"
				>Poly Haven, <strong>All Textures</strong> (resource for textures)</a
			>
		</li>
	</ol>
	<p>
		In the simplest terms, a <b>texture</b> is an image that covers a geometry's
		surface. As an image, textures come in many different types and effects:
	</p>
	<ul>
		<li>
			A color, or more formally, an <b>albedo</b>, is a texture. It's a single
			image of uniform color.
		</li>
		<li>
			An <b>alpha map</b> is another common texture: a grayscale image where
			whites are visible and blacks invisible.
		</li>
		<li>
			Like alpha, a <b>height map</b> or <b>displacement map</b> is a grayscale
			image texture. This texture creates the effect of a <i>relief</i>. If the
			color is white, the vertices move in a positive direction; black in a
			negative direction; and grey little to no movement (a perfect grey being
			no movement). With this texture, the image must have subdivisions. Because
			of this requirement, there are performance concerns.
		</li>
		<li>
			Unlike the height map, the <b>normal map</b> or <b>bump map</b> doesn't
			require subdivision. It can be used to add details, but the vertices do
			not move. Instead, details are added according to light position. Because
			normal maps don't need as many vertices as the height map, performance is
			better.
		</li>
		<li>
			<b>Ambient occlusion</b> is a grayscale image that adds shadows to
			crevices. While these aren't physically accurate, they can create contrast
			and detail.
		</li>
		<li>
			A <b>metallic</b> or <b>metalness map</b> is also a grayscale image. White
			is metallic, and and black is non-metallic. By classifying areas as
			metallic or non-metallic, we can in turn create the effect of reflection.
			If the material is metallic, we can see the environment behind us.
		</li>
		<li>
			A <b>roughness map</b> is another grayscale image usually used together
			with metallics. White is classified as rough, and black as smooth.
			Roughness maps are mostly used for light dissipation. For example, most
			carpets are rough material, and we do not see light bouncing off of its
			surface. In contrast, glass is very smooth material; light bouncing off of
			it can be blinding.
		</li>
	</ul>
	<p>
		There are many other kinds of textures, and we can even create our own. The
		textures above are the most commonly used. Importantly, they all follow a
		principle called <b>PBR (<q>Physically Based Rendering</q>)</b>. In PBR,
		renderings attempt to model objects as closely as possible to light flow in
		the real world. More broadly, PBR is an application of <i>photorealism</i>,
		a genre of art where paintings or drawings attempt to reproduce depictions
		as realistically as possible.
	</p>
	<section id="importing_images">
		<h3>Importing &amp; Loading Images</h3>
		<p>
			Because textures are images, we may want to import an image into our
			driver file. To do so, we write:
		</p>
		<ul class="syntax">
			<li>import ${n}$ from '${url}$'</li>
		</ul>
		<p>
			Where ${n}$ is a variable name, and ${url}$ is the filepath to the image.
			Alternatively, we can simply place the image in a
			<var>/static/</var> folder on the same level as our driver file:
		</p>
		<pre class="language-javascript"><code>
			const imageSource = '/image.png';
		</code></pre>
		<p>
			Alongside importing, we need a way to load the image. There are a variety
			of ways to load images. The most straightforward way is to use native
			JavaScript ():
		</p>
		<pre class="language-javascript"><code>
			const image = new Image();
			image.onload = () => { 
				console.log('image loaded');
			}
			image.src = '/textures/colors/color.jpg'
		</code></pre>
		<p>
			In the code above, we haven't imported the image. Instead, we create an
			<var>image</var> instance, listen to a <var>load</var> event, then change
			the relevant <var>src</var> attribute. Refreshing the page, the image
			loads.
		</p>
		<p>
			Even if the image is loaded, we cannot use it directly. ThreeJS doesn't
			know what an image is; we must convert it into a
			<var>Texture</var> object. This is done with a constructor:
		</p>
		<pre class="language-javascript"><code>
			const image = new Image();
			image.onload = () => { 
				const texture = new THREE.Texture(image);
			}
			image.src = '/textures/colors/color.jpg'
		</code></pre>
		<p>
			With the image now a texture, we must find a way to use the texture with
			the material. The hurdle: The <var>texture</var> variable is inside a
			function, and because of JavaScript's scoping rules, we cannot access it
			outside the function. Common newcomer approachs to solving this issue are
			the following (<em>do not do these</em>):
		</p>
		<ul>
			<li>Using <var>var</var> (not safe).</li>
			<li>
				Moving all of the mesh initialization code into the callback function
				(messy and unnecessary coupling).
			</li>
		</ul>
		<p>
			Again, the approaches above are what <em>not</em> to do. The safer
			approach is to (1) create the texture after instantiating the image, and
			(2) once the image is loaded, update the texture with ThreeJS's
			<span class="monoText"><mark>.needsUpdate</mark></span> property.<label
				for="needsupdate"
				class="margin-toggle"
				><sup></sup
			></label>
			<input
				type="checkbox"
				id="needsupdate"
				class="margin-toggle sidenote-number"
			/>
			<span class="marginnote"
				>The <var>.needsUpdate</var> property is a <var>material</var> property
				that specifies a material must be recompiled.</span
			>
		</p>
		<pre class="language-javascript"><code>
			const image = new Image();
			const texture = new THREE.Texture(image);
			image.onload = () => { 
				texture.needsUpdate = true; 
			}
			image.src = '/textures/colors/color.jpg';
		</code></pre>
	</section>
	<p>
		This entire process can be shortened by using ThreeJS's
		<span class="monoText"><mark>.TextureLoader()</mark></span> method:
	</p>
	<pre class="language-javascript"><code>
		const textureLoader = new THREE.TextureLoader();
		const texture = textureLoader.load('/textures/colors/color.jpg');
	</code></pre>

	<section id="loading_manager">
		<h3>Loading Manager</h3>
		<ol class="references">
			<li>
				<a
					href="https://threejs.org/docs/#api/en/loaders/managers/LoadingManager"
					target="_blank"
					>ThreeJS, <strong>Documentation: Loading Manager</strong></a
				>
			</li>
		</ol>
		<p>
			When we load images as textures, we should be mindful of potentially
			running into performance issues or bugs. Accordingly, we should always
			consider including a few have event handlers. The most common event
			handlers:
		</p>
		<ul>
			<li>
				A <b>load handler</b> to indicate when the image has loaded
				successfully.
			</li>
			<li>A <b>progress handler</b> to indicate the loading is progressing.</li>
			<li>An <b>error handler</b> to indicate something went wrong.</li>
		</ul>
		<p>
			We can write these functions directly inside the <var>load()</var> method:
		</p>
		<pre class="language-javascript"><code>
			const textureLoader = new THREE.TextureLoader();
			const texture = textureLoader.load(
					'/textures/colors/color.jpg',
					() => { console.log('load') },
					() => { console.log('progress') },
					() => { console.log('error') }
			);
		</code></pre>
		<p>
			A better way, however, is to use ThreeJS's load manager. The load manager
			has the following syntax:
		</p>
		<ul class="syntax">
			<li>const ${c}$ = new THREE.LoadingManager();</li>
		</ul>
		<p>
			Once we've instantiated a loading manager, we can use the loading manager
			inside our loaders:
		</p>
		<pre class="language-javascript"><code>
			const loadingManager = new THREE.LoadingManager();
			const textureLoader = new THREE.TextureLoader(loadingManager);
			const texture = textureLoader.load(
					'/textures/colors/color.jpg',
			);
		</code></pre>
		<p>
			The code above alone doesn't do anything noticeable, but that's only
			because we haven't included functions for the loading manager to use:
		</p>
		<pre class="language-javascript"><code>
			const loadingManager = new THREE.LoadingManager();

			loadingManager.onStart = () => { 
					console.log('Started loading');
			}

			loadingManager.onProgress = () => { 
					console.log('Loading in progress')
			}

			loadingManager.onLoad = () => { 
					console.log('Loaded');
			}

			loadingManager.onError = () => { 
					console.log('Error: loading failed.')
			}

			const textureLoader = new THREE.TextureLoader(loadingManager);
			const texture = textureLoader.load(
					'/textures/door/color.jpg',
			);
		</code></pre>
		<p>We can see that the loading managers work by inserting more textures:</p>
		<pre class="language-javascript"><code>
			// Loading Management
			const loadingManager = new THREE.LoadingManager();
			loadingManager.onStart = () => {console.log('Started loading');}
			loadingManager.onProgress = () => {console.log('Loading in progress')}
			loadingManager.onLoad = () => {console.log('Loaded');}
			loadingManager.onError = () => {console.log('Error: loading failed.')}

			// Textures
			const textureLoader = new THREE.TextureLoader(loadingManager);
			const colorTexture = textureLoader.load('/textures/door/color.jpg',);
			const alphaTexture = textureLoader.load('/textures/door/alpha.jpg',);
			const ambientOcclusionTexture = textureLoader.load('/textures/door/ambientOcclusion.jpg',);
			const metalnessTexture = textureLoader.load('/textures/door/metalness.jpg',);
			const roughnessTexture = textureLoader.load('/textures/door/roughness.jpg',);
		</code></pre>
		<pre class="language-bash"><code>
			Started loading
			(5) Loading in progress
			Loaded
		</code></pre>
	</section>

	<section id="uv_unwrapping">
		<h3>UV Unwrapping</h3>
		<p>
			When we apply a texture to a mesh, the texture is stretched and squeezed
			in different ways to cover the geometry. This is is called
			<b>UV unwrapping</b>. Depending on the geometry, this can make the texture
			look odd or distorted.
		</p>
		<aside>
			<figure>
				<img
					src="{% static 'images/origami_crane.svg' %}"
					alt="origami crane unwrapped"
					loading="lazy"
					class="sixty-p"
				/>
			</figure>
			<figcaption>Origami crane unwrapped.</figcaption>
		</aside>
		<p>
			We can think of UV unwrapping as akin to unwrapping an origami crane and
			flattening it. In doing so, we're left with (usually) a square, where each
			vertex in the original object is mapped to a coordate on a 2D plane. The
			resulting coordinates are called <b>UV coordinates</b>.
		</p>
		<p>
			UV unwrapping is more generally called <b>UV mapping</b>. In UV mapping,
			we take a 2D image and project it on to a 3D model's surface for texture
			mapping. The letters <q>U</q> and <q>V</q> denote the axes of the 2D
			texture (since ${x,}$ ${y,}$ and ${z}$ are already associated with the
			axes of the 3D model).
		</p>
		<p>
			We can access UV coordinates through
			<var><mark>.attributes.uv</mark>. Executing the code below:</var>
		</p>
		<pre class="language-javascript"><code>
			const geometry = new THREE.BoxBufferGeometry(1, 0.35, 32, 100)
			const material = new THREE.MeshBasicMaterial({ map: colorTexture })
			const mesh = new THREE.Mesh(geometry, material)
			scene.add(mesh)

			console.log(geometry.attributes.uv);
		</code></pre>
		<pre class="language-bash"><code>
			Float32BufferAttribute {name: '', array: Float32Array(1632), itemSize: 2, count: 816, normalized: false, …}
				array: Float32Array(1632) [0, 1, 1, …]
				count: 816
				itemSize: 2
				name: ""
				normalized: false
				updateRange: {offset: 0, count: -1}
				usage: 35044
				version: 0
				dynamic: (...)
				length: (...)
				[[Prototype]]: BufferAttribute
		</code></pre>
		<p>
			We see that the <var>uv</var> property (the UV coordinates) is a
			<var>Float32BufferAttribute</var>. The property <var>count</var> tells us
			that that there are ${816}$ coordinates, and the property
			<var>itemSize</var> tells us that the <var>Float32Array</var> elements are
			grouped by two (corresponding to the fact that UV coordinates consist of
			two number values).
		</p>
		<p>
			These coordinates are not mapped randomly. There are specific algorithms
			for creating UV coordinates, and we will examine some of those algorithms
			in later sections. When we create our own geometries, we can implement our
			own approaches to UV mapping.
		</p>
	</section>

	<section id="texture_transformations">
		<h3>Texture Transformations</h3>
		<p>
			ThreeJS provides several methods and properties for transforming textures.
			For example, consider the color texture we saw earlier:
		</p>
		<pre class="language-javascript"><code>
			const textureLoader = new THREE.TextureLoader(loadingManager);
			const colorTexture = textureLoader.load('/textures/door/color.jpg',);
		</code></pre>

		<section id="repeat_texture">
			<h4>Repeating a Texture</h4>
			<p>
				We can repeat this texture using the
				<var><mark>repeat</mark></var> property. The <var>repeat</var> property
				is a <var>Vector2</var> property with both <var>x</var> and
				<var>y</var> properties:
			</p>
			<pre class="language-javascript"><code>
				const textureLoader = new THREE.TextureLoader(loadingManager);
				const colorTexture = textureLoader.load('/textures/colors/color.jpg',);
				colorTexture.repeat.x = 2;
				colorTexture.repeat.y = 3;
			</code></pre>
			<p>
				Writing the code above, we might see some unexpected results. By
				default, the texture does not repeat and the last pixel gets stretched.
				To fix this, we use the
				<var><mark>THREE.RepeatWrapping</mark></var> property on the texture's
				<var><mark>wrapS</mark></var> and <var>wrapT</var> properties.
			</p>
			<pre class="language-javascript"><code>
				const textureLoader = new THREE.TextureLoader(loadingManager);
				const colorTexture = textureLoader.load('/textures/colors/color.jpg',);
				colorTexture.repeat.x = 2;
				colorTexture.repeat.y = 3;
				colorTexture.wrapS = THREE.RepeatWrapping;
				colorTexture.wrapT = THREE.RepeatWrapping;
			</code></pre>
			<p>Now we see that the texture actually repeats around the mesh.</p>
		</section>

		<section id="offsetting">
			<h4>Offsetting</h4>
			<p>
				We can offset a texture with <var>Vector2</var> property
				<var><mark>offset</mark></var
				>. This property only offsets the texture; it does not offset the entire
				UV.
			</p>
			<pre class="language-javascript"><code>
				const textureLoader = new THREE.TextureLoader(loadingManager);
				const colorTexture = textureLoader.load('/textures/door/color.jpg',);
				colorTexture.offset.x = 0.5;
				colrTexture.offset.y = 0.5;
			</code></pre>
		</section>

		<section id="rotating_textures">
			<h4>Rotating Textures</h4>
			<p>
				We can also rotate textures. Two important points: (1) This is
				<em>not</em> a <var>Vector2</var> property. (2) We shouldn't think about
				texture rotations in terms of axes &mdash; the texture is just rotating
				inside a 2D space.
			</p>
			<p>
				The property for rotating textures is <var><mark>.rotation</mark></var
				>, and its assigned values are in radians, positive values measured
				counter-clockwise. Thus, to rotate the texture by a half-circle, we
				would use the value ${\pi,}$ for a quarter-circle, ${\pi/2,}$ and for a
				full-circle, ${2\pi.}$
			</p>
			<pre class="language-javascript"><code>
				const textureLoader = new THREE.TextureLoader(loadingManager);
				const colorTexture = textureLoader.load('/textures/door/color.jpg',);
				colorTexture.rotation = Math.PI * 0.25;
			</code></pre>
			<p>
				The texture rotation's pivot is on the geometry's center point. Thus, if
				we want to change the pivot, we can do so by changing the texture's
				<var>.center</var> property:
			</p>
			<pre class="language-javascript"><code>
				const textureLoader = new THREE.TextureLoader(loadingManager);
				const colorTexture = textureLoader.load('/textures/door/color.jpg',);
				colorTexture.rotation = Math.PI * 0.25;
				colorTexture.center.x = 0.5;
				colorTexture.center.y = 0.5;
			</code></pre>
		</section>
	</section>

	<section id="filtering_and_midmapping">
		<h3>Filtering &amp; Mipmapping</h3>
		<ol class="references">
			<li>
				<a
					href="https://threejs.org/docs/#api/en/constants/Textures"
					target="_blank"
					>ThreeJS, <strong>Documentation: Texture Constants</strong></a
				>
			</li>
		</ol>
		<p>
			<b>Mipmapping</b> is a rendering technique where a texture is created by
			creating half of a smaller version of the texture again and again until a
			${1 \times 1}$ texture is created. Each of the texture variatons are then
			sent to the GPU, and the GPU chooses the most appropriate version of the
			texture for certain areas of the material.
		</p>
		<p>
			To use these midmappings, ThreeJS employs two kinds of algorithms. The
			first is
			<b>minification filter</b>. Minification algorithms are used when the when
			the texture's pixels are smaller than the render's pixels (i.e., the
			texture is too large for the surface).
		</p>
		<p>
			The clearest example of a minification algorithm at work is seen when we
			zoom out on a render. As we zoom out further and further, the texture gets
			smaller and smaller, keeping the texture's details in proportion. Without
			a minification algorithm, the texture would appear distorted. ThreeJS
			allows us to change which minified texture is used through the
			<var><mark>minFilter</mark></var> property.
		</p>
		<p>There are several minification algorithms provided by ThreeJS:</p>
		<ul>
			<li>
				<var><mark>THREE.NearestFilter</mark></var>
			</li>
			<li>
				<var><mark>THREE.LinearFilter</mark></var>
			</li>
			<li>
				<var><mark>THREE.NearestMipmapNearestFilter</mark></var>
			</li>
			<li>
				<var><mark>THREE.NearestMipmapLinearFilter</mark></var>
			</li>
			<li>
				<var><mark>THREE.LinearMipmapNearestFilter</mark></var>
			</li>
			<li>
				<var><mark>THREE.LinearMipmapLinearFilter</mark></var> (default)
			</li>
		</ul>
		<p>
			For example, if we want the texture's details to appear sharper on zooming
			out, we can write:
		</p>
		<pre class="language-javascript"><code>
			const textureLoader = new THREE.TextureLoader(loadingManager);
			const colorTexture = textureLoader.load('/textures/colors/color.jpg',);
			colorTexture.minFilter = THREE.NearestFilter;
		</code></pre>
		<p>
			If the texture image is too small, we will see some blurring. This is not
			necessarily a bad thing. In fact, in most cases, it's ideal to have this
			blurring as a default. For complex renderings with many meshes, users
			seldom notice the blurring &mdash; the additional processing for a sharper
			look is unnecessary. Moreover, we may be working on renderings where we
			want that blurring. However, if we don't want this blurring, we can modify
			the texture's <var><mark>magFilter</mark></var> property:
		</p>
		<pre class="language-javascript"><code>
			const textureLoader = new THREE.TextureLoader(loadingManager);
			const colorTexture = textureLoader.load('/textures/colors/colors.png',);
			colorTexture.magFilter = THREE.NearestFilter;
		</code></pre>
		<p>
			When working with these filters, always keep performance costs in mind.
			<var>THREE.NearestFiler</var> consumes less resouces than the other
			filters. We can significantly improve performance by turning off
			mipmapping for textures we don't need mipmapping for:
		</p>
		<pre class="language-javascript"><code>
			const textureLoader = new THREE.TextureLoader(loadingManager);
			const colorTexture = textureLoader.load('/colors/colors.png',);
			colorTexture.generateMipmaps = false;
		</code></pre>
	</section>
	<section id="texture_formats_optimization">
		<h3>Texture Formats & Optimization</h3>
		<p>
			Textures present numerous opportunities for performance savings. Three
			things to always consider in choosing and preparing textures: (1) the
			texture's <i>weight</i>, (2) the texture's <i>size</i>, and (3) the
			texture's <i>data</i>.
		</p>
		<p>
			The texture's weight is the size of the texture image file. The most
			common image files used are <var>.jpg</var> and <var>.png</var> files.
			There are costs and benefits to both:
		</p>
		<div class="costBenefit">
			<ul class="choice">
				<li>.png</li>
				<ul class="benefits">
					<li>Lossless compression (image details are kept intact).</li>
					<li>Supports transparency.</li>
				</ul>
				<ul class="costs">
					<li>Heavier file size.</li>
				</ul>
			</ul>
			<ul class="choice">
				<li>.jpg</li>
				<ul class="benefits">
					<li>Lighter file size.</li>
				</ul>
				<ul class="costs">
					<li>Does not support transparency.</li>
					<li>
						Lossy compression (image details are removed to reduce file size).
					</li>
				</ul>
			</ul>
		</div>
		<p>
			In generaly, when faced with various images, we want to keep the weight as
			low as possible. One way to do so is through compression. Both .png and
			.jpg files can be compressed. For .png files, a common compression
			resource is
			<a href="https://tinypng.com/" target="_blank">TinyPNG</a>.
		</p>
		<p>
			The texture's size is the texture image's resolution. Textures are
			processed by sending the image's pixels to the GPU. The higher an image's
			resolution, the more pixels there are.<label
				for="resolution"
				class="margin-toggle"
				><sup></sup
			></label>
			<input
				type="checkbox"
				id="resolution"
				class="margin-toggle sidenote-number"
			/>
			<span class="marginnote"
				>Importantly, an image's resolution does not always correspond to its
				file size. A .jpg image might be small in weight, but have a high
				resolution.</span
			>
			And the more pixels there are, the more memory must be allocated for the
			GPU's storage for processing. The GPU, like any other processing component
			of the computer system, has storage limitations. Teetering close or
			exceeding those limitations (e.g., overuse of mipmapping) is a recipe for
			disaster. Accordingly, we must always consider whether we need such high
			resolution texture images.
		</p>
		<p>
			Related to resolutions, we should always use texture images whose width
			and height are powers of two. For example, ${512 \times 512,}$ ${1024
			\times 1024,}$ ${512 \times 2048,}$ etc. This is because mipmapping works
			by continuously dividing the texture by two. Using other resolutions will
			require ThreeJS to perform more computations for resizing, which can lead
			to unexpected results.
		</p>
		<p>
			The texture's data are the data attached to the texture image. For
			example, in the previous .png and .jpg comparison, we see that .png
			supports transparency, but .jpg does not. If we want our texture to
			support transparency, we must use a .png file.
		</p>
		<p>
			Some textures like <var>normal</var> require the exact coordinates for
			image's objects. These coordinates are another piece of data. They are
			preserved in lossless image formats like .png, but lost in lossy formats
			like .jpg.
		</p>
	</section>
</section>

<section id="materials">
	<h2>Materials</h2>
	<ol class="references">
		<li>
			<a
				href="https://threejs.org/docs/#api/en/materials/Material"
				target="_blank"
				>ThreeJS, <strong>Documentation: Materials</strong></a
			>
		</li>
		<li>
			<a
				href="https://threejs.org/docs/?q=color#api/en/math/Color"
				target="_blank"
				>ThreeJS, <strong>Documentation: Color</strong></a
			>
		</li>
	</ol>
	<p>
		In the previous sections, we used a basic material. ThreeJS, however,
		provides many different materials. Recall: A <b>material</b> is an object
		that assigns colors to each visible pixel of a <i>geometry</i>. The
		algorithms for executing these assignments are called <b>shaders</b>.
		Without materials, we would have to write shaders ourselves. As we'll see in
		the section on shaders, this can be a tedious and difficult task.
	</p>
	<p>
		We'll start by creating three meshes, all of which will use the same
		material.
	</p>
	<pre class="language-javascript"><code>
		// Canvas
		const canvas = document.querySelector('canvas.webgl')

		// Scene
		const scene = new THREE.Scene()

		// Objects
		const material = new THREE.MeshBasicMaterial();

		const sphere = new THREE.Mesh(
				new THREE.SphereBufferGeometry(0.5, 16, 16),
				material
		)
		const plane = new THREE.Mesh(
				new THREE.PlaneGeometry(1, 1),
				material
		)
		const torus = new THREE.Mesh(
				new THREE.TorusGeometry(10,3,16,100),
				material
		)

		// Add to scene
		scene.add(sphere, plane, torus);
	</code></pre>
	<p>
		The code above renders all of the objects in the same place. Let's change
		their position properties:
	</p>
	<pre class="language-javascript"><code>
		// Canvas
		const canvas = document.querySelector('canvas.webgl');

		// Scene
		const scene = new THREE.Scene();

		// Objects
		const material = new THREE.MeshBasicMaterial();

		const sphere = new THREE.Mesh(
				new THREE.SphereBufferGeometry(0.5, 16, 16),
				material
		);
		sphere.position.x = -1.5;
		const plane = new THREE.Mesh(
				new THREE.PlaneGeometry(1, 1),
				material
		);
		const torus = new THREE.Mesh(
				new THREE.TorusBufferGeometry(0.3, 0.2, 16, 32),
				material
		);
		torus.position.x = 1.5;

		// Add to scene
		scene.add(sphere, plane, torus);
	</code></pre>
	<p>
		Because all of the objects use <var>material</var>, modifying this object
		will affect changes to all:
	</p>
	<pre class="language-javascript"><code>
		const material = new THREE.MeshBasicMaterial({
				color: 0xff0000
		});
	</code></pre>
	<p>
		For performance reasons, it's best practice to minimize the amount of
		materials we use. Let's add some animations through the
		<var>tick()</var> function:
	</p>
	<pre class="language-javascript"><code>
		const clock = new THREE.Clock()
		const tick = () => {
			const elapsedTime = clock.getElapsedTime()

			// Update objects
			sphere.rotation.y = elapsedTime * 0.1;
			plane.rotation.y = elapsedTime * 0.1;
			torus.rotation.y = elapsedTime * 0.1;

			sphere.rotation.x = elapsedTime * 0.15;
			plane.rotation.x = elapsedTime * 0.15;
			torus.rotation.x = elapsedTime * 0.15;

			// Update controls
			controls.update()

			// Render
			renderer.render(scene, camera)

			// Call tick again on the next frame
			window.requestAnimationFrame(tick)
		}
		tick()
	</code></pre>
	<p>We'll also load a few textures:</p>
	<pre class="language-javascript"><code>
		const textureLoader = new THREE.TextureLoader();
		const gradientTexture = textureLoader.load('/textures/gradients/3.jpg');
		const matcapTexture = textureLoader.load('/textures/matcaps/2.png');
	</code></pre>
	<p>With all of this ready, we can now consider materials.</p>

	<section id="mesh_basic_material">
		<h3>Basic Material</h3>
		<p>
			So far, we've been using <var><mark>MeshBasicMaterial</mark></var
			>. As we've seen, <var>MeshBasicMaterial</var> applies a uniform texture
			&mdash; whether that's an image or color value &mdash; to the geometry.
		</p>
		<p>
			<var>MeshBasicMaterial</var> has many properties. Earlier, we saw one
			property, <var>color</var>:
		</p>
		<pre class="language-javascript"><code>
			const material = new THREE.MeshBasicMaterial({
				color: 0xff0000
			})
		</code></pre>
		<p>We can also use the <var>map</var> property to add a texture:</p>
		<pre class="language-javascript"><code>
			const material = new THREE.MeshBasicMaterial({
				map: gradientTexture
			});
		</code></pre>
		<p>The <var>map</var> property can also be set by writing:</p>
		<pre class="language-javascript"><code>
			material.map = gradientTexture;
		</code></pre>
		<p>
			We cannot, however, do the same with the <var>color</var> property. This
			is because the <var>color</var> property is an instance of
			<a
				href="https://threejs.org/docs/?q=color#api/en/math/Color"
				target="_blank"
				>the <var>Color</var> class</a
			>. Accordingly, outside of passing an object, there are two ways to set
			the color property. We can use the <var>Color</var> class's method
			<var><mark>.set()</mark></var
			>:
		</p>
		<pre class="language-javascript"><code>
			material.set('#ff0000');
		</code></pre>
		<p>or we can assign a <var>Color</var> object to the property:</p>
		<pre class="language-javascript"><code>
			material.color = new THREE.color('yellow');
		</code></pre>
		<p>
			<var>MeshBasicMaterial</var> also has the <var>wireframe</var> property,
			which we saw earlier:
		</p>
		<pre class="language-javascript"><code>
			material.wireframe = true
		</code></pre>
		<p>
			For some visualizations, we might want to see through a mesh. We can
			achieve through the the material's
			<var><mark>.opacity</mark></var> property. To use this property, we must
			ensure the material's <var><mark>.transparent</mark></var> property is set
			to <var>true</var>:
		</p>
		<pre class="language-javascript"><code>
			const material = new THREE.MeshBasicMaterial({
				opacity: 0.5,
				transparent: true
			});
		</code></pre>
		<p>
			Alternatively, we could achieve a similar effect with the
			<var><mark>.alphaMap</mark></var> property. Recall that
			<var>alphaMap</var> controls the transparency through a texture &mdash;
			whites are shown and blacks are hidden. Like the
			<var>opacity</var> property, the <var>alphaMap</var>'s assigned value will
			only be made visible if the <var>transparency</var> property is set to
			<var>true</var>. For example, assuming we've loaded some texture called
			<var>alphaTexture</var>:
		</p>
		<pre class="language-javascript"><code>
			const material = new THREE.MeshBasicMaterial({
				alphaMap: alphaTexture,
				transparent: true
			});
		</code></pre>
	</section>

	<section id="mesh_normal_material">
		<h3>Normal Material</h3>
		<p>
			A <b>normal</b> is a piece of information about the outside direction of
			an object's face. For example, the outside of a sphere's face is the
			sphere's surface. We can think of the sphere's normals as arrows coming
			off of the sphere's surface.
		</p>
		<p>
			We can use normals for lighting, reflection, and refraction. For example,
			consider the process of visualizing light bouncing off of a sphere's
			surface. How does ThreeJS know which side is lighter and which side is
			darker? Through the sphere's normals. If the sphere's normal is pointed in
			the direction of the light source, then that particular area with the
			normal should be lighter. If the sphere's normal is pointed away from the
			light source, then the area with the normal should be darker.
		</p>
		<p>
			ThreeJS provides normal material through
			<var><mark>.MeshNormalMaterial()</mark></var
			>:
		</p>
		<pre class="language-javascript"><code>
			const material = new THREE.MeshNormalMaterial();
		</code></pre>
		<p>
			With normal materials, we have properties like
			<var><mark>.flatShading</mark></var
			>, which flattens the mesh's faces.
		</p>
	</section>

	<section id="mesh_matcap_material">
		<h3>Matcap Materials</h3>
		<ol class="references">
			<li>
				<a href="https://github.com/nidorx/matcaps" target="_blank"
					>nidorx, <strong>Github: matcaps</strong> (large collection of matcap
					materials)</a
				>
			</li>
		</ol>
		<p>
			A <b>matcap material</b> is a material that displays a color, but with the
			added feature of rendering different shades on the surface. This is done
			by using the normals in a texture image as a reference. The effect: the
			mesh appears illuminated. In ThreeJS:
		</p>
		<pre class="language-javascript"><code>
			const material = new THREE.MeshMatcapMaterial()
			material.matcap = matcapTexture
		</code></pre>
		<p>
			Matcap materials effectively allow us to simulate light without having an
			actual light source.
		</p>
	</section>

	<section id="mesh_depth_material">
		<h3>Depth Material</h3>
		<p>
			Recall that cameras have both <var><mark>near</mark></var> and
			<var><mark>far</mark></var> properties. A <b>depth material</b> takes
			advantage of these two properties. If we are close to the mesh (a greater
			<var>near</var> value), the depth material colors the close area white. If
			we are further from the mesh (a greater <var>far</var> value), the depth
			material colors the far area black. Instantiating:
		</p>
		<pre class="language-javascript"><code>
			const material = new THREE.MeshDepthMaterial();
		</code></pre>
		<p>
			Depth material creates the effect of illuminating a particular area as we
			get closer. We can think of it like exploring a cave with a flashlight.
			Objects closer to us are more illuminated than objects further.
		</p>
	</section>

	<section id="light_reliant_materials">
		<h3>Light-reliant Materials</h3>
		<p>
			The materials below can be referred to as <b>light-reliant materials</b>.
			These are materials that must have some light source. We will examine
			lights in closer detail at a later juncture, but these materials provide a
			sneak peak at instantiating and using lights. For the following materials,
			we will use the following code:
		</p>
		<pre class="language-javascript"><code>
			const ambientLight = new THREE.AmbientLight(0xffffff, 0.5);
			const pointLight = new THREE.PointLight(0xffffff, 0.5);
			pointLight.position.x = 2;
			pointLight.position.y = 3;
			pointLight.position.z = 4;
			scene.add(ambientLight, pointLight)
		</code></pre>
		<p>With the lights established, let's now examine the materials.</p>

		<section id="lambert_material">
			<h4>Lambert Material</h4>
			<p>
				The most basic light-reliant material is a <b>lambert material</b>. This
				material simply reacts to light.
			</p>
			<pre class="language-javascript"><code>
				const material = new THREE.MeshLambertMaterial();
			</code></pre>
			<p>
				Applying lambert material, we might notice some white blurring along the
				mesh's edges. This is to be expected. Out of all the light-reliant
				materials, the lambert material is the most performant. The tradeoff,
				however, is meticulous detail.
			</p>
		</section>

		<section id="phong_material">
			<h4>Phong Materials</h4>
			<p>
				A <b>phong material</b> looks similar to a lambert material, but with a
				few differences. First, the white blurring along the mesh's edges are
				essentially eliminated, and we see light bouncing off of the material.
				With the lambert material, the surface pointed towards the light is
				merely illuminated. Initializing:
			</p>
			<pre class="language-javascript"><code>
				const material = new THREE.MeshPhongMaterial();
			</code></pre>
			<p>
				Because phong materials can reflect light, they have a parameter,
				<var><mark>shininess</mark></var
				>, which we can modify to set how <q>shiny</q> the object is:
			</p>
			<pre class="language-javascript"><code>
				const material = new THREE.MeshPhongMaterial();
				material.shininess = 80;
			</code></pre>
			<p>
				Additionally, we can change the shine's color by modifying the
				<var><mark>specular</mark></var> property:
			</p>
			<pre class="language-javascript"><code>
				const material = new THREE.MeshPhongMaterial();
				material.specular = new THREE.color(0xff0000);
			</code></pre>
			<p>
				The drawback with phong materials, of course, is that they aren't as
				performant as lambert materials.
			</p>
		</section>

		<section id="mesh_toon_material">
			<h4>Mesh Toon Material</h4>
			<p>
				The <b>toon material</b> achieves the same effect as the lambert and
				phong materials, but with higher contrast on the shine. This achieves a
				cartoon-like effect. Instantiating:
			</p>
			<pre class="language-javascript"><code>
				const material = new THREE.MeshToonMaterial();
			</code></pre>
			<p>
				We can control the contrasts by adding more steps to the coloration
				through the <var><mark>.gradientMap</mark></var> property and the
				<var><mark>.gradientTexture</mark></var> property.
			</p>
			<pre class="language-javascript"><code>
				const textureLoader = new THREE.TextureLoader();
				const gradientTexture = textureLoader.load('/textures/gradients/3.jpg');
				const material = new THREE.MeshToonMaterial();
				material.gradientMap = gradientTexture;
			</code></pre>
			<p>
				In the code above, we applied a texture called
				<var>gradientTexture</var>, which is an image to some image file
				displaying a gradient. If the gradient is small, executing the code
				above renders a mesh that looks more like a lambert mesh than the
				original toon mesh. We're seeing this behavior because the computer is
				attempting to resolve the small gradient through mipmapping rather than
				a clear separation (as we would expect with a toon mesh).
			</p>
			<p>
				We can avoid this behavior by setting the <var>minFilter</var> and
				<var>magFilter</var> to the object <var>THREE.NearestFilter</var>:
			</p>
			<pre class="language-javascript"><code>
				const textureLoader = new THREE.TextureLoader();
				const gradientTexture = textureLoader.load('/textures/gradients/3.jpg');

				gradientTexture.minFilter = THREE.NearestFilter;
				gradientTexture.magFilter = THREE.NearestFilter;

				const material = new THREE.MeshToonMaterial();
				material.gradientMap = gradientTexture;
			</code></pre>
			<p>
				Becuse mip maps can be resource-intensive, using
				<var>.NearestFilter</var> presents an opportunity to turn off
				mipmapping:
			</p>
			<pre class="language-javascript"><code>
				const textureLoader = new THREE.TextureLoader();
				const gradientTexture = textureLoader.load('/textures/gradients/3.jpg');

				gradientTexture.minFilter = THREE.NearestFilter;
				gradientTexture.magFilter = THREE.NearestFilter;
				gradientTexture.generateMipmaps = false;

				const material = new THREE.MeshToonMaterial();
				material.gradientMap = gradientTexture;
			</code></pre>
			<p>
				Doing so can save a significant amount of memory and processing power.
			</p>
		</section>
	</section>

	<section id="mesh_standard_material">
		<h3>Standard Material</h3>
		<p>
			We can think of the <b>standard material</b> as an upgraded basic
			material. It supports lights and all of the other features we've seen with
			other materials, but implements them through PBR principles. This leads to
			materials appearing far more realistic. The standard material also
			provides more descriptive parameters like <var>metalness</var> and
			<var>roughness</var>.
		</p>
		<pre class="language-javascript"><code>
			const material = new THREE.MeshStandardMaterial();
			const material = new THREE.MeshStandardMaterial();
			material.metalness = 0.45;
			material.roughness = 0.65;
		</code></pre>
		<p>
			With the standard material, we also have access to a property called
			<var><mark>aoMap</mark></var> (<q>ambient occlusion map</q>). This
			property adds shadows to dark textures. To use this property, we must
			provide a second set of UV coordinates named <var><mark>uv2</mark></var
			>. Recall that UV coordinates are what determine how textures are applied
			to a geometry, and they exist as attributes of a particular geometry:
		</p>
		<pre class="language-javascript"><code>
			console.log(plane.geometry.attributes);
		</code></pre>
		<pre class="language-bash"><code>
			{position: Float32BufferAttribute, normal: Float32BufferAttribute, uv: Float32BufferAttribute}
		</code></pre>
		<p>
			For example, if we wanted to modify the <var>aoMap</var> property for a
			plane geometry, we would first have to set the plane geometry's
			<var>uv2</var> property. To do so, we must pass as an argument an instance
			of a <var><mark>BufferAttribute</mark></var> to the
			<var>setAttribute()</var> method. Because we're creating a
			<var>uv2</var> buffer attribute, we must pass the
			<var>uv</var> coordinates for the plane geometry, alongside the grouping
			number (in the code below, <var>2</var>, since UV coordinates consist of
			two numbers):
		</p>
		<pre class="language-javascript"><code>
			plane.geometry.setAttribute(
				'uv2',
				new THREE.BufferAttribute(plane.geometry.attributes.uv.array, 2)
			)			
		</code></pre>
		<p>
			Once we've initialized the <var>uv2</var> property, we can then use the
			material's <var>aoMap</var> property:
		</p>
		<pre class="language-javascript"><code>
			const textureLoader = new THREE.TextureLoader();
			const doorAmbientOcclusionTexture = textureLoader.load('/textures/ambientOcclusion.jpg');
			material.aoMap = ambientOcclusionTexture;
		</code></pre>
		<p>We can also change the <var>aoMap</var>'s intensity:</p>
		<pre class="language-javascript"><code>
			material.aoMapIntensity = 10;
		</code></pre>
		<p>
			This will increase the shadows' intensity &mdash; making them even more
			dark.
		</p>
		<p>
			The <var>aoMap</var> can create the effect of a relief from a fixed point,
			but rotating the mesh, everything still appears flat. To create true
			relief, we can use the <var><mark>displacementMap</mark></var> property:
		</p>
		<pre class="language-javascript"><code>
			const textureLoader = new THREE.TextureLoader();
			const heightTexture = textureLoader.load('/textures/height.jpg');
			material.displacementMap = heightTexture;
		</code></pre>
		<p>
			Depending on what the texture image is, <var>displacementMap</var> can
			lead to some bizarre renderings. These bizarreries almost always result
			from a lack of vertices, a displacement value too large, or a combination
			of the two. We can fix the oddities by playing with the subdivisions and
			displacement values. For example, with a torus geometry:
		</p>
		<pre class="language-javascript"><code>
			material.displacementScale = 0.05;
			const torus = new THREE.Mesh(
				new THREE.TorusBufferGeometry(0.3, 0.2, 64, 128),
				material
			);
		</code></pre>
		<p>
			We also have the properties <var><mark>metalnessMap</mark></var> and
			<var><mark>roughnessMap</mark></var> for applying metallic and rough
			textures. Importantly, these properties are distinct from the properties
			<var><mark>metalness</mark></var> and <var><mark>roughness</mark></var
			>. In general, we want to avoid using these these properties all together.
			Pick one set or the other. Using both can lead to (1) performance issues,
			and (2) too much or too little light reflection.
		</p>
		<p>
			To add even more details &mdash; relief, shadows, lighting, etc. &mdash;
			we can use the <var><mark>normalMap</mark></var> property. This property
			essentially fakes the orientation of normals and adds detials on the
			surface, regardless of the amount of subdivisions a geometry has.
		</p>
		<pre class="language-javascript"><code>
			material.normalMap = doorNormalTexture;
		</code></pre>
		<p>
			We can change the normal map's intensity by modifying the
			<var><mark>normalScale</mark></var> property:
		</p>
		<pre class="language-javascript"><code>
			material.normalScale.set(0.01, 0.01); 
		</code></pre>
	</section>
</section>

<section id="threeD_text">
	<h2>3D Text</h2>
	<ol class="references">
		<li>
			<a
				href="https://threejs.org/docs/#examples/en/geometries/TextGeometry"
				target="_blank"
				>ThreeJS, <strong>Documentation: Text Geometry</strong></a
			>
		</li>
		<li>
			<a href="https://gero3.github.io/facetype.js/" target="_blank"
				>gero3, <strong>Facetype.js</strong> (Tool for converting fonts to
				typeface.js format).</a
			>
		</li>
	</ol>
	<p>
		In this section, we examine how to render three-dimensional text. This is
		done using ThreeJS's <var><mark>TextGeometry</mark></var> class.
	</p>
	<p>
		The first requirement we must meet to use <var>TextGeometry</var> is to
		ensure the font we use is a <b>typeface.json generated font</b>. The easiest
		way to do this is to upload our font to
		<a href="https://gero3.github.io/facetype.js/" target="_blank"
			>Facetype.js</a
		>, a program for converting fonts to <var>typeface.js</var> fonts.
		Alternatively, we can use
		<a
			href="https://threejs.org/docs/#examples/en/geometries/TextGeometry"
			target="_blank"
			>ThreeJS's provided fonts</a
		>. Once we've decided on our fonts, we can place them in our project
		directory and import them to our driver file, just as we would other files:
	</p>
	<pre class="language-javascript"><code>
		import typeFaceFont from 'three/fonts/typeface.json'
	</code></pre>
	<p>
		Alternatively, and the more common way to load fonts, is to use ThreeJS's
		<var><mark>FontLoader()</mark></var> method:
	</p>
	<pre class="language-javascript"><code>
		// Import the font loader
		import { FontLoader } from 'three/examples/jsm/loaders/FontLoader.js'

		// Load the fonts
		const fontLoader = new FontLoader();
		fontLoader.load(
				'/fonts/myFont.typeface.json',
				(font) => { 
						console.log('loaded');
				}
		);
	</code></pre>
	<pre class="language-bash"><code>
		loaded
	</code></pre>
	<p>
		Above, we called <var>fontLoader.load()</var> and passed it two arguments:
		(1) The file path to our typeface font, and (2) a callback function. This
		callback function is used to instantiate a <var>TextGeometry</var>. Because
		the <var>TextGeometry</var> class is in a separate module from the usual
		modules, we must import it separately. Once we imported, we can instantiate
		<var>TextGeometry</var> inside the callback function for
		<var>fontloader.load()</var>:
	</p>
	<pre class="language-javascript"><code>
		// Import text geometry 
		import { TextGeometry } from 'three/examples/jsm/geometries/TextGeometry.js'

		// Import the font loader
		import { FontLoader } from 'three/examples/jsm/loaders/FontLoader.js'

		// Load the fonts
		const fontLoader = new FontLoader();
		fontLoader.load(
				'/fonts/helvetiker_regular.typeface.json',
				(font) => {
						const textGeometry = new TextGeometry();
				}
		);
	</code></pre>
	<p>
		Inside the callback function's return, we write the following (we'll go over
		each property momentarily):
	</p>
	<pre class="language-javascript"><code>
		const fontLoader = new FontLoader();
		fontLoader.load(
			'/fonts/helvetiker_regular.typeface.json',
			(font) => {
				const textGeometry = new TextGeometry(
					'Text',
					{
						font: font,
						size: 0.5,
						height: 0.2,
						curveSegments: 12,
						bevelEnabled: true,
						bevelThickness: 0.03,
						bevelSize: 0.02,
						bevelOffset: 0,
						bevelSegments: 5
					}
				)
				const textMaterial = new THREE.MeshBasicMaterial({wireframe: true})
				const text = new THREE.Mesh(textGeometry, textMaterial)
				scene.add(text)
			}
		);
	</code></pre>
	<p>
		In the callback function, the return is an object. Inside that object, we
		have a <var>TextGeometry</var> object, a material, a mesh, and a scene-add.
		The <var>TextGeometry</var> constructor takes two arguments: (1) a string
		&mdash; corresponding to the text we want displayed; and (2) an object
		containing the displayed text's properties.
	</p>
	<p>
		Before examining the text properties, a few things to note. First, creating
		a text geometry is long and resource-intensive. Without optimizations, the
		text geometry often contains numerous unnecessary segments. These are all
		hampers on performance. Accordingly, we must always perform the following
		when working with text geometries:
	</p>
	<ol>
		<li>
			Ask: Is this text geometry necessary? If there's another, more performant
			way to achieve the same purpose, use that approach. Minimizing text
			geometries as much as possible yields substantial gains in performance.
		</li>
		<li>
			Act: Keep the geometry as low as possible by reducing the
			<var>curveSegments</var> and <var>bevelSegments</var> properties. Curve
			segments are the segments that compose most of the text. We can think of
			them as the primary segments for geometry. Bevel segments are used to add
			bevels to the the geometry &mdash; sharp edges are smoothened to establish
			a rounder effect.
		</li>
	</ol>
	<p>
		<span class="topic">Centering Text.</span> By default, a text geometry's
		center is at the most bottom-left corner of the first character rendered. To
		center the text, we could manipulate the mesh's position, but doing so would
		prevent us from performing animations like rotations on the text's center.
		(Since the center is still on the text's most bottom-left corner). There are
		two ways to center text geometries. The first is to use a <b>bounding</b>.
	</p>
	<p>
		A bounding is data associated with a given geometry, corresponding to the
		space occupied by that geometry. This space is either a box or a sphere.
		ThreeJS uses bounding for various procedures, the most common being
		<b>frustrum culling</b> &mdash; roughly, the process of determining whether
		an object is on the screen. For example, if an object is behind the camera,
		the object should not be rendered. Or if the object is not within the
		camera's view more generally, the object should not be rendered. Determining
		whether a particular object is within the camera's view is just one example
		of frustrum culling.
	</p>
	<p>
		For a text geometry, we can use a box bounding to center the text. By
		default, ThreeJS uses <b>sphere bounding</b>. To use box bounding, we first
		call the <var><mark>computeBoundingBox</mark></var> method:
	</p>
	<pre class="language-javascript"><code>
		const fontLoader = new FontLoader();
		fontLoader.load(
			'/fonts/helvetiker_regular.typeface.json',
			(font) => {
				const textGeometry = new TextGeometry(
					'Text',
					{
						font: font,
						size: 0.5,
						height: 0.2,
						curveSegments: 5,
						bevelEnabled: true,
						bevelThickness: 0.03,
						bevelSize: 0.02,
						bevelOffset: 0,
						bevelSegments: 4
					}
				)

				// Bounding Box
				textGeometry.computeBoundingBox();
				console.log(textGeometry.boundingBox);

				const textMaterial = new THREE.MeshBasicMaterial({wireframe: true})
				const text = new THREE.Mesh(textGeometry, textMaterial)
				scene.add(text)
			}
		);
	</code></pre>
	<pre class="language-bash"><code>
		Box3
			max: Vector3 {x: 1.409500002861023, y: 0.5264999866485596, z: 0.23000000417232513}
			min: Vector3
				x: -0.019999999552965164
				y: -0.033042728900909424
				z: -0.029999999329447746
				[[Prototype]]: Object
			[[Prototype]]: Object
	</code></pre>
	<p>
		Logging to the console <var>textGeometry.boundingBox</var>, we see that we
		get back an instance of the class <var><mark>Box3</mark></var
		>. Instances of <var>Box3</var> are not visible objects. It's purely a
		purely mathematical entity. We can think of it as a box, but it is not a
		visible box. Notice that the <var>Box3</var> object has a <var>max</var> and
		<var>min</var> value. Both of which are <var>Vector3</var> objects.
		Examining them closely, we see that they're coordinates. But why are there
		negative values for the <var>min</var> object? We're seeing these negative
		values because of the <var>bevelThickness</var> and
		<var>bevelSize</var> properties.
	</p>
	<p>
		The idea behind using the bounding box is to move the
		<em>geometry</em> rather than the mesh. In other words, we move the geometry
		such that its center point is the mesh's center point, rather than moving
		the mesh entirely. To move a text geometry, we can use the
		<var>translate()</var> method, since <var>textGeometry</var> inherits from
		<var>BufferGeometry</var>.
	</p>
	<pre class="language-javascript"><code>
		const fontLoader = new FontLoader();
		fontLoader.load(
			'/fonts/helvetiker_regular.typeface.json',
			(font) => {
				const textGeometry = new TextGeometry(
						'Text',
						{
								font: font,
								size: 0.5,
								height: 0.2,
								curveSegments: 5,
								bevelEnabled: true,
								bevelThickness: 0.03,
								bevelSize: 0.02,
								bevelOffset: 0,
								bevelSegments: 4
						}
				)

				// Bounding Box
				textGeometry.computeBoundingBox()

				// Center Text
				textGeometry.translate(
						- textGeometry.boundingBox.max.x * 0.5,
						- textGeometry.boundingBox.max.y * 0.5,
						- textGeometry.boundingBox.max.z * 0.5
					)

				const textMaterial = new THREE.MeshBasicMaterial({wireframe: true})
				const text = new THREE.Mesh(textGeometry, textMaterial)
				scene.add(text)
			}
		);
	</code></pre>
	<p>
		The text above, however, is not entirely centered. This is because of the
		two properties we mentioned earlier, <var>bevelThickness</var> and
		<var>bevelSize</var>. Accordingly, what we want to do is subtract these
		property values from the <var>textGeometry.boundingBox.max</var> values:
	</p>
	<pre class="language-javascript"><code>
		const fontLoader = new FontLoader();
		fontLoader.load(
			'/fonts/helvetiker_regular.typeface.json',
			(font) => {
				const textGeometry = new TextGeometry(
						'Text',
						{
								font: font,
								size: 0.5,
								height: 0.2,
								curveSegments: 5,
								bevelEnabled: true,
								bevelThickness: 0.03,
								bevelSize: 0.02,
								bevelOffset: 0,
								bevelSegments: 4
						}
				)

				// Bounding Box
				textGeometry.computeBoundingBox()

				// Center Text
				textGeometry.translate(
						- (textGeometry.boundingBox.max.x - 0.02) * 0.5,
						- (textGeometry.boundingBox.max.y - 0.02) * 0.5,
						- (textGeometry.boundingBox.max.z - 0.03) * 0.5
					)

				const textMaterial = new THREE.MeshBasicMaterial({wireframe: true})
				const text = new THREE.Mesh(textGeometry, textMaterial)
				scene.add(text)
			}
		);
	</code></pre>
	<p>
		Now our text is really centered. This entire process is encapsulated by a
		method in ThreeJS called <var><mark>center()</mark></var
		>:
	</p>
	<pre class="language-javascript"><code>
		const fontLoader = new FontLoader();
		fontLoader.load(
			'/fonts/helvetiker_regular.typeface.json',
			(font) => {
				const textGeometry = new TextGeometry(
						'Text',
						{
								font: font,
								size: 0.5,
								height: 0.2,
								curveSegments: 5,
								bevelEnabled: true,
								bevelThickness: 0.03,
								bevelSize: 0.02,
								bevelOffset: 0,
								bevelSegments: 4
						}
				)

				textGeometry.center()

				const textMaterial = new THREE.MeshBasicMaterial({wireframe: true})
				const text = new THREE.Mesh(textGeometry, textMaterial)
				scene.add(text)
			}
		);
	</code></pre>
</section>

<section id="creating_multiple_meshes">
	<h2>Creating Multiple Meshes</h2>
	<p>
		Suppose we have a torus geometry, and we want to create a hundred instances
		of the torus geometry. One way to do so is by writing the following:
	</p>
	<pre class="language-javascript"><code>
		const textureLoader = new THREE.TextureLoader();
		const matCapTexture = textureLoader.load('/textures/matcaps/1.png');
		for (let i = 0; i < 100; i++) {
			// Instantiate geometry and material
			const torus = new THREE.TorusBufferGeometry(0.3, 0.2, 20, 45);
			const torusMaterial = new THREE.MeshMatCapMaterial({
				matcap: matCapTexture;
			});

			// Create mesh
			const torus = new THREE.Mesh(torus, torusMaterial);

			// Vary the toruses's positions
			torus.position.x = (Math.random() - 0.5) * 10;
			torus.position.y = (Math.random() - 0.5) * 10;
			torus.position.z = (Math.random() - 0.5) * 10;

			// Vary the toruses's rotations
			torus.rotation.x = Math.random() * Math.PI;

			// Vary the toruses's scales
			const scale = Math.random();
			torus.scale.x = scale;
			torus.scale.y = scale;
			torus.scale.z = scale;

			// Add to scene
			scene.add(torus)
		}
	</code></pre>
	<p>
		The problem with this approach, however, is that it's not performant. We can
		see the time cost approach for this approach by using the JavaScript's
		<var><mark>.time()</mark></var> and
		<var><mark>.timeEnd()</mark></var> methods:
	</p>
	<pre class="language-javascript"><code>
		console.time('torus');

		const textureLoader = new THREE.TextureLoader();
		const matCapTexture = textureLoader.load('/textures/matcaps/1.png');
		for (let i = 0; i < 100; i++) {
			// Instantiate geometry and material
			const torus = new THREE.TorusBufferGeometry(0.3, 0.2, 20, 45);
			const torusMaterial = new THREE.MeshMatCapMaterial({
				matcap: matCapTexture;
			});

			// Create mesh
			const torus = new THREE.Mesh(torus, torusMaterial);

			// Vary the toruses's positions
			torus.position.x = (Math.random() - 0.5) * 10;
			torus.position.y = (Math.random() - 0.5) * 10;
			torus.position.z = (Math.random() - 0.5) * 10;

			// Vary the toruses's rotations
			torus.rotation.x = Math.random() * Math.PI;

			// Vary the toruses's scales
			const scale = Math.random();
			torus.scale.x = scale;
			torus.scale.y = scale;
			torus.scale.z = scale;

			// Add to scene
			scene.add(torus)
		}

		console.timeEnd('torus');
	</code></pre>
	<pre class="language-bash"><code>
		70.4560546875 ms
	</code></pre>
	<p>
		It takes about ${70.5~\text{ms}}$ to execute the code above. At a thousand
		toruses, we would see this time cost increase to nearly half a second. This
		is bad. Very bad. With a sufficiently complex scene, we would see our
		renderings freeze, or worse, crash.
	</p>
	<p>
		We're seeing an enormous rendering time because JavaScript is processing all
		of the statements in the loop's body at <em>each iteration</em>. A single
		call to a method like <var>new HREE.TorusBufferGeometry()</var> is a
		significant undertaking. Now multiply that by a hundred.
	</p>
	<p>
		It follows then that we can significantly reduce this time cost by
		processing
		<b>invariant commands</b> &mdash; code to be repeated &mdash; ahead of their
		repetitions or reuse. In this case, the material and the geometry are always
		the same. The only things that vary are their properties:
	</p>
	<pre class="language-javascript"><code>
		console.time('torus');

		const textureLoader = new THREE.TextureLoader();
		const matCapTexture = textureLoader.load('/textures/matcaps/1.png');

		// Instantiate geometry and material
		const torus = new THREE.TorusBufferGeometry(0.3, 0.2, 20, 45);
		const torusMaterial = new THREE.MeshMatCapMaterial({
			matcap: matCapTexture;
		});

		for (let i = 0; i < 100; i++) {
			// Create mesh
			const torus = new THREE.Mesh(torus, torusMaterial);

			// Vary the toruses's positions
			torus.position.x = (Math.random() - 0.5) * 10;
			torus.position.y = (Math.random() - 0.5) * 10;
			torus.position.z = (Math.random() - 0.5) * 10;

			// Vary the toruses's rotations
			torus.rotation.x = Math.random() * Math.PI;

			// Vary the toruses's scales
			const scale = Math.random();
			torus.scale.x = scale;
			torus.scale.y = scale;
			torus.scale.z = scale;

			// Add to scene
			scene.add(torus)
		}

		console.timeEnd('torus');
	</code></pre>
	<pre class="language-bash"><code>
		3.361083984375 ms
	</code></pre>
	<p>
		By just moving the geometry and material instantiation outside of the loop,
		we went from rough ${70.5~\text{ms}}$ to ${3.4~\text{ms}.}$ This is roughly
		a ${95.177\%}$ decrease in rendering time. That's a massive improvement.
	</p>
	<p>
		This illustrates a critical lesson in graphics programming: Always &mdash;
		always &mdash; look for opportunities to optimize and take them. In many
		other areas of programming, we can get away with practices like leaving
		computations and function calls inside for-loops. That is not the case with
		graphics programming. Computations are costly, both in time and space. Even
		the smallest changes &mdash; like moving two lines of code elsewhere &mdash;
		can yield great returns.
	</p>
</section>

<section id="lights">
	<h2>Lights</h2>
	<p>
		Adding a light is no different from adding meshes. With ThreeJS, we just
		have to instantiate the property class and add the resulting instance to our
		scene. ThreeJS provides several kinds of lights. Let's go over them.
	</p>

	<section id="ambient_light">
		<h3>Ambient Light</h3>
		<p>
			An <b>ambient light</b> applies what we call
			<b>omnidirectional lighting</b>. This means that light radiating from an
			ambient light source comes from all directions. In ThreeJS, ambient lights
			are provided through the <var><mark>AmbientLight</mark></var> class. The
			constructor:
		</p>

		<ul class="syntax">
			<li>const ambientLight = new THREE.AmbientLight(${c}$, ${t}$)</li>
		</ul>

		<p>
			The first parameter, ${c,}$ corresponds to the light's color. The second
			parameter, ${t,}$ corresponds to the light's intensity. In JavaScript:
		</p>

		<pre class="language-javascript"><code>
			const ambientLight = new THREE.AmbientLight(0xffffff, 0.5);
			scene.add(ambientLight);
		</code></pre>

		<p>
			Adding an ambient light to the scene creates uniform light on the meshes.
			This means that, without more, ambient light looks more like a material
			than an actual light source. It can, however, be a very useful tool.
		</p>
		<p>As an aside, we can add a light source to debug panel as follows:</p>
		<pre class="language-javascript"><code>
			import * as debugUI from 'lil-gui'
			const debugPanel = new debugGUI.GUI()
			debugPanel.add(ambientLight, 'intensity').min(0).max(1).step(0.01);
		</code></pre>
	</section>
</section>
{% endblock %}
