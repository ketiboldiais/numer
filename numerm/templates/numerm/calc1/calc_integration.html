{% extends '../layout.html' %} {% load static %} {% block description %}
<meta name="description" content="Notes on integration" />
{% endblock %} {% block title %}
<title>Integration</title>
{% endblock %} {% block content %}
<h1>Integration</h1>
<section id="intro">
	<p>
		In this section, we examine the other half of calculus,
		<i>integral calculus</i>.
	</p>
	<p>
		To begin, we introduce some new notation. First, suppose we have the
		function ${y = f(x).}$ The <b>differential</b> of ${y}$ is denoted
		${dy,}$ where:
	</p>
	<figure>$$ dy = f'(x)dx $$</figure>
	<p>
		Because ${y}$ is equal to ${f(x),}$ we often read ${dy}$ as
		<q>the differential of ${f.}$</q> Notice that this notation looks
		similar to the notation for a derivative:
	</p>
	<figure>
		$$ \begin{aligned} dy &= f'(x)dx \\[1em] \dfrac{dy}{dx} &= f'(x)
		\end{aligned} $$
	</figure>
	<p>Indeed, the notations are closely related:</p>
	<figure>$$ dy = f'(x)dx \iff \dfrac{dy}{dx} = f'(x) $$</figure>
	<p>
		This close relationship is no accident. It stems directly from the
		Leibnizian (after the mathematician Gottfried Wilhelm Leibniz)
		interpretation of the derivative: Derivatives are ratios of
		<i>differentials</i>. Hence the notation:
	</p>
	<figure>$$ \dfrac{dy}{dx} = f'(x) $$</figure>
	<p>
		So ${dy}$ and ${dx}$ are differentials. But what is a differential? A
		differential is a kind of <b>infinitesimal</b>. We can think of an
		infinitesimal as something akin to an infinitely small quantity.
	</p>
	<p>
		Leibniz is credited with perfecting techniques for handling
		infinitesimals. In part because of how effective his notation was
		&mdash; when it comes to handling infinitesimals, Leibniz's notation is
		far, far more effective than Newton's.<sup></sup> In fact, so much so
		that some historians argue the reliance on Newton's notation set
		British mathematics behind continental Europe's by over a century.
	</p>
	<div class="note">
		<p>
			Constructing a notation system is an exercise in abstraction. A good
			notation system can significantly impact how easy or difficult a
			problem is &mdash; it allows its user to rapidly draw inferences,
			which is precisely how mathematics is done. It doesn't take much
			imagination to see why this is the case. If we had to write computer
			programs in binary &mdash; the language understood by computers
			&mdash; it's unlikely we'd see the myriad of technologies we see
			today.
		</p>
	</div>
	<p>
		A good way to see how the notation works is through linear
		approximation. Recall how we used ${\Delta x}$ and ${\Delta y}$ to
		denote the change in ${x}$ and the change in ${y}$ respectively:
	</p>
	<figure>
		<img
			src="{% static 'images/newton_notation_infinitesimal.svg' %}"
			alt=""
			loading="lazy"
		/>
	</figure>
	<p>
		This is Newton's notation. With Leibniz notation, we replace ${\Delta
		x}$ with ${dx,}$ and ${\Delta y}$ with ${dy:}$
	</p>
	<figure>
		<img
			src="{% static 'images/leibniz_notation_infinitesimal.svg' %}"
			alt=""
			loading="lazy"
		/>
	</figure>
	<p>For example, consider the problem:</p>
	<dfn>
		<small>Problem</small>
		<p>What is ${(64.1)^{\frac{1}{3}}}$ approximately equal to?</p>
	</dfn>
	<p>We start by writing:</p>
	<figure>$$ y = x^{\frac{1}{3}} $$</figure>
	<p>With Leibniz notation, we <i>take the differential of ${y:}$</i></p>
	<figure>$$ dy = \dfrac{1}{3}x^{-\frac{2}{3}} dx $$</figure>
	<p>
		Following the same ideas we saw with linear approximations, we pick a
		starting point, say ${x = 64:}$
	</p>
	<figure>$$ y = 64^{\frac{1}{3}} = 4 $$</figure>
	<p>Returning to Leibniz notation, we get:</p>
	<figure>
		$$ \begin{aligned} dy &= \dfrac{1}{3} (64)^{- \frac{2}{3}}dx \\[1em] &=
		\dfrac{1}{3} \dfrac{1}{16} dx \\[1em] &= \dfrac{1}{48} dx \end{aligned}
		$$
	</figure>
	<p>Given ${x = 64,}$ our equation is:</p>
	<figure>$$ x + dx = 64.1 $$</figure>
	<p>This means that:</p>
	<figure>
		$$ \begin{aligned} x + dx &= 64.1 \\ 64 + dx &= 64.1 \\ dx &= 0.1
		\end{aligned} $$
	</figure>
	<p>
		Hence, we know that ${dx = 1/10.}$ This is the increment, or
		infinitesimal change, we're interested in. Carrying out the
		approximation. We know that ${dy = \dfrac{1}{48}dx,}$ so:
	</p>
	<figure>
		$$ \begin{aligned} (64.1)^{\frac{1}{3}} &\approx y + dy \\[1em]
		&\approx 4 + \dfrac{1}{48}dx \\[1em] &\approx 4 + \dfrac{1}{48} \cdot
		\dfrac{1}{10} \\[1em] &\approx 4 + \dfrac{1}{480} \\[1em] &\approx
		4.002 \end{aligned} $$
	</figure>
</section>

<section id="antiderivative">
	<h2>Antiderivative</h2>
	<p>Another piece of notation we'll introduce is the following:</p>
	<figure>
		$$ \textcolor{lightblue}{G(x)} = \textcolor{#8FBC8F}{\int} g(x) dx $$
	</figure>
	<p>
		In the notation above, ${\textcolor{lightblue}{G(x)}}$ is called the
		<b>antiderivative</b> or <b>indefinite integral</b> of ${g(x).}$ The
		${\textcolor{#8FBC8F}{\int}}$ is the <b>integral symbol</b>.
	</p>
	<p>
		To understand what these terms mean, let's start with some examples.
		When see the expression:
	</p>
	<figure>$$ \int \sin x~dx $$</figure>
	<p>
		we read it as <q>the integral of ${\sin x.}$</q>
		<i
			>The integral of ${\sin x}$ is a function whose derivative is ${\sin
			x.}$
		</i>
		From our discussion of derivatives, we know that the derivative of ${-
		\cos x}$ is ${\sin x.}$ Accordingly, we say that ${- \cos x}$ is the
		antiderivative, or indefinite integral, of ${\sin x:}$
	</p>
	<figure>$$ G(x) = \int \sin x~dx = - \cos x $$</figure>
	<p>
		But why is it called <q>indefinite?</q> Because the derivative of a
		constant is zero, so we can add any constant to ${- \cos x}$ and still
		get the derivative ${\sin x:}$
	</p>
	<figure>$$ G(x) = \int \sin x~dx = - \cos x + C $$</figure>
	<p>
		Whenever we take the antiderivative of a function, it's ambiguous up to
		some constant. Let's consider another example. What does the expression
		evaluate to:
	</p>
	<figure>$$ \int x^a~dx $$</figure>
	<p>
		This example is essentially asking us,
		<q>What function, when differentiated, yields ${x^a?}$</q> We know from
		the power rule that:
	</p>
	<figure>$$ (x^a)' = ax^{a-1}$$</figure>
	<p>Thus, to get ${x^a,}$ we need to cancel out the ${a:}$</p>
	<figure>$$ \int x^a~dx = \dfrac{1}{a+1}x^{a+1} $$</figure>
	<p>Checking our work using differential notation:</p>
	<figure>
		$$ \begin{aligned} d(x^{a+1}) &= (a+1)x^a~dx \\ (a+1)x^{a+1-1} &=
		(a+1)x^a~dx \\ \cancel{(a+1)}x^{a} &= \cancel{(a+1)}x^a~dx \\
		\end{aligned} $$
	</figure>
	<p>And because we can add any constant, we must write:</p>
	<figure>$$ \int x^a~dx = \dfrac{1}{a+1}x^{a+1} + C $$</figure>
	<p>
		Furthermore, examining the antiderivative, we see a restriction. The
		proposition:
	</p>
	<figure>$$ \int x^a dx = \dfrac{1}{a+1} x^{a+1} + C $$</figure>
	<p>
		is true if, and only if, ${a \neq -1.}$ On the other hand, the
		proposition:
	</p>
	<figure>$$ d(x^{a+1}) = (a+1)x^a~dx $$</figure>
	<p>
		contains no such restriction. Consider another problem. Evaluate the
		expression below:
	</p>
	<figure>$$ \int \dfrac{dx}{x} $$</figure>
	<p>
		The first step is to rewrite the expression into a more familiar form:
	</p>
	<figure>$$ \int \dfrac{1}{x}~dx $$</figure>
	<p>
		Looking at this, we immediately see that the function we're looking for
		is ${\ln x}$ (don't forget the constant):
	</p>
	<figure>$$ \int \dfrac{1}{x}~dx = \ln x + C$$</figure>
	<p>
		If we think carefully about the derivative of ${f(x) = \ln x,}$ we'd
		realize that we can also get the derivative ${f'(x) = \dfrac{1}{x}}$
		when ${x}$ is negative:
	</p>
	<figure>
		$$ \begin{aligned} \dfrac{d}{dx} \ln (-x) &= \dfrac{1}{-x} \cdot
		\dfrac{d}{dx} (-x) \\[1em] &= \dfrac{1}{-x} \cdot -1 \\[1em] &=
		\dfrac{-1}{-x} \\[1em] &= \dfrac{1}{x} \end{aligned} $$
	</figure>
	<p>Accordingly, the antiderivative is more properly written as:</p>
	<figure>$$ \int \dfrac{1}{x}~dx = \ln\lvert x \rvert + C$$</figure>
	<p>Another example:</p>
	<figure>$$ \int \sec^2x~dx $$</figure>
	<p>Here we get:</p>
	<figure>$$ \int \sec^2x~dx = \tan x + C $$</figure>
	<p>
		The addition of a constant is the only ambiguous thing about
		antiderivatives.
	</p>
	<p>
		If we examine the antiderivative closely, we can draw a few inferences.
		Say we had some function ${F(x)}$ and another function ${G(x)}$ If
		${F'(x) = G'(x),}$ then it follows that:
	</p>
	<figure>
		$$ \begin{aligned} (F(x)-G(x))' &= F'(x) - G'(x) \\ &= 0 \end{aligned}
		$$
	</figure>
	<p>If ${(F(x)-G(x))' = 0,}$ then it must be the case that:</p>
	<figure>$$ F(x) - G(x) = C $$</figure>
	<p>Rearranging:</p>
	<figure>$$ F(x) = G(x) + C $$</figure>
	<p>In other words, ${F(x) - G(x)}$ results in a constant.<sup></sup></p>
	<div class="note">
		<p>
			Notice that this was one of the lemmas we drew from the
			<cite>mean value theorem</cite>.
		</p>
	</div>
	<p>
		What this tells us is that the addition of a constant is the only
		ambiguous thing about antiderivatives. Beyond that constant, the
		antiderivative is <i>unique</i> &mdash; the antiderivative ${F(x),}$
		where ${F(x) = G(x) + C,}$ is the <em>only</em> antiderivative defined
		as ${G(x) + C.}$
	</p>
	<dfn>
		<small>Antiderivative Uniqueness Theorem</small>
		<p>If ${F'(x) = G'(x),}$ then ${F(x) = G(x) + C.}$</p>
	</dfn>
</section>

<section id="method_of_substitution">
	<h2>Integration Method: Substitution</h2>
	<p>
		In terms of symbolic manipulation, integration is much, much harder
		&mdash; if not impossible &mdash; compared to differentiation. As such,
		whenever we divine integrals, we must always do so gently and
		carefully. Fortunately, there are a few methods that can help us
		accomplish the task. One such method is the
		<b>method of substitution</b>. To illustrate this method, consider the
		following indefinite integral:
	</p>
	<figure>$$ \int x^3(x^4 + 2)^5 dx $$</figure>
	<p>
		On its face, it looks like we can apply the power rule backwards. The
		trouble, however, is that fifth power. Expanding the ${(x^4+2)^5}$ is
		messy.<sup></sup>
	</p>
	<div class="note">
		<p>
			It comes out to: ${ x^{20} + 10x^{16} + 40x^{12} + 80x^8 + 80x^4 + 32
			}$
		</p>
	</div>
</section>
{% endblock %}
