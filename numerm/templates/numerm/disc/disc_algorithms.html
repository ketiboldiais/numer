{% extends '../layout.html' %} {% load static %} {% block description %}
<meta
	name="description"
	content="What is Big O notation? Algorithmic analysis, asymptotic analysis, theta notation, and omega notation."
/>
{% endblock %} {% block title %}
<title>Algorithms</title>
{% endblock %} {% block content %}
<h1>Algorithmic Analysis</h1>
<section id="introduction">
	<p>
		<span class="drop">I</span>n discrete mathematics &mdash; and computer
		science &mdash; many problems are best solved by first constructing a
		<span class="italicsText">model</span>: translate the problem into
		mathematical terms. For example, to find a fleeing murder suspect, we might
		place all their possible hiding places into a set. We might then arrange the
		hiding places into a graph, where each hiding place is a node. Each of those
		nodes likely have varying distances between them, so we might construct a
		function computing each node's likelihood of containing the suspect. That
		function might take numerous inputs, ranging from the distance between the
		nodes to sightings made by various eyewitnesses. Based on the function's
		outputs, we might abstract the nodes into an array, sorted from most likely
		to least likely to contain the suspect.
	</p>
	<p>
		Discrete mathematics provides a war chest of various structures we can use
		for modelling: sets, classes, algebras, sequences, matrices, functions,
		permutations, relations, graphs, trees, networks, finite state machines,
		truth tables, and countless others.
		<span class="marginnote"
			>Our problem solving abilities are further expanded with tools from
			continuous mathematics.</span
		>
		However, knowing <span class="italicsText">what</span> the tools do is
		insufficient. We can buy expensive knives and cutting-edge kitchen
		appliances, but they do not make us Michellin-star chefs. Just as with any
		skilled craft, the tools are only useful if we know
		<span class="italicsText">how to</span> use them. That
		<span class="italicsText">how-to</span> knowledge is provided by an
		<span class="term">algorithm</span>.
	</p>
	<figure>
		<div class="rule">
			<p>
				<span class="topic">Definition.</span> An algorithm is a finite sequence
				of instructions for performing a computation.
			</p>
		</div>
	</figure>
	<p>
		To study algorithms from a discrete mathematics perspective, we will employ
		pseudocode. For those familiar with at least one programming language, the
		pseudocode that follows should be straightforward and immediately readable.
		<span class="marginnote"
			>For those without a programming knowledge background, this author
			recommends an introductory course on Python. Two reasons: (1) It is a
			fairly &mdash; almost deceptively &mdash; beginner-friendly language. (2)
			When writing the pseudocode for these materials, some of the code looked
			an awful lot like Python.</span
		>
	</p>
	<p>For example, consider this problem:</p>
	<figure>
		<div class="rule">
			<p>
				<span class="topic">Problem.</span> Given a finite sequence of integers,
				find the maximum value.
			</p>
		</div>
	</figure>
	<p>
		In constructing an algorithm, we must always think in terms of a computer. A
		computer cannot see all the values in a structure all at once, whether that
		structure is a sequence, graph, tree, etc. Instead, the computer can only
		&#8220;see&#8221; one value at a time. We can think of it like standing
		right in front of an opening at the airport's baggage claim area. Standing
		just in front of the opening, we can only see one bag come out at a time. We
		cannot see all the bags coming before it. The same idea goes for computers
		&mdash; the computer can only see one value at a time.
	</p>
	<p>
		With that in mind, we can elaborate on a potential algorithm as such: (1)
		Suppose the first integer in the sequence is the maximum; call it ${max.}$
		(2) Compare ${max}$ to the next integer in the sequence: If the integer is
		greater than ${max,}$ then suppose ${max}$ is that integer. Otherwise,
		compare the next integer. (3) Repeat the previous steps until all the
		integers in the sequence have been compared. (4) If all the integers in the
		seuqence have been compared, then the maximum is ${max.}$
	</p>
	<p>
		Those are a lot of words for a relatively simple computation. While its
		expressiveness in poetry and literature is immeasurable, English is a
		painful and severely limited language for expressing complex and abstract
		ideas, while maintaining precision (hence the human tendency to develop
		notation systems, as seen in symbolic logic, mathematics, and the sciences).
		Because of this limitation, let's write the algorithm in pseudocode:
	</p>
	<figure>
		<div class="syntax">
			<ul>
				<li>${\ell \coloneqq [a_1, a_2, \ldots a_n] : n \in \Z}$</li>
				<li>${\textit{max}(\ell)}$</li>
				<ul>
					<li>${\textit{max} \coloneqq a_1}$</li>
					<li>for ${i \coloneqq 2}$ to ${n}$:</li>
					<ul>
						<li>${\textit{max} < a_i \implies \textit{max} \coloneqq a_i}$</li>
					</ul>
					<li>return ${\textit{max}}$</li>
				</ul>
			</ul>
		</div>
	</figure>
	<p>
		The algorithm above works. This is a good point for further refining our
		definition of an algorithm. We can think of an algorithm as a sequence of
		instructions towards solving a problem. However, with that definition,
		numerous procedures would qualify as algorithms: the banana bread recipe;
		the YouTube video on changing a tire; counsel's instructions on how to avoid
		a congressional hearing; counsel's instructions on how to testify at a
		congressional hearing. While the WikiHow article falls under the definition
		of an algorithm, it is likely not the kind of algorithm we are concerned
		with. Instead, the algorithms we study are those with the following
		properties:
	</p>
	<p>
		<span class="term">Domain.</span> Every algorithm takes an element, called
		an <span class="italicsText">input</span>, from its domain &mdash; the set
		of all possible values the algorithm may take from.
	</p>
	<p>
		<span class="term">Range.</span> For every input an algorithm takes, the
		algorithm returns an element, called the
		<span class="italicsText">output</span>, from its range &mdash; the set of
		all possible values the algorithm may return. The algorithm's outputs are a
		subset of the range. This subset is called the algorithm's
		<span class="term">solution</span>.
	</p>
	<p>
		<span class="term">Definiteness.</span> An algorithm is definite if, and
		only if, all of its operation, or steps, are defined precisely.
	</p>
	<p>
		<span class="term">Correctness.</span> An algorithm is correct if, and only
		if, the algorithm returns the desired output for every input.
	</p>
	<p>
		<span class="term">Finiteness.</span> An algorithm is finite if, and only
		if, it produces the desired output after a finite number of operations, or
		steps, for every input from the domain.
	</p>
	<p>
		<span class="term">Effectiveness.</span> An algorithm is effective if, and
		only if, it is possible to perform each of its operations, or steps, exactly
		and in a finite amount of time.
	</p>
	<p>
		<span class="term">Generality.</span> An algorithm is general if, and only
		if, it can be executed for all the problem's inputs, rather than a subset of
		the problem's inputs.
	</p>
</section>

<section id="big_o_notation">
	<h2>Big-O Notation</h2>
	<p>
		Big-O notations have been used in mathematics for a very long time. The
		capital O is often called the
		<span class="term">Landau symbol</span>, after the German mathematician
		Edmund Landau for his extensive use of the symbol. Its use for complexity
		analysis (alongside big-theta and big-omega), however, was popularized by
		Donald Knuth in the 1970s.
	</p>
	<p>We begin by stating the formal definition of Big-O notation:</p>
	<figure>
		<div class="rule">
			<p>
				<span class="topic">Definition: Big O.</span> Suppose ${f}$ and ${g}$
				are functions from ${\N \to \R}$ or from ${\R \to \R.}$ If there are
				constants ${C}$ and ${k}$ such that whenever ${n > k,}$ the following is
				true:
			</p>
			<figure>
				<div>
					<p>${\lvert f(n) \rvert \leq C \lvert g(n) \rvert}$</p>
				</div>
			</figure>
			<p>we say that ${f(n)}$ is big-O of ${g(n),}$ and write:</p>
			<figure>
				<div>
					<p>${f(n) \in O(g(n))}$</p>
				</div>
			</figure>
		</div>
	</figure>
	<p>
		There's a lot going on with this definition, so let's break it down into a
		checklist:
	</p>
	<figure>
		<ol class="checklist">
			<h3 class="listHeader">Big-O Checklist</h3>
			<li>There are two functions, ${f}$ and ${g.}$</li>
			<li>One of the following is true:</li>
			<ol>
				<li>${f}$ and ${g}$ map integers to real numbers,</li>
				<em>or</em>
				<li>${f}$ and ${g}$ map real numbers to real numbers.</li>
			</ol>
			<li>There are two constants, ${C}$ and ${k.}$</li>
			<li>If ${n > k,}$ then:</li>
			<ol>
				<li>${\lvert f(n) \rvert < C\lvert g(n) \rvert }$</li>
				<em>or</em>
				<li>${\lvert f(n) \rvert = C\lvert g(n) \rvert }$</li>
			</ol>
		</ol>
	</figure>
	<p>
		Check all of the boxes above, and we can conclude: ${f(n) \in O(g(n)).}$ The
		definition above essentially means that ${f(n)}$ grows slower than some
		fixed multiple of ${g(n)}$ as ${n}$ approaches positive infinity.
	</p>
	<p>
		The key item in the checklist above is the requirement for a pair of
		constants, ${C}$ and ${k,}$ and the last item, and implication. We call the
		constants ${C}$ and ${k}$ the
		<span class="term">witnesses</span> to the relationship ${f(n)}$ is
		${O(g(n)).}$ As long as we can find at least one pair of ${C}$ and ${k}$
		(and check off the other items), we can reach our conclusion.
	</p>
	<p>For example, consider the following proposition:</p>
	<figure>
		<div>
			<p>
				Given ${f(n) = 4n^2 + 16n + 2,}$ ${f(n)}$ is ${O(n^4),}$ where ${n \in
				\R^{+}.}$
			</p>
		</div>
	</figure>
	<p>
		Is this proposition true? Well, let's apply the definition. We have two
		functions, both mapping real numbers to real numbers. These two requirements
		are seldom an issue. The bigger hurdle is finding the witnesses. One
		approach is plug-and-play. Let's suppose that ${C = 1,}$ and that ${k = 0.}$
		If ${k = 0,}$ then ${n > 0.}$ Let's try some values of ${n > 0:}$
	</p>

	<table class="punnett">
		<thead>
			<th>${n}$</th>
			<th>${\lvert 4n^2 + 16n + 2 \rvert }$</th>
			<th>Relation</th>
			<th>${C \lvert n^4 \rvert }$</th>
			<th>Truth Value</th>
		</thead>
		<tbody>
			<tr>
				<td>1</td>
				<td>${\lvert 4(1)^2 + 16(1) + 2 \rvert = 22}$</td>
				<td>${\leq}$</td>
				<td>${(1)\lvert (1)^4 \rvert = 4}$</td>
				<td>False</td>
			</tr>
			<tr>
				<td>2</td>
				<td>${\lvert 4(2)^2 + 16(2) + 2 \rvert = 50}$</td>
				<td>${\leq}$</td>
				<td>${(2)\lvert (2)^4 \rvert = 16}$</td>
				<td>False</td>
			</tr>
			<tr>
				<td>3</td>
				<td>${\lvert 4(3)^2 + 16(3) + 2 \rvert = 86}$</td>
				<td>${\leq}$</td>
				<td>${(1)\lvert (3)^4 \rvert = 81}$</td>
				<td>False</td>
			</tr>
			<tr>
				<td>4</td>
				<td>${\lvert 4(4)^2 + 16(4) + 2 \rvert = 130}$</td>
				<td>${\leq}$</td>
				<td>${(1) \lvert (4)^4 \rvert = 256}$</td>
				<td>True</td>
			</tr>
		</tbody>
	</table>

	<p>
		From our analysis above, we see that when ${n > 4,}$ the proposition is
		true. This is because once we find a single pair of witnesses, there are
		<span class="underlineText">infinitely many</span> pairs of witnesses. This
		is because we're comparing the absolute values of the functions ${f}$ and
		${g.}$ Any value greater than ${C,}$ call it ${C',}$ and any value greater
		than ${k,}$ call it ${k',}$ implies that ${C' > C}$ and ${k' > k.}$ This in
		turn implies that ${C'\lvert g(x) \rvert \geq C\lvert g(x) \rvert}$ whenever
		${x > k' > k.}$ As such, we change our value for the witness ${k:}$ The
		proposition above is true when ${k = 4}$ and ${C = 1.}$
	</p>
	<p>
		Now, if we could never find values of ${k}$ and ${C}$ such that ${\lvert
		f(n) \rvert \leq C \lvert g(n) \rvert}$ is true, then the proposition
		&#8220;${f(n)}$ is ${O(g(n))}$&#8221; is false.
	</p>
	<p>
		Notice further that we could have replaced ${g(n) = n^4}$ with another
		function. It could have been ${g(n) = n^5}$ or ${g(n) = n^3 + 1,}$ in which
		case we could've said &#8220;${f(n)}$ is ${O(n^5)}$&#8221; and
		&#8220;${f(n)}$ is ${O(n^3 + 1)}$&#8221; respectively. Big-O notation is
		just a means of comparing the growth rate of functions: ${f(n)}$ grows
		slower than some fixed multiple of ${g(n)}$ as ${n}$ approaches positive
		infinity.
	</p>
	<p>
		Because of this fact, we would have also said that ${f(n) = 4n^2 + 16n + 2}$
		is ${O(n^2).}$ Why? Because we can come up with a pair of ${C}$ and ${k}$
		that satisfy the definition: ${k = 0}$ and ${C = 23.}$ Once more, we can say
		that ${4n^2 + 16n + 2}$ is ${O(n^2),}$ and vice versa, that ${n^2}$ is
		${O(4n^2 + 16n + 2.)}$ When we have this relationship between ${f(n)}$ and
		${g(n),}$ we say that the big-O relations are
		<span class="term">of the same order.</span>
	</p>
	<p>
		When we see ${f(n) \in O(g(n)),}$ it's helpful to interpret the proposition
		expressed as something along the lines of:
	</p>
	<ul>
		<li>
			Eventually, ${f(n)}$ is less than some scaled up version of ${g(n).}$
		</li>
		<li>${f}$'s growth rate is no greater than ${g}$'s growth rate.</li>
		<li>${f}$ can't grow any faster than ${g.}$</li>
	</ul>
	<p>Here are some other examples:</p>
	<ul>
		<li>${n - 1 \in O(n).}$</li>
		<p>True. Suppose ${C = 1}$ and ${k = 0.}$</p>
		<p>Then ${n - 1 \leq 1 \cdot n}$ for all ${n \in \R^{+}.}$</p>
		<li>${n + 1 \in O(n).}$</li>
		<p>True. Suppose ${C = 2}$ and ${k = 1.}$</p>
		<p>Then ${n + 1 \leq 2n}$ for ${n > 1.}$</p>
		<li>${2n - 1 \in O(n)}$</li>
		<p>True. Suppose ${C = 3}$ and ${k = 0.}$</p>
		<p>Then ${2n - 1 \leq 3n}$ for all ${n \in \R^{+}.}$</p>
	</ul>
	<p>
		The key point: As long as we find some constants ${C}$ and ${k}$ that
		satisfy the relation ${\lvert f(n) \rvert \leq C\lvert g(n) \rvert}$
		whenever ${n > k}$ and where ${n \in \R}$ or ${n \in \Z,}$ we've satisfied
		the definition.
	</p>
</section>

<section id="big_omega_notation">
	<h2>Big-Omega Notation</h2>
	<p>
		While Big-O notation has been used for centuries, Big-Omega notation was
		introduced by Donald Knuth in 1976 to remedy the misuse of Big-O notation.
		The shortcoming of Big-O notation is that it only provides an upper bound
		&mdash; ${g(n)}$ &mdash; for the size of ${f(n)}$ given large values of
		${n.}$ The misuse arises when we use Big-O notation to somehow communicate a
		lower bound. As we know, Big-O notation cannot express a lower bound by
		definition. Responding to this problem, Donald Knuth introduced Big-Omega
		notation:
	</p>
	<figure>
		<div class="rule">
			<p>
				<span class="topic">Definition.</span> Suppose ${f}$ and ${g}$ are
				functions from ${\N \to \R}$ or from ${\R \to \R.}$ If there are
				positive constants ${C}$ and ${k}$ such that whenever ${n > k,}$ the
				following is true:
			</p>
			<figure>
				<div>
					<p>${\lvert f(n) \rvert \geq C\lvert g(n) \rvert}$</p>
				</div>
			</figure>
			<p>We say that ${f(n)}$ is big-Omega of ${g(n),}$ and write:</p>
			<figure>
				<div>
					<p>${f(n) \in \Omega(g(n))}$</p>
				</div>
			</figure>
		</div>
	</figure>
	<p>
		Notice that the only difference between Big-O and Big-Omega is that the
		inequality is reversed. Breaking the definition down into a checklist:
	</p>
	<figure>
		<ol class="checklist">
			<h3 class="listHeader">Big-Omega Checklist</h3>
			<li>There are two functions, ${f}$ and ${g.}$</li>
			<li>One of the following is true:</li>
			<ol>
				<li>${f}$ and ${g}$ map integers to real numbers,</li>
				<em>or</em>
				<li>${f}$ and ${g}$ map real numbers to real numbers.</li>
			</ol>
			<li>There are two constants, ${C}$ and ${k.}$</li>
			<li>If ${n > k,}$ then:</li>
			<ol>
				<li>${\lvert f(n) \rvert > C\lvert g(n) \rvert }$</li>
				<em>or</em>
				<li>${\lvert f(n) \rvert = C\lvert g(n) \rvert }$</li>
			</ol>
		</ol>
	</figure>
	<p>
		If we check all of the boxes above, we can say that ${f(n) \in
		\Omega(g(n)).}$ Whenever we see the proposition ${f(n) \in \Omega(g(n)),}$
		we should interpret it as follows:
	</p>
	<ul>
		<li>
			Eventually, ${f(n)}$ is <span class="underlineText">more than</span> some
			scaled up version of ${g(n).}$
		</li>
		<li>
			${f}$'s growth rate is <span class="underlineText">no less</span> than
			${g}$'s growth rate.
		</li>
		<li>${f}$ can't grow any slower than ${g.}$</li>
	</ul>
	<p>
		Essentially, we're establishing a floor, or lower bound, in terms of
		${g(n).}$ For example, consider the following proposition:
	</p>
	<figure>
		<div>
			<p>
				Given ${f(n) = 4n^2 + 16n + 2,}$ ${f(n) \in \Omega(n^2),}$ where ${n \in
				\R^{+}.}$
			</p>
		</div>
	</figure>
	<p>Once more, we need witnesses ${C}$ and ${k}$ such that:</p>
	<figure>
		<div>
			<p>
				${\lvert 4n^2 + 16n + 2 \rvert \geq C\lvert n^2 \rvert }$ where ${n >
				k.}$
			</p>
		</div>
	</figure>
	<p>
		This is straightforward. Suppose ${C = 1}$ and ${k = 0.}$ Then suppose ${n =
		1.}$ We have:
	</p>
	<figure>
		<div>
			$$ \begin{aligned} \lvert 4(1)^2 + 16(1) + 2 \rvert &\geq (1)(1)^2 \\[1em]
			22 &\geq 1 \end{aligned} $$
		</div>
	</figure>
	<p>
		which is true. The function ${f(n) = 4n^2 + 16n + 2}$ cannot run slower than
		${g(n) = n^2.}$ Or, put another way, ${f(n) = 4n^2 + 16n + 2}$ always runs
		faster than ${g(n) = n^2.}$ Again, the brunt of our work will almost always
		be to look for a ${C}$ and a ${k}$ that satisfy the relation.
	</p>
</section>

<section id="big_theta_notation">
	<h2>Big-Theta Notation</h2>
</section>

{% endblock %}
